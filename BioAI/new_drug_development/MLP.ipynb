{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d1a03e6",
   "metadata": {},
   "source": [
    "### Installation & Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "39c26167",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, Descriptors, Lipinski\n",
    "from rdkit import DataStructs\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from pl_bolts.optimizers.lr_scheduler import LinearWarmupCosineAnnealingLR\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "785f170c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'NBITS': 2048,\n",
    "    'SEED': 42\n",
    "}\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True  # 재현성\n",
    "    torch.backends.cudnn.benchmark = False   #안정성\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "seed_everything(CFG['SEED'])\n",
    "\n",
    "# SMILES 데이터를 분자 지문으로 변환\n",
    "def smiles_to_fingerprint(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is not None:\n",
    "        fp = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=CFG['NBITS'])\n",
    "        return np.array(fp)\n",
    "    else:\n",
    "        return np.zeros((CFG['NBITS'],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4c6221a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def IC50_to_pIC50(ic50_nM):\n",
    "    ic50_nM = np.clip(ic50_nM, 1e-10, None)\n",
    "    return 9 - np.log10(ic50_nM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9f46b689",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pIC50_to_IC50(pIC50):\n",
    "    return 10 ** (9 - pIC50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189fd78c",
   "metadata": {},
   "source": [
    "### Data loading & Molecular Descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f49f4aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:10] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:11] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:12] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:07:13] DEPRECATION WARNING: please use MorganGenerator\n"
     ]
    }
   ],
   "source": [
    "IC50_dataset = pd.read_csv(\"C:/Users/user/Desktop/dacon_drug_development/IC50_dataset.csv\")\n",
    "IC50_dataset['Fingerprint'] = IC50_dataset['smiles'].apply(smiles_to_fingerprint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9d70a8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def descriptors(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    f = {}\n",
    "    # 1. 화학기 존재 여부\n",
    "    # Tetrazole ring\n",
    "    tetrazole_smarts = ['c1nn[n+](n1)[O-]', 'c1[nH]nnn1']\n",
    "    f['has_tetrazole'] = any(mol.HasSubstructMatch(Chem.MolFromSmarts(s)) for s in tetrazole_smarts)\n",
    "    # Triazole ring\n",
    "    f['has_triazole'] = mol.HasSubstructMatch(Chem.MolFromSmarts('c1nnc(n1)'))\n",
    "    # Sulfoxide group\n",
    "    f['has_sulfoxide'] = mol.HasSubstructMatch(Chem.MolFromSmarts('S(=O)(C)'))\n",
    "    # Amide carbonyl\n",
    "    amide_smarts = ['C(=O)N', 'NC(=O)']\n",
    "    f['has_amide'] = any(mol.HasSubstructMatch(Chem.MolFromSmarts(s)) for s in amide_smarts)\n",
    "    # Sulfonamide group\n",
    "    f['has_sulfonamide'] = mol.HasSubstructMatch(Chem.MolFromSmarts('S(=O)(=O)N'))\n",
    "    \n",
    "    # 2. 분자량\n",
    "    mw = Descriptors.MolWt(mol)\n",
    "    f['mw'] = mw\n",
    "    f['mw_300_500'] = 300<=mw<=500\n",
    "    \n",
    "    # 3. logP\n",
    "    logp = Descriptors.MolLogP(mol)\n",
    "    f['logP'] = logp\n",
    "    f['logP_2_4'] = 2<=logp<=4\n",
    "    \n",
    "    # 4. TPSA\n",
    "    tpsa = Descriptors.TPSA(mol)\n",
    "    f['TPSA'] = tpsa\n",
    "    f['TPSA_60_120'] = 60<=tpsa<=120\n",
    "    \n",
    "    # 5. rotatable bonds (IC50 값 높임_bad)\n",
    "    rot = Lipinski.NumRotatableBonds(mol)\n",
    "    f['rotatable'] = rot\n",
    "    f['rot_gt7'] = rot>7\n",
    "\n",
    "    # 6. ring (+aromatic)\n",
    "    # ring 2개 이상\n",
    "    num_rings = mol.GetRingInfo().NumRings()\n",
    "    f['num_rings'] = num_rings\n",
    "    f['ring_count_ge_2'] = num_rings >= 2\n",
    "    # aromatic ring 존재 여부\n",
    "    ssr = Chem.GetSymmSSSR(mol)\n",
    "    aromatic_ring = any(all(mol.GetAtomWithIdx(idx).GetIsAromatic() for idx in ring) for ring in ssr)\n",
    "    f['has_aromatic_ring'] = aromatic_ring\n",
    "    # Ring 2개 이상 + 그 중 적어도 하나 aromatic인가?\n",
    "    f['ring_ge2_and_aromatic'] = f['ring_count_ge_2'] and f['has_aromatic_ring']\n",
    "\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "680739f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = pd.DataFrame([descriptors(s) for s in IC50_dataset['smiles']])\n",
    "final_dataset = pd.concat([IC50_dataset, features_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e37e2e9",
   "metadata": {},
   "source": [
    "### Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d2b4df43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        features = torch.tensor(self.X[idx]).float()\n",
    "        if self.y is not None:\n",
    "            target = torch.tensor(self.y[idx]).float()\n",
    "            return features, target\n",
    "        else:\n",
    "            return features\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "82858876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature 준비 함수 (Fingerprint 등 합치기)\n",
    "def prepare_features(df):\n",
    "    fp_array = np.stack(df['Fingerprint']).astype(np.float32)\n",
    "    feature_cols = [col for col in df.columns if col not in ['pIC50', 'Fingerprint']]\n",
    "    feature_df = df[feature_cols].copy()\n",
    "    for c in feature_df.columns:\n",
    "        if feature_df[c].dtype == 'bool':\n",
    "            feature_df[c] = feature_df[c].astype(int)\n",
    "    feature_array = feature_df.values.astype(np.float32)\n",
    "    X = np.hstack([fp_array, feature_array])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "216d1524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/valid split\n",
    "train = final_dataset.drop(['smiles', 'IC50_nM'], axis=1)\n",
    "train_df_split, valid_df_split = train_test_split(train, test_size=0.3, random_state=42)\n",
    "\n",
    "# feature 추출\n",
    "X_train = prepare_features(train_df_split)\n",
    "X_valid = prepare_features(valid_df_split)\n",
    "\n",
    "y_train = train_df_split['pIC50'].values.astype(np.float32)\n",
    "y_valid = valid_df_split['pIC50'].values.astype(np.float32)\n",
    "\n",
    "# transform은 train에만 fit, valid는 transform만\n",
    "transform = VarianceThreshold(threshold=0.05)\n",
    "X_train_tr = transform.fit_transform(X_train)\n",
    "X_valid_tr = transform.transform(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c94eb350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_size: 201\n"
     ]
    }
   ],
   "source": [
    "# Dataset\n",
    "train_dataset = CustomDataset(X_train_tr, y_train)\n",
    "valid_dataset = CustomDataset(X_valid_tr, y_valid)\n",
    "\n",
    "# input_size\n",
    "input_size = train_dataset.X.shape[1]\n",
    "print(\"input_size:\", input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2953daac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "CFG_model = {\n",
    "    'BATCH_SIZE': 256,\n",
    "    'EPOCHS': 200,\n",
    "    'INPUT_SIZE': input_size,\n",
    "    'HIDDEN_SIZE': 2048,\n",
    "    'OUTPUT_SIZE': 1,\n",
    "    'DROPOUT_RATE': 0.15,\n",
    "    'LEARNING_RATE': 0.0005\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "78bb10ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=CFG_model['BATCH_SIZE'], shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=CFG_model['BATCH_SIZE'], shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43b5799",
   "metadata": {},
   "source": [
    "### Building MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a0ec20cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_rate, out_size):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        bottleneck_size = hidden_size // 2\n",
    "\n",
    "        # fc 레이어 3개와 출력 레이어 (bottleneck 구조)\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)   # (input_size, 2048)\n",
    "        self.fc2 = nn.Linear(hidden_size, bottleneck_size)   # (2048, 1024)\n",
    "        self.fc3 = nn.Linear(bottleneck_size, hidden_size)   # (1024, 2048)\n",
    "        self.fc_out = nn.Linear(hidden_size, out_size)\n",
    "\n",
    "        # 정규화\n",
    "        self.ln1 = nn.LayerNorm(hidden_size)\n",
    "        self.ln2 = nn.LayerNorm(bottleneck_size)\n",
    "        self.ln3 = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        # 활성화 함수\n",
    "        self.activation = nn.LeakyReLU()\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)   # 0.15\n",
    "        self.dropout2 = nn.Dropout(dropout_rate+0.1)   # 0.25\n",
    "        self.dropout3 = nn.Dropout(dropout_rate+0.2)   # 0.35\n",
    "     \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.activation(out)\n",
    "        out = self.ln1(out)\n",
    "        out1 = self.dropout1(out)\n",
    "\n",
    "        out = self.fc2(out1)\n",
    "        out = self.activation(out)\n",
    "        out = self.ln2(out)\n",
    "        out = self.dropout2(out)\n",
    "        \n",
    "        out = self.fc3(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.ln3(out)\n",
    "        out2 = self.dropout3(out)\n",
    "        out2 = out1 + out2\n",
    "\n",
    "        out = self.fc_out(out2)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ba72e00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(CFG_model['INPUT_SIZE'],CFG_model['HIDDEN_SIZE'],CFG_model['DROPOUT_RATE'],CFG_model['OUTPUT_SIZE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6961ef59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CFG_model hyperparameters 추가\n",
    "num_samples = len(train_dataset)\n",
    "batch_size = CFG_model['BATCH_SIZE']\n",
    "num_epochs = CFG_model['EPOCHS']\n",
    "\n",
    "# step 단위 변수 대신 epoch 단위로 변환\n",
    "warmup_epochs = int(num_epochs * 0.02)  # warmup epochs\n",
    "\n",
    "# CFG_model에 epoch 단위 key 파라미터 추가/업데이트\n",
    "CFG_model['WARMUP_EPOCHS'] = warmup_epochs\n",
    "CFG_model['MAX_EPOCHS'] = num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a751cf7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_11076\\4145934217.py:3: UnderReviewWarning: The feature LinearWarmupCosineAnnealingLR is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  scheduler = LinearWarmupCosineAnnealingLR(\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CFG_model['LEARNING_RATE'], weight_decay=1e-4)\n",
    "scheduler = LinearWarmupCosineAnnealingLR(\n",
    "    optimizer,\n",
    "    warmup_epochs=CFG_model['WARMUP_EPOCHS'],\n",
    "    max_epochs=CFG_model['MAX_EPOCHS'],\n",
    "    eta_min=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa67beb1",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "80658212",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train(train_loader, valid_loader, model, criterion, optimizer, scheduler, epochs):\n",
    "    train_loss = []\n",
    "    valid_loss = []\n",
    "    lrs = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(inputs).squeeze(dim=1)  # torch.Size([128, 1])\n",
    "            # target shape은 항상 torch.Size([128])\n",
    "            loss = criterion(output, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)  # 배치 크기만큼 loss를 누적\n",
    "        \n",
    "        avg_train_loss = running_loss / len(train_loader.dataset)  # 전체 샘플 수로 나누기\n",
    "        train_loss.append(avg_train_loss)\n",
    "\n",
    "        # Validatation\n",
    "        model.eval()\n",
    "        valid_running_loss = 0\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in valid_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                output = model(inputs).squeeze(dim=1)\n",
    "                loss = criterion(output, targets)\n",
    "                valid_running_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "        avg_valid_loss = valid_running_loss / len(valid_loader.dataset)\n",
    "        valid_loss.append(avg_valid_loss)\n",
    "\n",
    "        scheduler.step()\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        lrs.append(lr)\n",
    "\n",
    "        print(f'Epoch: {epoch+1}/{epochs}, '\n",
    "                  f'Train Loss: {avg_train_loss:.5f}, '\n",
    "                  f'Valid Loss: {avg_valid_loss:.5f}, '\n",
    "                  f'lr: {lr:.6f}'\n",
    "                  )\n",
    "            \n",
    "        model.train()\n",
    "    \n",
    "    return model, train_loss, valid_loss, lrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d908e83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KfoldCustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y=None, is_test=False):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            self.X = X.values.astype(np.float32)\n",
    "        else:\n",
    "            self.X = X.astype(np.float32)\n",
    "\n",
    "        if y is not None:\n",
    "            if isinstance(y, pd.Series):\n",
    "                self.y = y.values.astype(np.float32)\n",
    "            else:\n",
    "                self.y = y.astype(np.float32)\n",
    "        else:\n",
    "            self.y = None\n",
    "\n",
    "        self.is_test = is_test\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]\n",
    "        if self.is_test or self.y is None:\n",
    "            return torch.tensor(x, dtype=torch.float32)\n",
    "        else:\n",
    "            y = self.y[idx]\n",
    "            return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62721964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Fold 1 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_11076\\2664917137.py:52: UnderReviewWarning: The feature LinearWarmupCosineAnnealingLR is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  scheduler = LinearWarmupCosineAnnealingLR(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/200, Train Loss: 76.26148, Valid Loss: 65.74511, lr: 0.000075\n",
      "Epoch: 2/200, Train Loss: 22.29554, Valid Loss: 21.41787, lr: 0.000150\n",
      "Epoch: 3/200, Train Loss: 6.11774, Valid Loss: 3.81052, lr: 0.000225\n",
      "Epoch: 4/200, Train Loss: 4.26243, Valid Loss: 5.65843, lr: 0.000300\n",
      "Epoch: 5/200, Train Loss: 3.61540, Valid Loss: 3.05229, lr: 0.000300\n",
      "Epoch: 6/200, Train Loss: 3.42525, Valid Loss: 3.75473, lr: 0.000300\n",
      "Epoch: 7/200, Train Loss: 3.45452, Valid Loss: 2.90449, lr: 0.000300\n",
      "Epoch: 8/200, Train Loss: 3.24760, Valid Loss: 3.11662, lr: 0.000300\n",
      "Epoch: 9/200, Train Loss: 3.29397, Valid Loss: 2.99368, lr: 0.000300\n",
      "Epoch: 10/200, Train Loss: 3.11499, Valid Loss: 3.00766, lr: 0.000300\n",
      "Epoch: 11/200, Train Loss: 3.20468, Valid Loss: 2.96887, lr: 0.000300\n",
      "Epoch: 12/200, Train Loss: 3.18359, Valid Loss: 2.95481, lr: 0.000299\n",
      "Epoch: 13/200, Train Loss: 3.04567, Valid Loss: 2.92355, lr: 0.000299\n",
      "Epoch: 14/200, Train Loss: 3.06385, Valid Loss: 2.95727, lr: 0.000299\n",
      "Epoch: 15/200, Train Loss: 3.08147, Valid Loss: 2.95318, lr: 0.000299\n",
      "Epoch: 16/200, Train Loss: 3.03591, Valid Loss: 2.88884, lr: 0.000298\n",
      "Epoch: 17/200, Train Loss: 3.09995, Valid Loss: 2.94666, lr: 0.000298\n",
      "Epoch: 18/200, Train Loss: 3.08574, Valid Loss: 2.91115, lr: 0.000298\n",
      "Epoch: 19/200, Train Loss: 3.08822, Valid Loss: 2.91131, lr: 0.000297\n",
      "Epoch: 20/200, Train Loss: 3.03948, Valid Loss: 2.88244, lr: 0.000297\n",
      "Epoch: 21/200, Train Loss: 3.08083, Valid Loss: 2.85112, lr: 0.000297\n",
      "Epoch: 22/200, Train Loss: 3.01612, Valid Loss: 2.86162, lr: 0.000296\n",
      "Epoch: 23/200, Train Loss: 2.96210, Valid Loss: 2.88866, lr: 0.000296\n",
      "Epoch: 24/200, Train Loss: 3.00310, Valid Loss: 2.88747, lr: 0.000295\n",
      "Epoch: 25/200, Train Loss: 3.03460, Valid Loss: 2.84954, lr: 0.000295\n",
      "Epoch: 26/200, Train Loss: 2.98716, Valid Loss: 2.89842, lr: 0.000294\n",
      "Epoch: 27/200, Train Loss: 2.95491, Valid Loss: 2.78842, lr: 0.000294\n",
      "Epoch: 28/200, Train Loss: 3.04380, Valid Loss: 3.00051, lr: 0.000293\n",
      "Epoch: 29/200, Train Loss: 3.02077, Valid Loss: 2.76966, lr: 0.000293\n",
      "Epoch: 30/200, Train Loss: 2.98768, Valid Loss: 2.95709, lr: 0.000292\n",
      "Epoch: 31/200, Train Loss: 2.95740, Valid Loss: 2.75832, lr: 0.000291\n",
      "Epoch: 32/200, Train Loss: 2.92094, Valid Loss: 2.94977, lr: 0.000291\n",
      "Epoch: 33/200, Train Loss: 2.90997, Valid Loss: 2.79215, lr: 0.000290\n",
      "Epoch: 34/200, Train Loss: 2.84841, Valid Loss: 2.78471, lr: 0.000289\n",
      "Epoch: 35/200, Train Loss: 2.87973, Valid Loss: 2.83350, lr: 0.000289\n",
      "Epoch: 36/200, Train Loss: 2.84067, Valid Loss: 2.79781, lr: 0.000288\n",
      "Epoch: 37/200, Train Loss: 2.83124, Valid Loss: 2.77082, lr: 0.000287\n",
      "Epoch: 38/200, Train Loss: 2.84141, Valid Loss: 2.81927, lr: 0.000286\n",
      "Epoch: 39/200, Train Loss: 2.87982, Valid Loss: 2.71841, lr: 0.000285\n",
      "Epoch: 40/200, Train Loss: 2.87883, Valid Loss: 2.90609, lr: 0.000285\n",
      "Epoch: 41/200, Train Loss: 2.89899, Valid Loss: 2.70804, lr: 0.000284\n",
      "Epoch: 42/200, Train Loss: 2.76821, Valid Loss: 2.79197, lr: 0.000283\n",
      "Epoch: 43/200, Train Loss: 2.83146, Valid Loss: 2.75607, lr: 0.000282\n",
      "Epoch: 44/200, Train Loss: 2.81815, Valid Loss: 2.75958, lr: 0.000281\n",
      "Epoch: 45/200, Train Loss: 2.85500, Valid Loss: 2.72357, lr: 0.000280\n",
      "Epoch: 46/200, Train Loss: 2.80954, Valid Loss: 2.74264, lr: 0.000279\n",
      "Epoch: 47/200, Train Loss: 2.71178, Valid Loss: 2.68817, lr: 0.000278\n",
      "Epoch: 48/200, Train Loss: 2.76378, Valid Loss: 2.72561, lr: 0.000277\n",
      "Epoch: 49/200, Train Loss: 2.72456, Valid Loss: 2.75516, lr: 0.000276\n",
      "Epoch: 50/200, Train Loss: 2.74649, Valid Loss: 2.68896, lr: 0.000275\n",
      "Epoch: 51/200, Train Loss: 2.72062, Valid Loss: 2.71631, lr: 0.000274\n",
      "Epoch: 52/200, Train Loss: 2.79748, Valid Loss: 2.66833, lr: 0.000273\n",
      "Epoch: 53/200, Train Loss: 2.74991, Valid Loss: 2.71517, lr: 0.000272\n",
      "Epoch: 54/200, Train Loss: 2.72149, Valid Loss: 2.63087, lr: 0.000270\n",
      "Epoch: 55/200, Train Loss: 2.71014, Valid Loss: 2.80223, lr: 0.000269\n",
      "Epoch: 56/200, Train Loss: 2.72263, Valid Loss: 2.60355, lr: 0.000268\n",
      "Epoch: 57/200, Train Loss: 2.77165, Valid Loss: 2.86289, lr: 0.000267\n",
      "Epoch: 58/200, Train Loss: 2.75965, Valid Loss: 2.59187, lr: 0.000266\n",
      "Epoch: 59/200, Train Loss: 2.74318, Valid Loss: 2.74197, lr: 0.000264\n",
      "Epoch: 60/200, Train Loss: 2.68596, Valid Loss: 2.58837, lr: 0.000263\n",
      "Epoch: 61/200, Train Loss: 2.64093, Valid Loss: 2.65245, lr: 0.000262\n",
      "Epoch: 62/200, Train Loss: 2.70499, Valid Loss: 2.66065, lr: 0.000261\n",
      "Epoch: 63/200, Train Loss: 2.66070, Valid Loss: 2.58473, lr: 0.000259\n",
      "Epoch: 64/200, Train Loss: 2.69991, Valid Loss: 2.78704, lr: 0.000258\n",
      "Epoch: 65/200, Train Loss: 2.68520, Valid Loss: 2.55128, lr: 0.000257\n",
      "Epoch: 66/200, Train Loss: 2.61807, Valid Loss: 2.73100, lr: 0.000255\n",
      "Epoch: 67/200, Train Loss: 2.64241, Valid Loss: 2.54365, lr: 0.000254\n",
      "Epoch: 68/200, Train Loss: 2.57185, Valid Loss: 2.62102, lr: 0.000253\n",
      "Epoch: 69/200, Train Loss: 2.54809, Valid Loss: 2.58356, lr: 0.000251\n",
      "Epoch: 70/200, Train Loss: 2.61411, Valid Loss: 2.58426, lr: 0.000250\n",
      "Epoch: 71/200, Train Loss: 2.55952, Valid Loss: 2.57747, lr: 0.000249\n",
      "Epoch: 72/200, Train Loss: 2.58144, Valid Loss: 2.56483, lr: 0.000247\n",
      "Epoch: 73/200, Train Loss: 2.57564, Valid Loss: 2.54035, lr: 0.000246\n",
      "Epoch: 74/200, Train Loss: 2.56105, Valid Loss: 2.57763, lr: 0.000244\n",
      "Epoch: 75/200, Train Loss: 2.52126, Valid Loss: 2.52211, lr: 0.000243\n",
      "Epoch: 76/200, Train Loss: 2.49292, Valid Loss: 2.59334, lr: 0.000241\n",
      "Epoch: 77/200, Train Loss: 2.58435, Valid Loss: 2.49390, lr: 0.000240\n",
      "Epoch: 78/200, Train Loss: 2.53767, Valid Loss: 2.54831, lr: 0.000238\n",
      "Epoch: 79/200, Train Loss: 2.52724, Valid Loss: 2.57223, lr: 0.000237\n",
      "Epoch: 80/200, Train Loss: 2.46771, Valid Loss: 2.50940, lr: 0.000235\n",
      "Epoch: 81/200, Train Loss: 2.50270, Valid Loss: 2.56383, lr: 0.000234\n",
      "Epoch: 82/200, Train Loss: 2.47999, Valid Loss: 2.48856, lr: 0.000232\n",
      "Epoch: 83/200, Train Loss: 2.48651, Valid Loss: 2.46498, lr: 0.000231\n",
      "Epoch: 84/200, Train Loss: 2.45871, Valid Loss: 2.55448, lr: 0.000229\n",
      "Epoch: 85/200, Train Loss: 2.52552, Valid Loss: 2.47448, lr: 0.000228\n",
      "Epoch: 86/200, Train Loss: 2.38634, Valid Loss: 2.49276, lr: 0.000226\n",
      "Epoch: 87/200, Train Loss: 2.46282, Valid Loss: 2.45599, lr: 0.000225\n",
      "Epoch: 88/200, Train Loss: 2.42943, Valid Loss: 2.49790, lr: 0.000223\n",
      "Epoch: 89/200, Train Loss: 2.48078, Valid Loss: 2.44223, lr: 0.000222\n",
      "Epoch: 90/200, Train Loss: 2.50426, Valid Loss: 2.52406, lr: 0.000220\n",
      "Epoch: 91/200, Train Loss: 2.43572, Valid Loss: 2.42564, lr: 0.000218\n",
      "Epoch: 92/200, Train Loss: 2.38070, Valid Loss: 2.50347, lr: 0.000217\n",
      "Epoch: 93/200, Train Loss: 2.42765, Valid Loss: 2.40782, lr: 0.000215\n",
      "Epoch: 94/200, Train Loss: 2.44470, Valid Loss: 2.48241, lr: 0.000214\n",
      "Epoch: 95/200, Train Loss: 2.40359, Valid Loss: 2.43900, lr: 0.000212\n",
      "Epoch: 96/200, Train Loss: 2.38409, Valid Loss: 2.49593, lr: 0.000210\n",
      "Epoch: 97/200, Train Loss: 2.38193, Valid Loss: 2.38073, lr: 0.000209\n",
      "Epoch: 98/200, Train Loss: 2.38647, Valid Loss: 2.49822, lr: 0.000207\n",
      "Epoch: 99/200, Train Loss: 2.39836, Valid Loss: 2.40432, lr: 0.000206\n",
      "Epoch: 100/200, Train Loss: 2.34253, Valid Loss: 2.38480, lr: 0.000204\n",
      "Epoch: 101/200, Train Loss: 2.38816, Valid Loss: 2.47222, lr: 0.000202\n",
      "Epoch: 102/200, Train Loss: 2.33025, Valid Loss: 2.38954, lr: 0.000201\n",
      "Epoch: 103/200, Train Loss: 2.34515, Valid Loss: 2.39089, lr: 0.000199\n",
      "Epoch: 104/200, Train Loss: 2.29744, Valid Loss: 2.42156, lr: 0.000198\n",
      "Epoch: 105/200, Train Loss: 2.36988, Valid Loss: 2.35336, lr: 0.000196\n",
      "Epoch: 106/200, Train Loss: 2.28488, Valid Loss: 2.53938, lr: 0.000194\n",
      "Epoch: 107/200, Train Loss: 2.32145, Valid Loss: 2.36950, lr: 0.000193\n",
      "Epoch: 108/200, Train Loss: 2.25411, Valid Loss: 2.35614, lr: 0.000191\n",
      "Epoch: 109/200, Train Loss: 2.25988, Valid Loss: 2.45775, lr: 0.000190\n",
      "Epoch: 110/200, Train Loss: 2.31231, Valid Loss: 2.40247, lr: 0.000188\n",
      "Epoch: 111/200, Train Loss: 2.37157, Valid Loss: 2.34901, lr: 0.000186\n",
      "Epoch: 112/200, Train Loss: 2.26824, Valid Loss: 2.36335, lr: 0.000185\n",
      "Epoch: 113/200, Train Loss: 2.30115, Valid Loss: 2.39437, lr: 0.000183\n",
      "Epoch: 114/200, Train Loss: 2.23589, Valid Loss: 2.41587, lr: 0.000182\n",
      "Epoch: 115/200, Train Loss: 2.29416, Valid Loss: 2.37539, lr: 0.000180\n",
      "Epoch: 116/200, Train Loss: 2.27200, Valid Loss: 2.39702, lr: 0.000178\n",
      "Epoch: 117/200, Train Loss: 2.28325, Valid Loss: 2.31944, lr: 0.000177\n",
      "Epoch: 118/200, Train Loss: 2.26651, Valid Loss: 2.37816, lr: 0.000175\n",
      "Epoch: 119/200, Train Loss: 2.25711, Valid Loss: 2.39767, lr: 0.000174\n",
      "Epoch: 120/200, Train Loss: 2.25109, Valid Loss: 2.34467, lr: 0.000172\n",
      "Epoch: 121/200, Train Loss: 2.28936, Valid Loss: 2.33821, lr: 0.000171\n",
      "Epoch: 122/200, Train Loss: 2.20256, Valid Loss: 2.37360, lr: 0.000169\n",
      "Epoch: 123/200, Train Loss: 2.25992, Valid Loss: 2.41749, lr: 0.000168\n",
      "Epoch: 124/200, Train Loss: 2.26299, Valid Loss: 2.32381, lr: 0.000166\n",
      "Epoch: 125/200, Train Loss: 2.24154, Valid Loss: 2.34634, lr: 0.000165\n",
      "Epoch: 126/200, Train Loss: 2.26015, Valid Loss: 2.44148, lr: 0.000163\n",
      "Epoch: 127/200, Train Loss: 2.29231, Valid Loss: 2.37628, lr: 0.000162\n",
      "Epoch: 128/200, Train Loss: 2.23662, Valid Loss: 2.33361, lr: 0.000160\n",
      "Epoch: 129/200, Train Loss: 2.20493, Valid Loss: 2.32836, lr: 0.000159\n",
      "Epoch: 130/200, Train Loss: 2.26785, Valid Loss: 2.45311, lr: 0.000157\n",
      "Epoch: 131/200, Train Loss: 2.18013, Valid Loss: 2.37410, lr: 0.000156\n",
      "Epoch: 132/200, Train Loss: 2.20714, Valid Loss: 2.31719, lr: 0.000154\n",
      "Epoch: 133/200, Train Loss: 2.23733, Valid Loss: 2.47104, lr: 0.000153\n",
      "Epoch: 134/200, Train Loss: 2.20453, Valid Loss: 2.32487, lr: 0.000151\n",
      "Epoch: 135/200, Train Loss: 2.22307, Valid Loss: 2.40898, lr: 0.000150\n",
      "Epoch: 136/200, Train Loss: 2.21250, Valid Loss: 2.31191, lr: 0.000149\n",
      "Epoch: 137/200, Train Loss: 2.21148, Valid Loss: 2.30928, lr: 0.000147\n",
      "Epoch: 138/200, Train Loss: 2.26724, Valid Loss: 2.46721, lr: 0.000146\n",
      "Epoch: 139/200, Train Loss: 2.24518, Valid Loss: 2.38121, lr: 0.000145\n",
      "Epoch: 140/200, Train Loss: 2.28561, Valid Loss: 2.30815, lr: 0.000143\n",
      "Epoch: 141/200, Train Loss: 2.20705, Valid Loss: 2.40866, lr: 0.000142\n",
      "Epoch: 142/200, Train Loss: 2.26432, Valid Loss: 2.40176, lr: 0.000141\n",
      "Epoch: 143/200, Train Loss: 2.21996, Valid Loss: 2.32164, lr: 0.000139\n",
      "Epoch: 144/200, Train Loss: 2.18487, Valid Loss: 2.34353, lr: 0.000138\n",
      "Epoch: 145/200, Train Loss: 2.17887, Valid Loss: 2.46225, lr: 0.000137\n",
      "Epoch: 146/200, Train Loss: 2.19541, Valid Loss: 2.30751, lr: 0.000136\n",
      "Epoch: 147/200, Train Loss: 2.20044, Valid Loss: 2.34871, lr: 0.000134\n",
      "Epoch: 148/200, Train Loss: 2.17873, Valid Loss: 2.35493, lr: 0.000133\n",
      "Epoch: 149/200, Train Loss: 2.17371, Valid Loss: 2.34398, lr: 0.000132\n",
      "Epoch: 150/200, Train Loss: 2.21422, Valid Loss: 2.33108, lr: 0.000131\n",
      "Epoch: 151/200, Train Loss: 2.17769, Valid Loss: 2.33600, lr: 0.000130\n",
      "Epoch: 152/200, Train Loss: 2.18881, Valid Loss: 2.34004, lr: 0.000128\n",
      "Epoch: 153/200, Train Loss: 2.15487, Valid Loss: 2.33517, lr: 0.000127\n",
      "Epoch: 154/200, Train Loss: 2.14149, Valid Loss: 2.36970, lr: 0.000126\n",
      "Epoch: 155/200, Train Loss: 2.19625, Valid Loss: 2.53534, lr: 0.000125\n",
      "Epoch: 156/200, Train Loss: 2.19273, Valid Loss: 2.30619, lr: 0.000124\n",
      "Epoch: 157/200, Train Loss: 2.18055, Valid Loss: 2.36683, lr: 0.000123\n",
      "Epoch: 158/200, Train Loss: 2.15556, Valid Loss: 2.41177, lr: 0.000122\n",
      "Epoch: 159/200, Train Loss: 2.13995, Valid Loss: 2.30880, lr: 0.000121\n",
      "Epoch: 160/200, Train Loss: 2.20600, Valid Loss: 2.50354, lr: 0.000120\n",
      "Epoch: 161/200, Train Loss: 2.19638, Valid Loss: 2.31200, lr: 0.000119\n",
      "Epoch: 162/200, Train Loss: 2.22713, Valid Loss: 2.43351, lr: 0.000118\n",
      "Epoch: 163/200, Train Loss: 2.22024, Valid Loss: 2.34106, lr: 0.000117\n",
      "Epoch: 164/200, Train Loss: 2.18820, Valid Loss: 2.32430, lr: 0.000116\n",
      "Epoch: 165/200, Train Loss: 2.20045, Valid Loss: 2.37023, lr: 0.000115\n",
      "Epoch: 166/200, Train Loss: 2.17280, Valid Loss: 2.34610, lr: 0.000115\n",
      "Epoch: 167/200, Train Loss: 2.12982, Valid Loss: 2.32220, lr: 0.000114\n",
      "Epoch: 168/200, Train Loss: 2.14206, Valid Loss: 2.41985, lr: 0.000113\n",
      "Epoch: 169/200, Train Loss: 2.15461, Valid Loss: 2.31964, lr: 0.000112\n",
      "Epoch: 170/200, Train Loss: 2.15219, Valid Loss: 2.36231, lr: 0.000111\n",
      "Epoch: 171/200, Train Loss: 2.15919, Valid Loss: 2.32295, lr: 0.000111\n",
      "Epoch: 172/200, Train Loss: 2.15714, Valid Loss: 2.39180, lr: 0.000110\n",
      "Epoch: 173/200, Train Loss: 2.15066, Valid Loss: 2.35866, lr: 0.000109\n",
      "Epoch: 174/200, Train Loss: 2.10096, Valid Loss: 2.33124, lr: 0.000109\n",
      "Epoch: 175/200, Train Loss: 2.16595, Valid Loss: 2.35507, lr: 0.000108\n",
      "Epoch: 176/200, Train Loss: 2.14447, Valid Loss: 2.31092, lr: 0.000107\n",
      "Epoch: 177/200, Train Loss: 2.11086, Valid Loss: 2.38572, lr: 0.000107\n",
      "Epoch: 178/200, Train Loss: 2.10000, Valid Loss: 2.31275, lr: 0.000106\n",
      "Epoch: 179/200, Train Loss: 2.13681, Valid Loss: 2.38530, lr: 0.000106\n",
      "Epoch: 180/200, Train Loss: 2.10118, Valid Loss: 2.33439, lr: 0.000105\n",
      "Epoch: 181/200, Train Loss: 2.18936, Valid Loss: 2.38305, lr: 0.000105\n",
      "Epoch: 182/200, Train Loss: 2.15792, Valid Loss: 2.38008, lr: 0.000104\n",
      "Epoch: 183/200, Train Loss: 2.12132, Valid Loss: 2.33102, lr: 0.000104\n",
      "Epoch: 184/200, Train Loss: 2.11885, Valid Loss: 2.36448, lr: 0.000103\n",
      "Epoch: 185/200, Train Loss: 2.12811, Valid Loss: 2.33160, lr: 0.000103\n",
      "Epoch: 186/200, Train Loss: 2.11578, Valid Loss: 2.38841, lr: 0.000103\n",
      "Epoch: 187/200, Train Loss: 2.13909, Valid Loss: 2.31673, lr: 0.000102\n",
      "Epoch: 188/200, Train Loss: 2.16588, Valid Loss: 2.32463, lr: 0.000102\n",
      "Epoch: 189/200, Train Loss: 2.10476, Valid Loss: 2.36896, lr: 0.000102\n",
      "Epoch: 190/200, Train Loss: 2.15613, Valid Loss: 2.32467, lr: 0.000101\n",
      "Epoch: 191/200, Train Loss: 2.11944, Valid Loss: 2.36698, lr: 0.000101\n",
      "Epoch: 192/200, Train Loss: 2.17280, Valid Loss: 2.34731, lr: 0.000101\n",
      "Epoch: 193/200, Train Loss: 2.12004, Valid Loss: 2.35948, lr: 0.000101\n",
      "Epoch: 194/200, Train Loss: 2.10211, Valid Loss: 2.31552, lr: 0.000100\n",
      "Epoch: 195/200, Train Loss: 2.12600, Valid Loss: 2.35514, lr: 0.000100\n",
      "Epoch: 196/200, Train Loss: 2.13049, Valid Loss: 2.32843, lr: 0.000100\n",
      "Epoch: 197/200, Train Loss: 2.10182, Valid Loss: 2.35336, lr: 0.000100\n",
      "Epoch: 198/200, Train Loss: 2.13423, Valid Loss: 2.40074, lr: 0.000100\n",
      "Epoch: 199/200, Train Loss: 2.10768, Valid Loss: 2.31116, lr: 0.000100\n",
      "Epoch: 200/200, Train Loss: 2.15005, Valid Loss: 2.43865, lr: 0.000100\n",
      "\n",
      "===== Fold 2 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_11076\\2664917137.py:52: UnderReviewWarning: The feature LinearWarmupCosineAnnealingLR is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  scheduler = LinearWarmupCosineAnnealingLR(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/200, Train Loss: 67.95778, Valid Loss: 66.81854, lr: 0.000075\n",
      "Epoch: 2/200, Train Loss: 20.22815, Valid Loss: 20.09957, lr: 0.000150\n",
      "Epoch: 3/200, Train Loss: 6.44476, Valid Loss: 5.38516, lr: 0.000225\n",
      "Epoch: 4/200, Train Loss: 4.34215, Valid Loss: 4.41164, lr: 0.000300\n",
      "Epoch: 5/200, Train Loss: 3.85946, Valid Loss: 3.32282, lr: 0.000300\n",
      "Epoch: 6/200, Train Loss: 3.53929, Valid Loss: 2.98586, lr: 0.000300\n",
      "Epoch: 7/200, Train Loss: 3.32443, Valid Loss: 2.89055, lr: 0.000300\n",
      "Epoch: 8/200, Train Loss: 3.23439, Valid Loss: 2.89004, lr: 0.000300\n",
      "Epoch: 9/200, Train Loss: 3.21040, Valid Loss: 2.86760, lr: 0.000300\n",
      "Epoch: 10/200, Train Loss: 3.17071, Valid Loss: 2.88131, lr: 0.000300\n",
      "Epoch: 11/200, Train Loss: 3.20389, Valid Loss: 2.84954, lr: 0.000300\n",
      "Epoch: 12/200, Train Loss: 3.19437, Valid Loss: 2.86416, lr: 0.000299\n",
      "Epoch: 13/200, Train Loss: 3.09680, Valid Loss: 2.83214, lr: 0.000299\n",
      "Epoch: 14/200, Train Loss: 3.13553, Valid Loss: 2.84850, lr: 0.000299\n",
      "Epoch: 15/200, Train Loss: 3.12651, Valid Loss: 2.81443, lr: 0.000299\n",
      "Epoch: 16/200, Train Loss: 3.14933, Valid Loss: 2.81833, lr: 0.000298\n",
      "Epoch: 17/200, Train Loss: 2.99983, Valid Loss: 2.79889, lr: 0.000298\n",
      "Epoch: 18/200, Train Loss: 3.01895, Valid Loss: 2.79651, lr: 0.000298\n",
      "Epoch: 19/200, Train Loss: 3.00987, Valid Loss: 2.78392, lr: 0.000297\n",
      "Epoch: 20/200, Train Loss: 3.03391, Valid Loss: 2.77849, lr: 0.000297\n",
      "Epoch: 21/200, Train Loss: 2.99950, Valid Loss: 2.77780, lr: 0.000297\n",
      "Epoch: 22/200, Train Loss: 2.98580, Valid Loss: 2.76410, lr: 0.000296\n",
      "Epoch: 23/200, Train Loss: 2.99979, Valid Loss: 2.74853, lr: 0.000296\n",
      "Epoch: 24/200, Train Loss: 3.01205, Valid Loss: 2.75236, lr: 0.000295\n",
      "Epoch: 25/200, Train Loss: 2.99772, Valid Loss: 2.73376, lr: 0.000295\n",
      "Epoch: 26/200, Train Loss: 2.94935, Valid Loss: 2.74239, lr: 0.000294\n",
      "Epoch: 27/200, Train Loss: 2.94629, Valid Loss: 2.71755, lr: 0.000294\n",
      "Epoch: 28/200, Train Loss: 2.96502, Valid Loss: 2.71578, lr: 0.000293\n",
      "Epoch: 29/200, Train Loss: 2.87655, Valid Loss: 2.70712, lr: 0.000293\n",
      "Epoch: 30/200, Train Loss: 2.92534, Valid Loss: 2.69437, lr: 0.000292\n",
      "Epoch: 31/200, Train Loss: 2.93766, Valid Loss: 2.69909, lr: 0.000291\n",
      "Epoch: 32/200, Train Loss: 2.87803, Valid Loss: 2.68030, lr: 0.000291\n",
      "Epoch: 33/200, Train Loss: 2.95276, Valid Loss: 2.69557, lr: 0.000290\n",
      "Epoch: 34/200, Train Loss: 2.87381, Valid Loss: 2.66375, lr: 0.000289\n",
      "Epoch: 35/200, Train Loss: 2.81992, Valid Loss: 2.65992, lr: 0.000289\n",
      "Epoch: 36/200, Train Loss: 2.86303, Valid Loss: 2.65046, lr: 0.000288\n",
      "Epoch: 37/200, Train Loss: 2.89095, Valid Loss: 2.65687, lr: 0.000287\n",
      "Epoch: 38/200, Train Loss: 2.84029, Valid Loss: 2.63628, lr: 0.000286\n",
      "Epoch: 39/200, Train Loss: 2.80796, Valid Loss: 2.64512, lr: 0.000285\n",
      "Epoch: 40/200, Train Loss: 2.83714, Valid Loss: 2.62575, lr: 0.000285\n",
      "Epoch: 41/200, Train Loss: 2.83259, Valid Loss: 2.61010, lr: 0.000284\n",
      "Epoch: 42/200, Train Loss: 2.83794, Valid Loss: 2.62115, lr: 0.000283\n",
      "Epoch: 43/200, Train Loss: 2.77269, Valid Loss: 2.59598, lr: 0.000282\n",
      "Epoch: 44/200, Train Loss: 2.80247, Valid Loss: 2.60613, lr: 0.000281\n",
      "Epoch: 45/200, Train Loss: 2.74694, Valid Loss: 2.59183, lr: 0.000280\n",
      "Epoch: 46/200, Train Loss: 2.77453, Valid Loss: 2.57334, lr: 0.000279\n",
      "Epoch: 47/200, Train Loss: 2.73623, Valid Loss: 2.58537, lr: 0.000278\n",
      "Epoch: 48/200, Train Loss: 2.75627, Valid Loss: 2.55819, lr: 0.000277\n",
      "Epoch: 49/200, Train Loss: 2.78853, Valid Loss: 2.57991, lr: 0.000276\n",
      "Epoch: 50/200, Train Loss: 2.73203, Valid Loss: 2.54499, lr: 0.000275\n",
      "Epoch: 51/200, Train Loss: 2.75515, Valid Loss: 2.56799, lr: 0.000274\n",
      "Epoch: 52/200, Train Loss: 2.69511, Valid Loss: 2.53020, lr: 0.000273\n",
      "Epoch: 53/200, Train Loss: 2.75737, Valid Loss: 2.55527, lr: 0.000272\n",
      "Epoch: 54/200, Train Loss: 2.73551, Valid Loss: 2.51592, lr: 0.000270\n",
      "Epoch: 55/200, Train Loss: 2.69426, Valid Loss: 2.52964, lr: 0.000269\n",
      "Epoch: 56/200, Train Loss: 2.66839, Valid Loss: 2.50205, lr: 0.000268\n",
      "Epoch: 57/200, Train Loss: 2.63309, Valid Loss: 2.51308, lr: 0.000267\n",
      "Epoch: 58/200, Train Loss: 2.61946, Valid Loss: 2.48843, lr: 0.000266\n",
      "Epoch: 59/200, Train Loss: 2.60298, Valid Loss: 2.48743, lr: 0.000264\n",
      "Epoch: 60/200, Train Loss: 2.61102, Valid Loss: 2.48735, lr: 0.000263\n",
      "Epoch: 61/200, Train Loss: 2.63090, Valid Loss: 2.46955, lr: 0.000262\n",
      "Epoch: 62/200, Train Loss: 2.59020, Valid Loss: 2.46912, lr: 0.000261\n",
      "Epoch: 63/200, Train Loss: 2.61586, Valid Loss: 2.45323, lr: 0.000259\n",
      "Epoch: 64/200, Train Loss: 2.66298, Valid Loss: 2.46005, lr: 0.000258\n",
      "Epoch: 65/200, Train Loss: 2.56703, Valid Loss: 2.44418, lr: 0.000257\n",
      "Epoch: 66/200, Train Loss: 2.54529, Valid Loss: 2.43757, lr: 0.000255\n",
      "Epoch: 67/200, Train Loss: 2.58189, Valid Loss: 2.42946, lr: 0.000254\n",
      "Epoch: 68/200, Train Loss: 2.58688, Valid Loss: 2.46170, lr: 0.000253\n",
      "Epoch: 69/200, Train Loss: 2.57668, Valid Loss: 2.41604, lr: 0.000251\n",
      "Epoch: 70/200, Train Loss: 2.58356, Valid Loss: 2.42506, lr: 0.000250\n",
      "Epoch: 71/200, Train Loss: 2.57042, Valid Loss: 2.41279, lr: 0.000249\n",
      "Epoch: 72/200, Train Loss: 2.51086, Valid Loss: 2.39707, lr: 0.000247\n",
      "Epoch: 73/200, Train Loss: 2.47469, Valid Loss: 2.43468, lr: 0.000246\n",
      "Epoch: 74/200, Train Loss: 2.53137, Valid Loss: 2.37661, lr: 0.000244\n",
      "Epoch: 75/200, Train Loss: 2.54694, Valid Loss: 2.40311, lr: 0.000243\n",
      "Epoch: 76/200, Train Loss: 2.47684, Valid Loss: 2.36200, lr: 0.000241\n",
      "Epoch: 77/200, Train Loss: 2.52016, Valid Loss: 2.37411, lr: 0.000240\n",
      "Epoch: 78/200, Train Loss: 2.43769, Valid Loss: 2.34711, lr: 0.000238\n",
      "Epoch: 79/200, Train Loss: 2.45107, Valid Loss: 2.34106, lr: 0.000237\n",
      "Epoch: 80/200, Train Loss: 2.47235, Valid Loss: 2.35425, lr: 0.000235\n",
      "Epoch: 81/200, Train Loss: 2.46620, Valid Loss: 2.37103, lr: 0.000234\n",
      "Epoch: 82/200, Train Loss: 2.46697, Valid Loss: 2.35079, lr: 0.000232\n",
      "Epoch: 83/200, Train Loss: 2.46066, Valid Loss: 2.51574, lr: 0.000231\n",
      "Epoch: 84/200, Train Loss: 2.54084, Valid Loss: 2.31925, lr: 0.000229\n",
      "Epoch: 85/200, Train Loss: 2.46830, Valid Loss: 2.31631, lr: 0.000228\n",
      "Epoch: 86/200, Train Loss: 2.39713, Valid Loss: 2.35075, lr: 0.000226\n",
      "Epoch: 87/200, Train Loss: 2.42427, Valid Loss: 2.29351, lr: 0.000225\n",
      "Epoch: 88/200, Train Loss: 2.42202, Valid Loss: 2.29293, lr: 0.000223\n",
      "Epoch: 89/200, Train Loss: 2.37621, Valid Loss: 2.28108, lr: 0.000222\n",
      "Epoch: 90/200, Train Loss: 2.33806, Valid Loss: 2.28370, lr: 0.000220\n",
      "Epoch: 91/200, Train Loss: 2.33337, Valid Loss: 2.26876, lr: 0.000218\n",
      "Epoch: 92/200, Train Loss: 2.37093, Valid Loss: 2.27904, lr: 0.000217\n",
      "Epoch: 93/200, Train Loss: 2.41753, Valid Loss: 2.26175, lr: 0.000215\n",
      "Epoch: 94/200, Train Loss: 2.42519, Valid Loss: 2.24655, lr: 0.000214\n",
      "Epoch: 95/200, Train Loss: 2.38708, Valid Loss: 2.25488, lr: 0.000212\n",
      "Epoch: 96/200, Train Loss: 2.31463, Valid Loss: 2.26170, lr: 0.000210\n",
      "Epoch: 97/200, Train Loss: 2.32164, Valid Loss: 2.22974, lr: 0.000209\n",
      "Epoch: 98/200, Train Loss: 2.40140, Valid Loss: 2.26677, lr: 0.000207\n",
      "Epoch: 99/200, Train Loss: 2.33551, Valid Loss: 2.23347, lr: 0.000206\n",
      "Epoch: 100/200, Train Loss: 2.32721, Valid Loss: 2.22107, lr: 0.000204\n",
      "Epoch: 101/200, Train Loss: 2.33959, Valid Loss: 2.21297, lr: 0.000202\n",
      "Epoch: 102/200, Train Loss: 2.35366, Valid Loss: 2.38282, lr: 0.000201\n",
      "Epoch: 103/200, Train Loss: 2.36325, Valid Loss: 2.20480, lr: 0.000199\n",
      "Epoch: 104/200, Train Loss: 2.31516, Valid Loss: 2.22186, lr: 0.000198\n",
      "Epoch: 105/200, Train Loss: 2.32265, Valid Loss: 2.20939, lr: 0.000196\n",
      "Epoch: 106/200, Train Loss: 2.33799, Valid Loss: 2.24176, lr: 0.000194\n",
      "Epoch: 107/200, Train Loss: 2.26759, Valid Loss: 2.19088, lr: 0.000193\n",
      "Epoch: 108/200, Train Loss: 2.30703, Valid Loss: 2.20265, lr: 0.000191\n",
      "Epoch: 109/200, Train Loss: 2.29098, Valid Loss: 2.22877, lr: 0.000190\n",
      "Epoch: 110/200, Train Loss: 2.24109, Valid Loss: 2.18421, lr: 0.000188\n",
      "Epoch: 111/200, Train Loss: 2.30474, Valid Loss: 2.28318, lr: 0.000186\n",
      "Epoch: 112/200, Train Loss: 2.30834, Valid Loss: 2.20699, lr: 0.000185\n",
      "Epoch: 113/200, Train Loss: 2.30035, Valid Loss: 2.19106, lr: 0.000183\n",
      "Epoch: 114/200, Train Loss: 2.30410, Valid Loss: 2.27289, lr: 0.000182\n",
      "Epoch: 115/200, Train Loss: 2.29599, Valid Loss: 2.17573, lr: 0.000180\n",
      "Epoch: 116/200, Train Loss: 2.25962, Valid Loss: 2.16345, lr: 0.000178\n",
      "Epoch: 117/200, Train Loss: 2.28948, Valid Loss: 2.34236, lr: 0.000177\n",
      "Epoch: 118/200, Train Loss: 2.27862, Valid Loss: 2.16056, lr: 0.000175\n",
      "Epoch: 119/200, Train Loss: 2.24638, Valid Loss: 2.16982, lr: 0.000174\n",
      "Epoch: 120/200, Train Loss: 2.27120, Valid Loss: 2.22045, lr: 0.000172\n",
      "Epoch: 121/200, Train Loss: 2.28151, Valid Loss: 2.15580, lr: 0.000171\n",
      "Epoch: 122/200, Train Loss: 2.25085, Valid Loss: 2.28311, lr: 0.000169\n",
      "Epoch: 123/200, Train Loss: 2.27659, Valid Loss: 2.16019, lr: 0.000168\n",
      "Epoch: 124/200, Train Loss: 2.27425, Valid Loss: 2.16671, lr: 0.000166\n",
      "Epoch: 125/200, Train Loss: 2.23630, Valid Loss: 2.20091, lr: 0.000165\n",
      "Epoch: 126/200, Train Loss: 2.21648, Valid Loss: 2.14784, lr: 0.000163\n",
      "Epoch: 127/200, Train Loss: 2.19138, Valid Loss: 2.21185, lr: 0.000162\n",
      "Epoch: 128/200, Train Loss: 2.24716, Valid Loss: 2.24010, lr: 0.000160\n",
      "Epoch: 129/200, Train Loss: 2.28824, Valid Loss: 2.19199, lr: 0.000159\n",
      "Epoch: 130/200, Train Loss: 2.35412, Valid Loss: 2.25850, lr: 0.000157\n",
      "Epoch: 131/200, Train Loss: 2.25297, Valid Loss: 2.15048, lr: 0.000156\n",
      "Epoch: 132/200, Train Loss: 2.26889, Valid Loss: 2.14182, lr: 0.000154\n",
      "Epoch: 133/200, Train Loss: 2.21626, Valid Loss: 2.14989, lr: 0.000153\n",
      "Epoch: 134/200, Train Loss: 2.23236, Valid Loss: 2.13967, lr: 0.000151\n",
      "Epoch: 135/200, Train Loss: 2.21444, Valid Loss: 2.20208, lr: 0.000150\n",
      "Epoch: 136/200, Train Loss: 2.23610, Valid Loss: 2.14259, lr: 0.000149\n",
      "Epoch: 137/200, Train Loss: 2.21650, Valid Loss: 2.20157, lr: 0.000147\n",
      "Epoch: 138/200, Train Loss: 2.21191, Valid Loss: 2.14327, lr: 0.000146\n",
      "Epoch: 139/200, Train Loss: 2.19926, Valid Loss: 2.33720, lr: 0.000145\n",
      "Epoch: 140/200, Train Loss: 2.23290, Valid Loss: 2.13687, lr: 0.000143\n",
      "Epoch: 141/200, Train Loss: 2.22945, Valid Loss: 2.20114, lr: 0.000142\n",
      "Epoch: 142/200, Train Loss: 2.23863, Valid Loss: 2.18735, lr: 0.000141\n",
      "Epoch: 143/200, Train Loss: 2.24153, Valid Loss: 2.13131, lr: 0.000139\n",
      "Epoch: 144/200, Train Loss: 2.23053, Valid Loss: 2.17949, lr: 0.000138\n",
      "Epoch: 145/200, Train Loss: 2.21295, Valid Loss: 2.13795, lr: 0.000137\n",
      "Epoch: 146/200, Train Loss: 2.22176, Valid Loss: 2.13759, lr: 0.000136\n",
      "Epoch: 147/200, Train Loss: 2.16517, Valid Loss: 2.14298, lr: 0.000134\n",
      "Epoch: 148/200, Train Loss: 2.20602, Valid Loss: 2.26203, lr: 0.000133\n",
      "Epoch: 149/200, Train Loss: 2.21181, Valid Loss: 2.13397, lr: 0.000132\n",
      "Epoch: 150/200, Train Loss: 2.33642, Valid Loss: 2.21280, lr: 0.000131\n",
      "Epoch: 151/200, Train Loss: 2.23997, Valid Loss: 2.18006, lr: 0.000130\n",
      "Epoch: 152/200, Train Loss: 2.20261, Valid Loss: 2.12969, lr: 0.000128\n",
      "Epoch: 153/200, Train Loss: 2.24370, Valid Loss: 2.18375, lr: 0.000127\n",
      "Epoch: 154/200, Train Loss: 2.20803, Valid Loss: 2.13648, lr: 0.000126\n",
      "Epoch: 155/200, Train Loss: 2.24426, Valid Loss: 2.13447, lr: 0.000125\n",
      "Epoch: 156/200, Train Loss: 2.19985, Valid Loss: 2.15013, lr: 0.000124\n",
      "Epoch: 157/200, Train Loss: 2.17371, Valid Loss: 2.13331, lr: 0.000123\n",
      "Epoch: 158/200, Train Loss: 2.16148, Valid Loss: 2.13289, lr: 0.000122\n",
      "Epoch: 159/200, Train Loss: 2.17578, Valid Loss: 2.12781, lr: 0.000121\n",
      "Epoch: 160/200, Train Loss: 2.21515, Valid Loss: 2.16356, lr: 0.000120\n",
      "Epoch: 161/200, Train Loss: 2.17591, Valid Loss: 2.12322, lr: 0.000119\n",
      "Epoch: 162/200, Train Loss: 2.17547, Valid Loss: 2.14889, lr: 0.000118\n",
      "Epoch: 163/200, Train Loss: 2.12208, Valid Loss: 2.13373, lr: 0.000117\n",
      "Epoch: 164/200, Train Loss: 2.17007, Valid Loss: 2.13969, lr: 0.000116\n",
      "Epoch: 165/200, Train Loss: 2.17020, Valid Loss: 2.12403, lr: 0.000115\n",
      "Epoch: 166/200, Train Loss: 2.12397, Valid Loss: 2.17559, lr: 0.000115\n",
      "Epoch: 167/200, Train Loss: 2.18471, Valid Loss: 2.11433, lr: 0.000114\n",
      "Epoch: 168/200, Train Loss: 2.21560, Valid Loss: 2.26432, lr: 0.000113\n",
      "Epoch: 169/200, Train Loss: 2.26432, Valid Loss: 2.11304, lr: 0.000112\n",
      "Epoch: 170/200, Train Loss: 2.18320, Valid Loss: 2.20279, lr: 0.000111\n",
      "Epoch: 171/200, Train Loss: 2.15754, Valid Loss: 2.11452, lr: 0.000111\n",
      "Epoch: 172/200, Train Loss: 2.16118, Valid Loss: 2.14213, lr: 0.000110\n",
      "Epoch: 173/200, Train Loss: 2.16331, Valid Loss: 2.14645, lr: 0.000109\n",
      "Epoch: 174/200, Train Loss: 2.17342, Valid Loss: 2.12188, lr: 0.000109\n",
      "Epoch: 175/200, Train Loss: 2.16700, Valid Loss: 2.12994, lr: 0.000108\n",
      "Epoch: 176/200, Train Loss: 2.12895, Valid Loss: 2.21145, lr: 0.000107\n",
      "Epoch: 177/200, Train Loss: 2.17578, Valid Loss: 2.11829, lr: 0.000107\n",
      "Epoch: 178/200, Train Loss: 2.17499, Valid Loss: 2.16664, lr: 0.000106\n",
      "Epoch: 179/200, Train Loss: 2.14887, Valid Loss: 2.17031, lr: 0.000106\n",
      "Epoch: 180/200, Train Loss: 2.16220, Valid Loss: 2.10961, lr: 0.000105\n",
      "Epoch: 181/200, Train Loss: 2.14086, Valid Loss: 2.22057, lr: 0.000105\n",
      "Epoch: 182/200, Train Loss: 2.14470, Valid Loss: 2.11156, lr: 0.000104\n",
      "Epoch: 183/200, Train Loss: 2.15517, Valid Loss: 2.19410, lr: 0.000104\n",
      "Epoch: 184/200, Train Loss: 2.15626, Valid Loss: 2.11004, lr: 0.000103\n",
      "Epoch: 185/200, Train Loss: 2.21924, Valid Loss: 2.15832, lr: 0.000103\n",
      "Epoch: 186/200, Train Loss: 2.15233, Valid Loss: 2.13642, lr: 0.000103\n",
      "Epoch: 187/200, Train Loss: 2.15756, Valid Loss: 2.11310, lr: 0.000102\n",
      "Epoch: 188/200, Train Loss: 2.18767, Valid Loss: 2.17838, lr: 0.000102\n",
      "Epoch: 189/200, Train Loss: 2.12240, Valid Loss: 2.12532, lr: 0.000102\n",
      "Epoch: 190/200, Train Loss: 2.14550, Valid Loss: 2.12797, lr: 0.000101\n",
      "Epoch: 191/200, Train Loss: 2.13681, Valid Loss: 2.15349, lr: 0.000101\n",
      "Epoch: 192/200, Train Loss: 2.13205, Valid Loss: 2.12880, lr: 0.000101\n",
      "Epoch: 193/200, Train Loss: 2.18712, Valid Loss: 2.15591, lr: 0.000101\n",
      "Epoch: 194/200, Train Loss: 2.15969, Valid Loss: 2.13470, lr: 0.000100\n",
      "Epoch: 195/200, Train Loss: 2.11414, Valid Loss: 2.12817, lr: 0.000100\n",
      "Epoch: 196/200, Train Loss: 2.13607, Valid Loss: 2.11438, lr: 0.000100\n",
      "Epoch: 197/200, Train Loss: 2.13881, Valid Loss: 2.18003, lr: 0.000100\n",
      "Epoch: 198/200, Train Loss: 2.09752, Valid Loss: 2.10621, lr: 0.000100\n",
      "Epoch: 199/200, Train Loss: 2.19089, Valid Loss: 2.17471, lr: 0.000100\n",
      "Epoch: 200/200, Train Loss: 2.16767, Valid Loss: 2.12996, lr: 0.000100\n",
      "\n",
      "===== Fold 3 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_11076\\2664917137.py:52: UnderReviewWarning: The feature LinearWarmupCosineAnnealingLR is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  scheduler = LinearWarmupCosineAnnealingLR(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/200, Train Loss: 68.28238, Valid Loss: 64.04412, lr: 0.000075\n",
      "Epoch: 2/200, Train Loss: 19.57488, Valid Loss: 20.67774, lr: 0.000150\n",
      "Epoch: 3/200, Train Loss: 6.43582, Valid Loss: 4.16690, lr: 0.000225\n",
      "Epoch: 4/200, Train Loss: 4.27180, Valid Loss: 5.14028, lr: 0.000300\n",
      "Epoch: 5/200, Train Loss: 3.66545, Valid Loss: 2.99824, lr: 0.000300\n",
      "Epoch: 6/200, Train Loss: 3.50089, Valid Loss: 3.44145, lr: 0.000300\n",
      "Epoch: 7/200, Train Loss: 3.45023, Valid Loss: 2.83242, lr: 0.000300\n",
      "Epoch: 8/200, Train Loss: 3.28512, Valid Loss: 2.94415, lr: 0.000300\n",
      "Epoch: 9/200, Train Loss: 3.19104, Valid Loss: 2.85889, lr: 0.000300\n",
      "Epoch: 10/200, Train Loss: 3.18663, Valid Loss: 2.84769, lr: 0.000300\n",
      "Epoch: 11/200, Train Loss: 3.27620, Valid Loss: 2.86093, lr: 0.000300\n",
      "Epoch: 12/200, Train Loss: 3.15967, Valid Loss: 2.81394, lr: 0.000299\n",
      "Epoch: 13/200, Train Loss: 3.10630, Valid Loss: 2.88510, lr: 0.000299\n",
      "Epoch: 14/200, Train Loss: 3.07606, Valid Loss: 2.78321, lr: 0.000299\n",
      "Epoch: 15/200, Train Loss: 3.07602, Valid Loss: 2.83638, lr: 0.000299\n",
      "Epoch: 16/200, Train Loss: 3.10088, Valid Loss: 2.78436, lr: 0.000298\n",
      "Epoch: 17/200, Train Loss: 3.07157, Valid Loss: 2.79448, lr: 0.000298\n",
      "Epoch: 18/200, Train Loss: 3.05469, Valid Loss: 2.82871, lr: 0.000298\n",
      "Epoch: 19/200, Train Loss: 3.07287, Valid Loss: 2.75852, lr: 0.000297\n",
      "Epoch: 20/200, Train Loss: 3.05242, Valid Loss: 2.75983, lr: 0.000297\n",
      "Epoch: 21/200, Train Loss: 3.06116, Valid Loss: 2.75915, lr: 0.000297\n",
      "Epoch: 22/200, Train Loss: 3.01456, Valid Loss: 2.76880, lr: 0.000296\n",
      "Epoch: 23/200, Train Loss: 3.02733, Valid Loss: 2.73006, lr: 0.000296\n",
      "Epoch: 24/200, Train Loss: 2.98397, Valid Loss: 2.72957, lr: 0.000295\n",
      "Epoch: 25/200, Train Loss: 3.00271, Valid Loss: 2.78881, lr: 0.000295\n",
      "Epoch: 26/200, Train Loss: 2.99018, Valid Loss: 2.68886, lr: 0.000294\n",
      "Epoch: 27/200, Train Loss: 2.91127, Valid Loss: 2.73271, lr: 0.000294\n",
      "Epoch: 28/200, Train Loss: 2.95788, Valid Loss: 2.71011, lr: 0.000293\n",
      "Epoch: 29/200, Train Loss: 2.95643, Valid Loss: 2.75690, lr: 0.000293\n",
      "Epoch: 30/200, Train Loss: 2.93078, Valid Loss: 2.67719, lr: 0.000292\n",
      "Epoch: 31/200, Train Loss: 2.97665, Valid Loss: 2.67995, lr: 0.000291\n",
      "Epoch: 32/200, Train Loss: 2.88238, Valid Loss: 2.66527, lr: 0.000291\n",
      "Epoch: 33/200, Train Loss: 2.93702, Valid Loss: 2.65756, lr: 0.000290\n",
      "Epoch: 34/200, Train Loss: 2.95455, Valid Loss: 2.72879, lr: 0.000289\n",
      "Epoch: 35/200, Train Loss: 2.91767, Valid Loss: 2.61825, lr: 0.000289\n",
      "Epoch: 36/200, Train Loss: 2.90314, Valid Loss: 2.71586, lr: 0.000288\n",
      "Epoch: 37/200, Train Loss: 2.86713, Valid Loss: 2.60751, lr: 0.000287\n",
      "Epoch: 38/200, Train Loss: 2.79949, Valid Loss: 2.67330, lr: 0.000286\n",
      "Epoch: 39/200, Train Loss: 2.84962, Valid Loss: 2.59284, lr: 0.000285\n",
      "Epoch: 40/200, Train Loss: 2.82316, Valid Loss: 2.67950, lr: 0.000285\n",
      "Epoch: 41/200, Train Loss: 2.87238, Valid Loss: 2.61129, lr: 0.000284\n",
      "Epoch: 42/200, Train Loss: 2.78473, Valid Loss: 2.62323, lr: 0.000283\n",
      "Epoch: 43/200, Train Loss: 2.79554, Valid Loss: 2.60906, lr: 0.000282\n",
      "Epoch: 44/200, Train Loss: 2.72332, Valid Loss: 2.57379, lr: 0.000281\n",
      "Epoch: 45/200, Train Loss: 2.81469, Valid Loss: 2.60605, lr: 0.000280\n",
      "Epoch: 46/200, Train Loss: 2.70266, Valid Loss: 2.59215, lr: 0.000279\n",
      "Epoch: 47/200, Train Loss: 2.77087, Valid Loss: 2.60130, lr: 0.000278\n",
      "Epoch: 48/200, Train Loss: 2.72674, Valid Loss: 2.53341, lr: 0.000277\n",
      "Epoch: 49/200, Train Loss: 2.75412, Valid Loss: 2.66659, lr: 0.000276\n",
      "Epoch: 50/200, Train Loss: 2.74870, Valid Loss: 2.51125, lr: 0.000275\n",
      "Epoch: 51/200, Train Loss: 2.74275, Valid Loss: 2.63660, lr: 0.000274\n",
      "Epoch: 52/200, Train Loss: 2.67401, Valid Loss: 2.50062, lr: 0.000273\n",
      "Epoch: 53/200, Train Loss: 2.72558, Valid Loss: 2.67595, lr: 0.000272\n",
      "Epoch: 54/200, Train Loss: 2.73732, Valid Loss: 2.48677, lr: 0.000270\n",
      "Epoch: 55/200, Train Loss: 2.71590, Valid Loss: 2.62749, lr: 0.000269\n",
      "Epoch: 56/200, Train Loss: 2.69975, Valid Loss: 2.47052, lr: 0.000268\n",
      "Epoch: 57/200, Train Loss: 2.67314, Valid Loss: 2.52494, lr: 0.000267\n",
      "Epoch: 58/200, Train Loss: 2.67339, Valid Loss: 2.54165, lr: 0.000266\n",
      "Epoch: 59/200, Train Loss: 2.61052, Valid Loss: 2.49183, lr: 0.000264\n",
      "Epoch: 60/200, Train Loss: 2.63636, Valid Loss: 2.55009, lr: 0.000263\n",
      "Epoch: 61/200, Train Loss: 2.64611, Valid Loss: 2.46453, lr: 0.000262\n",
      "Epoch: 62/200, Train Loss: 2.64048, Valid Loss: 2.47804, lr: 0.000261\n",
      "Epoch: 63/200, Train Loss: 2.59261, Valid Loss: 2.44511, lr: 0.000259\n",
      "Epoch: 64/200, Train Loss: 2.59157, Valid Loss: 2.52671, lr: 0.000258\n",
      "Epoch: 65/200, Train Loss: 2.57903, Valid Loss: 2.43308, lr: 0.000257\n",
      "Epoch: 66/200, Train Loss: 2.60958, Valid Loss: 2.43091, lr: 0.000255\n",
      "Epoch: 67/200, Train Loss: 2.54076, Valid Loss: 2.43584, lr: 0.000254\n",
      "Epoch: 68/200, Train Loss: 2.59536, Valid Loss: 2.39765, lr: 0.000253\n",
      "Epoch: 69/200, Train Loss: 2.55398, Valid Loss: 2.52481, lr: 0.000251\n",
      "Epoch: 70/200, Train Loss: 2.51344, Valid Loss: 2.38562, lr: 0.000250\n",
      "Epoch: 71/200, Train Loss: 2.49811, Valid Loss: 2.47737, lr: 0.000249\n",
      "Epoch: 72/200, Train Loss: 2.53365, Valid Loss: 2.39480, lr: 0.000247\n",
      "Epoch: 73/200, Train Loss: 2.48516, Valid Loss: 2.47860, lr: 0.000246\n",
      "Epoch: 74/200, Train Loss: 2.53118, Valid Loss: 2.48749, lr: 0.000244\n",
      "Epoch: 75/200, Train Loss: 2.52597, Valid Loss: 2.34952, lr: 0.000243\n",
      "Epoch: 76/200, Train Loss: 2.56394, Valid Loss: 2.43052, lr: 0.000241\n",
      "Epoch: 77/200, Train Loss: 2.48752, Valid Loss: 2.43352, lr: 0.000240\n",
      "Epoch: 78/200, Train Loss: 2.46803, Valid Loss: 2.33593, lr: 0.000238\n",
      "Epoch: 79/200, Train Loss: 2.46115, Valid Loss: 2.42299, lr: 0.000237\n",
      "Epoch: 80/200, Train Loss: 2.48345, Valid Loss: 2.44006, lr: 0.000235\n",
      "Epoch: 81/200, Train Loss: 2.43686, Valid Loss: 2.33229, lr: 0.000234\n",
      "Epoch: 82/200, Train Loss: 2.42293, Valid Loss: 2.38945, lr: 0.000232\n",
      "Epoch: 83/200, Train Loss: 2.42829, Valid Loss: 2.33770, lr: 0.000231\n",
      "Epoch: 84/200, Train Loss: 2.33521, Valid Loss: 2.35132, lr: 0.000229\n",
      "Epoch: 85/200, Train Loss: 2.39482, Valid Loss: 2.34984, lr: 0.000228\n",
      "Epoch: 86/200, Train Loss: 2.34992, Valid Loss: 2.33646, lr: 0.000226\n",
      "Epoch: 87/200, Train Loss: 2.39146, Valid Loss: 2.38055, lr: 0.000225\n",
      "Epoch: 88/200, Train Loss: 2.39100, Valid Loss: 2.42682, lr: 0.000223\n",
      "Epoch: 89/200, Train Loss: 2.41055, Valid Loss: 2.30712, lr: 0.000222\n",
      "Epoch: 90/200, Train Loss: 2.28206, Valid Loss: 2.32834, lr: 0.000220\n",
      "Epoch: 91/200, Train Loss: 2.34609, Valid Loss: 2.34195, lr: 0.000218\n",
      "Epoch: 92/200, Train Loss: 2.30691, Valid Loss: 2.39456, lr: 0.000217\n",
      "Epoch: 93/200, Train Loss: 2.34566, Valid Loss: 2.28581, lr: 0.000215\n",
      "Epoch: 94/200, Train Loss: 2.37680, Valid Loss: 2.28954, lr: 0.000214\n",
      "Epoch: 95/200, Train Loss: 2.27634, Valid Loss: 2.31633, lr: 0.000212\n",
      "Epoch: 96/200, Train Loss: 2.31325, Valid Loss: 2.33311, lr: 0.000210\n",
      "Epoch: 97/200, Train Loss: 2.27747, Valid Loss: 2.37703, lr: 0.000209\n",
      "Epoch: 98/200, Train Loss: 2.29831, Valid Loss: 2.26488, lr: 0.000207\n",
      "Epoch: 99/200, Train Loss: 2.31538, Valid Loss: 2.32695, lr: 0.000206\n",
      "Epoch: 100/200, Train Loss: 2.32027, Valid Loss: 2.66146, lr: 0.000204\n",
      "Epoch: 101/200, Train Loss: 2.34048, Valid Loss: 2.28797, lr: 0.000202\n",
      "Epoch: 102/200, Train Loss: 2.24771, Valid Loss: 2.28979, lr: 0.000201\n",
      "Epoch: 103/200, Train Loss: 2.26696, Valid Loss: 2.29041, lr: 0.000199\n",
      "Epoch: 104/200, Train Loss: 2.29747, Valid Loss: 2.29025, lr: 0.000198\n",
      "Epoch: 105/200, Train Loss: 2.29585, Valid Loss: 2.42670, lr: 0.000196\n",
      "Epoch: 106/200, Train Loss: 2.24466, Valid Loss: 2.29982, lr: 0.000194\n",
      "Epoch: 107/200, Train Loss: 2.23737, Valid Loss: 2.29282, lr: 0.000193\n",
      "Epoch: 108/200, Train Loss: 2.25721, Valid Loss: 2.45613, lr: 0.000191\n",
      "Epoch: 109/200, Train Loss: 2.31038, Valid Loss: 2.34727, lr: 0.000190\n",
      "Epoch: 110/200, Train Loss: 2.22476, Valid Loss: 2.32076, lr: 0.000188\n",
      "Epoch: 111/200, Train Loss: 2.28754, Valid Loss: 2.26076, lr: 0.000186\n",
      "Epoch: 112/200, Train Loss: 2.24418, Valid Loss: 2.52746, lr: 0.000185\n",
      "Epoch: 113/200, Train Loss: 2.22650, Valid Loss: 2.25938, lr: 0.000183\n",
      "Epoch: 114/200, Train Loss: 2.21026, Valid Loss: 2.31831, lr: 0.000182\n",
      "Epoch: 115/200, Train Loss: 2.18642, Valid Loss: 2.29741, lr: 0.000180\n",
      "Epoch: 116/200, Train Loss: 2.19629, Valid Loss: 2.30718, lr: 0.000178\n",
      "Epoch: 117/200, Train Loss: 2.21945, Valid Loss: 2.27688, lr: 0.000177\n",
      "Epoch: 118/200, Train Loss: 2.20574, Valid Loss: 2.29114, lr: 0.000175\n",
      "Epoch: 119/200, Train Loss: 2.18222, Valid Loss: 2.27901, lr: 0.000174\n",
      "Epoch: 120/200, Train Loss: 2.17913, Valid Loss: 2.35156, lr: 0.000172\n",
      "Epoch: 121/200, Train Loss: 2.23481, Valid Loss: 2.31169, lr: 0.000171\n",
      "Epoch: 122/200, Train Loss: 2.20557, Valid Loss: 2.39359, lr: 0.000169\n",
      "Epoch: 123/200, Train Loss: 2.21244, Valid Loss: 2.32682, lr: 0.000168\n",
      "Epoch: 124/200, Train Loss: 2.18054, Valid Loss: 2.25632, lr: 0.000166\n",
      "Epoch: 125/200, Train Loss: 2.22196, Valid Loss: 2.25524, lr: 0.000165\n",
      "Epoch: 126/200, Train Loss: 2.23628, Valid Loss: 2.31659, lr: 0.000163\n",
      "Epoch: 127/200, Train Loss: 2.21459, Valid Loss: 2.27938, lr: 0.000162\n",
      "Epoch: 128/200, Train Loss: 2.19314, Valid Loss: 2.30286, lr: 0.000160\n",
      "Epoch: 129/200, Train Loss: 2.24615, Valid Loss: 2.43717, lr: 0.000159\n",
      "Epoch: 130/200, Train Loss: 2.23851, Valid Loss: 2.48570, lr: 0.000157\n",
      "Epoch: 131/200, Train Loss: 2.26938, Valid Loss: 2.23822, lr: 0.000156\n",
      "Epoch: 132/200, Train Loss: 2.18032, Valid Loss: 2.26501, lr: 0.000154\n",
      "Epoch: 133/200, Train Loss: 2.20024, Valid Loss: 2.42696, lr: 0.000153\n",
      "Epoch: 134/200, Train Loss: 2.16098, Valid Loss: 2.28212, lr: 0.000151\n",
      "Epoch: 135/200, Train Loss: 2.15894, Valid Loss: 2.26424, lr: 0.000150\n",
      "Epoch: 136/200, Train Loss: 2.17344, Valid Loss: 2.35665, lr: 0.000149\n",
      "Epoch: 137/200, Train Loss: 2.11622, Valid Loss: 2.32686, lr: 0.000147\n",
      "Epoch: 138/200, Train Loss: 2.17242, Valid Loss: 2.25597, lr: 0.000146\n",
      "Epoch: 139/200, Train Loss: 2.16633, Valid Loss: 2.41963, lr: 0.000145\n",
      "Epoch: 140/200, Train Loss: 2.17794, Valid Loss: 2.24232, lr: 0.000143\n",
      "Epoch: 141/200, Train Loss: 2.16192, Valid Loss: 2.25519, lr: 0.000142\n",
      "Epoch: 142/200, Train Loss: 2.14247, Valid Loss: 2.47923, lr: 0.000141\n",
      "Epoch: 143/200, Train Loss: 2.16462, Valid Loss: 2.25274, lr: 0.000139\n",
      "Epoch: 144/200, Train Loss: 2.14762, Valid Loss: 2.32252, lr: 0.000138\n",
      "Epoch: 145/200, Train Loss: 2.12863, Valid Loss: 2.32461, lr: 0.000137\n",
      "Epoch: 146/200, Train Loss: 2.13316, Valid Loss: 2.26394, lr: 0.000136\n",
      "Epoch: 147/200, Train Loss: 2.13641, Valid Loss: 2.30334, lr: 0.000134\n",
      "Epoch: 148/200, Train Loss: 2.11558, Valid Loss: 2.29504, lr: 0.000133\n",
      "Epoch: 149/200, Train Loss: 2.14138, Valid Loss: 2.43975, lr: 0.000132\n",
      "Epoch: 150/200, Train Loss: 2.16726, Valid Loss: 2.23483, lr: 0.000131\n",
      "Epoch: 151/200, Train Loss: 2.19209, Valid Loss: 2.34656, lr: 0.000130\n",
      "Epoch: 152/200, Train Loss: 2.11983, Valid Loss: 2.28136, lr: 0.000128\n",
      "Epoch: 153/200, Train Loss: 2.09703, Valid Loss: 2.35178, lr: 0.000127\n",
      "Epoch: 154/200, Train Loss: 2.14981, Valid Loss: 2.37512, lr: 0.000126\n",
      "Epoch: 155/200, Train Loss: 2.11667, Valid Loss: 2.32914, lr: 0.000125\n",
      "Epoch: 156/200, Train Loss: 2.15931, Valid Loss: 2.27722, lr: 0.000124\n",
      "Epoch: 157/200, Train Loss: 2.13596, Valid Loss: 2.27510, lr: 0.000123\n",
      "Epoch: 158/200, Train Loss: 2.10184, Valid Loss: 2.36382, lr: 0.000122\n",
      "Epoch: 159/200, Train Loss: 2.08046, Valid Loss: 2.31014, lr: 0.000121\n",
      "Epoch: 160/200, Train Loss: 2.10996, Valid Loss: 2.26110, lr: 0.000120\n",
      "Epoch: 161/200, Train Loss: 2.14170, Valid Loss: 2.41696, lr: 0.000119\n",
      "Epoch: 162/200, Train Loss: 2.14640, Valid Loss: 2.26665, lr: 0.000118\n",
      "Epoch: 163/200, Train Loss: 2.16352, Valid Loss: 2.30576, lr: 0.000117\n",
      "Epoch: 164/200, Train Loss: 2.16805, Valid Loss: 2.27133, lr: 0.000116\n",
      "Epoch: 165/200, Train Loss: 2.15682, Valid Loss: 2.27389, lr: 0.000115\n",
      "Epoch: 166/200, Train Loss: 2.08362, Valid Loss: 2.39056, lr: 0.000115\n",
      "Epoch: 167/200, Train Loss: 2.10845, Valid Loss: 2.24385, lr: 0.000114\n",
      "Epoch: 168/200, Train Loss: 2.15204, Valid Loss: 2.38100, lr: 0.000113\n",
      "Epoch: 169/200, Train Loss: 2.08411, Valid Loss: 2.27197, lr: 0.000112\n",
      "Epoch: 170/200, Train Loss: 2.12452, Valid Loss: 2.29150, lr: 0.000111\n",
      "Epoch: 171/200, Train Loss: 2.10844, Valid Loss: 2.36403, lr: 0.000111\n",
      "Epoch: 172/200, Train Loss: 2.13297, Valid Loss: 2.28568, lr: 0.000110\n",
      "Epoch: 173/200, Train Loss: 2.11023, Valid Loss: 2.26371, lr: 0.000109\n",
      "Epoch: 174/200, Train Loss: 2.10733, Valid Loss: 2.32894, lr: 0.000109\n",
      "Epoch: 175/200, Train Loss: 2.11591, Valid Loss: 2.24838, lr: 0.000108\n",
      "Epoch: 176/200, Train Loss: 2.15449, Valid Loss: 2.39260, lr: 0.000107\n",
      "Epoch: 177/200, Train Loss: 2.12910, Valid Loss: 2.25949, lr: 0.000107\n",
      "Epoch: 178/200, Train Loss: 2.09356, Valid Loss: 2.30299, lr: 0.000106\n",
      "Epoch: 179/200, Train Loss: 2.09765, Valid Loss: 2.32675, lr: 0.000106\n",
      "Epoch: 180/200, Train Loss: 2.10725, Valid Loss: 2.28353, lr: 0.000105\n",
      "Epoch: 181/200, Train Loss: 2.07584, Valid Loss: 2.31754, lr: 0.000105\n",
      "Epoch: 182/200, Train Loss: 2.07619, Valid Loss: 2.28426, lr: 0.000104\n",
      "Epoch: 183/200, Train Loss: 2.09404, Valid Loss: 2.29127, lr: 0.000104\n",
      "Epoch: 184/200, Train Loss: 2.12596, Valid Loss: 2.29459, lr: 0.000103\n",
      "Epoch: 185/200, Train Loss: 2.11000, Valid Loss: 2.25121, lr: 0.000103\n",
      "Epoch: 186/200, Train Loss: 2.11224, Valid Loss: 2.32722, lr: 0.000103\n",
      "Epoch: 187/200, Train Loss: 2.08830, Valid Loss: 2.34270, lr: 0.000102\n",
      "Epoch: 188/200, Train Loss: 2.07163, Valid Loss: 2.28493, lr: 0.000102\n",
      "Epoch: 189/200, Train Loss: 2.07628, Valid Loss: 2.35186, lr: 0.000102\n",
      "Epoch: 190/200, Train Loss: 2.10569, Valid Loss: 2.24272, lr: 0.000101\n",
      "Epoch: 191/200, Train Loss: 2.10525, Valid Loss: 2.27840, lr: 0.000101\n",
      "Epoch: 192/200, Train Loss: 2.12047, Valid Loss: 2.32329, lr: 0.000101\n",
      "Epoch: 193/200, Train Loss: 2.05079, Valid Loss: 2.28779, lr: 0.000101\n",
      "Epoch: 194/200, Train Loss: 2.08619, Valid Loss: 2.28314, lr: 0.000100\n",
      "Epoch: 195/200, Train Loss: 2.07845, Valid Loss: 2.37207, lr: 0.000100\n",
      "Epoch: 196/200, Train Loss: 2.12497, Valid Loss: 2.25565, lr: 0.000100\n",
      "Epoch: 197/200, Train Loss: 2.10786, Valid Loss: 2.29535, lr: 0.000100\n",
      "Epoch: 198/200, Train Loss: 2.11768, Valid Loss: 2.36668, lr: 0.000100\n",
      "Epoch: 199/200, Train Loss: 2.13444, Valid Loss: 2.24553, lr: 0.000100\n",
      "Epoch: 200/200, Train Loss: 2.11407, Valid Loss: 2.26550, lr: 0.000100\n",
      "\n",
      "===== Fold 4 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_11076\\2664917137.py:52: UnderReviewWarning: The feature LinearWarmupCosineAnnealingLR is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  scheduler = LinearWarmupCosineAnnealingLR(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/200, Train Loss: 80.84002, Valid Loss: 84.52342, lr: 0.000075\n",
      "Epoch: 2/200, Train Loss: 22.69278, Valid Loss: 17.71427, lr: 0.000150\n",
      "Epoch: 3/200, Train Loss: 5.74634, Valid Loss: 5.28828, lr: 0.000225\n",
      "Epoch: 4/200, Train Loss: 4.20973, Valid Loss: 4.62801, lr: 0.000300\n",
      "Epoch: 5/200, Train Loss: 3.63088, Valid Loss: 3.79756, lr: 0.000300\n",
      "Epoch: 6/200, Train Loss: 3.43094, Valid Loss: 3.41148, lr: 0.000300\n",
      "Epoch: 7/200, Train Loss: 3.25068, Valid Loss: 3.44816, lr: 0.000300\n",
      "Epoch: 8/200, Train Loss: 3.20124, Valid Loss: 3.25684, lr: 0.000300\n",
      "Epoch: 9/200, Train Loss: 3.09433, Valid Loss: 3.30686, lr: 0.000300\n",
      "Epoch: 10/200, Train Loss: 3.21618, Valid Loss: 3.28742, lr: 0.000300\n",
      "Epoch: 11/200, Train Loss: 3.13881, Valid Loss: 3.24966, lr: 0.000300\n",
      "Epoch: 12/200, Train Loss: 3.09973, Valid Loss: 3.31141, lr: 0.000299\n",
      "Epoch: 13/200, Train Loss: 3.04563, Valid Loss: 3.23199, lr: 0.000299\n",
      "Epoch: 14/200, Train Loss: 3.01140, Valid Loss: 3.25450, lr: 0.000299\n",
      "Epoch: 15/200, Train Loss: 3.03105, Valid Loss: 3.25297, lr: 0.000299\n",
      "Epoch: 16/200, Train Loss: 3.02675, Valid Loss: 3.27769, lr: 0.000298\n",
      "Epoch: 17/200, Train Loss: 2.98431, Valid Loss: 3.23569, lr: 0.000298\n",
      "Epoch: 18/200, Train Loss: 2.89326, Valid Loss: 3.23729, lr: 0.000298\n",
      "Epoch: 19/200, Train Loss: 2.93063, Valid Loss: 3.21054, lr: 0.000297\n",
      "Epoch: 20/200, Train Loss: 2.94369, Valid Loss: 3.20411, lr: 0.000297\n",
      "Epoch: 21/200, Train Loss: 2.98006, Valid Loss: 3.21466, lr: 0.000297\n",
      "Epoch: 22/200, Train Loss: 2.98200, Valid Loss: 3.20522, lr: 0.000296\n",
      "Epoch: 23/200, Train Loss: 2.79886, Valid Loss: 3.19138, lr: 0.000296\n",
      "Epoch: 24/200, Train Loss: 2.93817, Valid Loss: 3.14253, lr: 0.000295\n",
      "Epoch: 25/200, Train Loss: 2.92402, Valid Loss: 3.22131, lr: 0.000295\n",
      "Epoch: 26/200, Train Loss: 2.83650, Valid Loss: 3.14095, lr: 0.000294\n",
      "Epoch: 27/200, Train Loss: 2.88169, Valid Loss: 3.16688, lr: 0.000294\n",
      "Epoch: 28/200, Train Loss: 2.89631, Valid Loss: 3.18091, lr: 0.000293\n",
      "Epoch: 29/200, Train Loss: 2.85207, Valid Loss: 3.12096, lr: 0.000293\n",
      "Epoch: 30/200, Train Loss: 2.83892, Valid Loss: 3.14052, lr: 0.000292\n",
      "Epoch: 31/200, Train Loss: 2.88912, Valid Loss: 3.16234, lr: 0.000291\n",
      "Epoch: 32/200, Train Loss: 2.84660, Valid Loss: 3.11028, lr: 0.000291\n",
      "Epoch: 33/200, Train Loss: 2.85172, Valid Loss: 3.12893, lr: 0.000290\n",
      "Epoch: 34/200, Train Loss: 2.82274, Valid Loss: 3.08443, lr: 0.000289\n",
      "Epoch: 35/200, Train Loss: 2.80896, Valid Loss: 3.14040, lr: 0.000289\n",
      "Epoch: 36/200, Train Loss: 2.71477, Valid Loss: 3.09331, lr: 0.000288\n",
      "Epoch: 37/200, Train Loss: 2.78990, Valid Loss: 3.11377, lr: 0.000287\n",
      "Epoch: 38/200, Train Loss: 2.81303, Valid Loss: 3.06880, lr: 0.000286\n",
      "Epoch: 39/200, Train Loss: 2.78309, Valid Loss: 3.05226, lr: 0.000285\n",
      "Epoch: 40/200, Train Loss: 2.74295, Valid Loss: 3.05082, lr: 0.000285\n",
      "Epoch: 41/200, Train Loss: 2.71630, Valid Loss: 3.08161, lr: 0.000284\n",
      "Epoch: 42/200, Train Loss: 2.74571, Valid Loss: 3.01634, lr: 0.000283\n",
      "Epoch: 43/200, Train Loss: 2.70782, Valid Loss: 3.04891, lr: 0.000282\n",
      "Epoch: 44/200, Train Loss: 2.71155, Valid Loss: 3.02190, lr: 0.000281\n",
      "Epoch: 45/200, Train Loss: 2.75155, Valid Loss: 3.01592, lr: 0.000280\n",
      "Epoch: 46/200, Train Loss: 2.66224, Valid Loss: 2.99632, lr: 0.000279\n",
      "Epoch: 47/200, Train Loss: 2.70446, Valid Loss: 3.00217, lr: 0.000278\n",
      "Epoch: 48/200, Train Loss: 2.70497, Valid Loss: 3.02978, lr: 0.000277\n",
      "Epoch: 49/200, Train Loss: 2.70459, Valid Loss: 2.95875, lr: 0.000276\n",
      "Epoch: 50/200, Train Loss: 2.68583, Valid Loss: 2.99124, lr: 0.000275\n",
      "Epoch: 51/200, Train Loss: 2.75000, Valid Loss: 2.96715, lr: 0.000274\n",
      "Epoch: 52/200, Train Loss: 2.61135, Valid Loss: 2.95998, lr: 0.000273\n",
      "Epoch: 53/200, Train Loss: 2.70927, Valid Loss: 2.98197, lr: 0.000272\n",
      "Epoch: 54/200, Train Loss: 2.63644, Valid Loss: 2.91862, lr: 0.000270\n",
      "Epoch: 55/200, Train Loss: 2.63541, Valid Loss: 2.93683, lr: 0.000269\n",
      "Epoch: 56/200, Train Loss: 2.66201, Valid Loss: 2.91129, lr: 0.000268\n",
      "Epoch: 57/200, Train Loss: 2.66102, Valid Loss: 2.95350, lr: 0.000267\n",
      "Epoch: 58/200, Train Loss: 2.64403, Valid Loss: 2.88807, lr: 0.000266\n",
      "Epoch: 59/200, Train Loss: 2.60500, Valid Loss: 2.93153, lr: 0.000264\n",
      "Epoch: 60/200, Train Loss: 2.61449, Valid Loss: 2.86452, lr: 0.000263\n",
      "Epoch: 61/200, Train Loss: 2.62609, Valid Loss: 2.89834, lr: 0.000262\n",
      "Epoch: 62/200, Train Loss: 2.60613, Valid Loss: 2.86057, lr: 0.000261\n",
      "Epoch: 63/200, Train Loss: 2.50745, Valid Loss: 2.89576, lr: 0.000259\n",
      "Epoch: 64/200, Train Loss: 2.53996, Valid Loss: 2.86748, lr: 0.000258\n",
      "Epoch: 65/200, Train Loss: 2.54806, Valid Loss: 2.82591, lr: 0.000257\n",
      "Epoch: 66/200, Train Loss: 2.54377, Valid Loss: 2.91082, lr: 0.000255\n",
      "Epoch: 67/200, Train Loss: 2.54355, Valid Loss: 2.80213, lr: 0.000254\n",
      "Epoch: 68/200, Train Loss: 2.57416, Valid Loss: 2.97540, lr: 0.000253\n",
      "Epoch: 69/200, Train Loss: 2.58977, Valid Loss: 2.78687, lr: 0.000251\n",
      "Epoch: 70/200, Train Loss: 2.55582, Valid Loss: 2.88738, lr: 0.000250\n",
      "Epoch: 71/200, Train Loss: 2.52846, Valid Loss: 2.78289, lr: 0.000249\n",
      "Epoch: 72/200, Train Loss: 2.49906, Valid Loss: 2.84698, lr: 0.000247\n",
      "Epoch: 73/200, Train Loss: 2.55441, Valid Loss: 2.77614, lr: 0.000246\n",
      "Epoch: 74/200, Train Loss: 2.55936, Valid Loss: 2.85219, lr: 0.000244\n",
      "Epoch: 75/200, Train Loss: 2.51630, Valid Loss: 2.73541, lr: 0.000243\n",
      "Epoch: 76/200, Train Loss: 2.51485, Valid Loss: 2.90946, lr: 0.000241\n",
      "Epoch: 77/200, Train Loss: 2.50275, Valid Loss: 2.73638, lr: 0.000240\n",
      "Epoch: 78/200, Train Loss: 2.48311, Valid Loss: 2.80817, lr: 0.000238\n",
      "Epoch: 79/200, Train Loss: 2.45068, Valid Loss: 2.70752, lr: 0.000237\n",
      "Epoch: 80/200, Train Loss: 2.46036, Valid Loss: 2.84078, lr: 0.000235\n",
      "Epoch: 81/200, Train Loss: 2.54245, Valid Loss: 2.69067, lr: 0.000234\n",
      "Epoch: 82/200, Train Loss: 2.51853, Valid Loss: 2.81002, lr: 0.000232\n",
      "Epoch: 83/200, Train Loss: 2.47466, Valid Loss: 2.67418, lr: 0.000231\n",
      "Epoch: 84/200, Train Loss: 2.51289, Valid Loss: 2.78265, lr: 0.000229\n",
      "Epoch: 85/200, Train Loss: 2.47055, Valid Loss: 2.70383, lr: 0.000228\n",
      "Epoch: 86/200, Train Loss: 2.45699, Valid Loss: 2.73148, lr: 0.000226\n",
      "Epoch: 87/200, Train Loss: 2.42390, Valid Loss: 2.70055, lr: 0.000225\n",
      "Epoch: 88/200, Train Loss: 2.41308, Valid Loss: 2.66812, lr: 0.000223\n",
      "Epoch: 89/200, Train Loss: 2.43970, Valid Loss: 2.70940, lr: 0.000222\n",
      "Epoch: 90/200, Train Loss: 2.38661, Valid Loss: 2.64362, lr: 0.000220\n",
      "Epoch: 91/200, Train Loss: 2.47347, Valid Loss: 2.71135, lr: 0.000218\n",
      "Epoch: 92/200, Train Loss: 2.44337, Valid Loss: 2.64331, lr: 0.000217\n",
      "Epoch: 93/200, Train Loss: 2.43525, Valid Loss: 2.61454, lr: 0.000215\n",
      "Epoch: 94/200, Train Loss: 2.40472, Valid Loss: 2.67431, lr: 0.000214\n",
      "Epoch: 95/200, Train Loss: 2.37854, Valid Loss: 2.64179, lr: 0.000212\n",
      "Epoch: 96/200, Train Loss: 2.40533, Valid Loss: 2.65979, lr: 0.000210\n",
      "Epoch: 97/200, Train Loss: 2.36895, Valid Loss: 2.63570, lr: 0.000209\n",
      "Epoch: 98/200, Train Loss: 2.34536, Valid Loss: 2.63499, lr: 0.000207\n",
      "Epoch: 99/200, Train Loss: 2.38292, Valid Loss: 2.60373, lr: 0.000206\n",
      "Epoch: 100/200, Train Loss: 2.32007, Valid Loss: 2.65029, lr: 0.000204\n",
      "Epoch: 101/200, Train Loss: 2.38485, Valid Loss: 2.55640, lr: 0.000202\n",
      "Epoch: 102/200, Train Loss: 2.34824, Valid Loss: 2.61938, lr: 0.000201\n",
      "Epoch: 103/200, Train Loss: 2.32230, Valid Loss: 2.54495, lr: 0.000199\n",
      "Epoch: 104/200, Train Loss: 2.36569, Valid Loss: 2.63267, lr: 0.000198\n",
      "Epoch: 105/200, Train Loss: 2.32617, Valid Loss: 2.54760, lr: 0.000196\n",
      "Epoch: 106/200, Train Loss: 2.31346, Valid Loss: 2.55273, lr: 0.000194\n",
      "Epoch: 107/200, Train Loss: 2.34162, Valid Loss: 2.57566, lr: 0.000193\n",
      "Epoch: 108/200, Train Loss: 2.30085, Valid Loss: 2.54122, lr: 0.000191\n",
      "Epoch: 109/200, Train Loss: 2.34538, Valid Loss: 2.55811, lr: 0.000190\n",
      "Epoch: 110/200, Train Loss: 2.33054, Valid Loss: 2.49928, lr: 0.000188\n",
      "Epoch: 111/200, Train Loss: 2.33579, Valid Loss: 2.59644, lr: 0.000186\n",
      "Epoch: 112/200, Train Loss: 2.28860, Valid Loss: 2.47111, lr: 0.000185\n",
      "Epoch: 113/200, Train Loss: 2.35510, Valid Loss: 2.58531, lr: 0.000183\n",
      "Epoch: 114/200, Train Loss: 2.31462, Valid Loss: 2.46543, lr: 0.000182\n",
      "Epoch: 115/200, Train Loss: 2.38506, Valid Loss: 2.62458, lr: 0.000180\n",
      "Epoch: 116/200, Train Loss: 2.30215, Valid Loss: 2.46067, lr: 0.000178\n",
      "Epoch: 117/200, Train Loss: 2.24489, Valid Loss: 2.51017, lr: 0.000177\n",
      "Epoch: 118/200, Train Loss: 2.35015, Valid Loss: 2.49388, lr: 0.000175\n",
      "Epoch: 119/200, Train Loss: 2.28132, Valid Loss: 2.46505, lr: 0.000174\n",
      "Epoch: 120/200, Train Loss: 2.29958, Valid Loss: 2.44416, lr: 0.000172\n",
      "Epoch: 121/200, Train Loss: 2.29322, Valid Loss: 2.47053, lr: 0.000171\n",
      "Epoch: 122/200, Train Loss: 2.28439, Valid Loss: 2.44938, lr: 0.000169\n",
      "Epoch: 123/200, Train Loss: 2.27475, Valid Loss: 2.45807, lr: 0.000168\n",
      "Epoch: 124/200, Train Loss: 2.26716, Valid Loss: 2.45008, lr: 0.000166\n",
      "Epoch: 125/200, Train Loss: 2.24544, Valid Loss: 2.45456, lr: 0.000165\n",
      "Epoch: 126/200, Train Loss: 2.28474, Valid Loss: 2.41716, lr: 0.000163\n",
      "Epoch: 127/200, Train Loss: 2.24822, Valid Loss: 2.50462, lr: 0.000162\n",
      "Epoch: 128/200, Train Loss: 2.29399, Valid Loss: 2.39587, lr: 0.000160\n",
      "Epoch: 129/200, Train Loss: 2.35819, Valid Loss: 2.45240, lr: 0.000159\n",
      "Epoch: 130/200, Train Loss: 2.32368, Valid Loss: 2.46125, lr: 0.000157\n",
      "Epoch: 131/200, Train Loss: 2.26909, Valid Loss: 2.38104, lr: 0.000156\n",
      "Epoch: 132/200, Train Loss: 2.31597, Valid Loss: 2.45632, lr: 0.000154\n",
      "Epoch: 133/200, Train Loss: 2.29817, Valid Loss: 2.47049, lr: 0.000153\n",
      "Epoch: 134/200, Train Loss: 2.25843, Valid Loss: 2.41157, lr: 0.000151\n",
      "Epoch: 135/200, Train Loss: 2.24785, Valid Loss: 2.40742, lr: 0.000150\n",
      "Epoch: 136/200, Train Loss: 2.21443, Valid Loss: 2.42366, lr: 0.000149\n",
      "Epoch: 137/200, Train Loss: 2.22534, Valid Loss: 2.40150, lr: 0.000147\n",
      "Epoch: 138/200, Train Loss: 2.24325, Valid Loss: 2.40897, lr: 0.000146\n",
      "Epoch: 139/200, Train Loss: 2.23435, Valid Loss: 2.38438, lr: 0.000145\n",
      "Epoch: 140/200, Train Loss: 2.18718, Valid Loss: 2.40077, lr: 0.000143\n",
      "Epoch: 141/200, Train Loss: 2.22984, Valid Loss: 2.35529, lr: 0.000142\n",
      "Epoch: 142/200, Train Loss: 2.16655, Valid Loss: 2.43737, lr: 0.000141\n",
      "Epoch: 143/200, Train Loss: 2.23763, Valid Loss: 2.35973, lr: 0.000139\n",
      "Epoch: 144/200, Train Loss: 2.22341, Valid Loss: 2.45422, lr: 0.000138\n",
      "Epoch: 145/200, Train Loss: 2.20897, Valid Loss: 2.35411, lr: 0.000137\n",
      "Epoch: 146/200, Train Loss: 2.22030, Valid Loss: 2.37006, lr: 0.000136\n",
      "Epoch: 147/200, Train Loss: 2.26859, Valid Loss: 2.40926, lr: 0.000134\n",
      "Epoch: 148/200, Train Loss: 2.22685, Valid Loss: 2.35198, lr: 0.000133\n",
      "Epoch: 149/200, Train Loss: 2.22985, Valid Loss: 2.37191, lr: 0.000132\n",
      "Epoch: 150/200, Train Loss: 2.23441, Valid Loss: 2.35601, lr: 0.000131\n",
      "Epoch: 151/200, Train Loss: 2.17709, Valid Loss: 2.36321, lr: 0.000130\n",
      "Epoch: 152/200, Train Loss: 2.22316, Valid Loss: 2.32896, lr: 0.000128\n",
      "Epoch: 153/200, Train Loss: 2.12509, Valid Loss: 2.43477, lr: 0.000127\n",
      "Epoch: 154/200, Train Loss: 2.23862, Valid Loss: 2.32004, lr: 0.000126\n",
      "Epoch: 155/200, Train Loss: 2.18016, Valid Loss: 2.45037, lr: 0.000125\n",
      "Epoch: 156/200, Train Loss: 2.18753, Valid Loss: 2.32832, lr: 0.000124\n",
      "Epoch: 157/200, Train Loss: 2.22320, Valid Loss: 2.39669, lr: 0.000123\n",
      "Epoch: 158/200, Train Loss: 2.21150, Valid Loss: 2.35125, lr: 0.000122\n",
      "Epoch: 159/200, Train Loss: 2.19582, Valid Loss: 2.31716, lr: 0.000121\n",
      "Epoch: 160/200, Train Loss: 2.25249, Valid Loss: 2.47190, lr: 0.000120\n",
      "Epoch: 161/200, Train Loss: 2.20775, Valid Loss: 2.30840, lr: 0.000119\n",
      "Epoch: 162/200, Train Loss: 2.21512, Valid Loss: 2.41337, lr: 0.000118\n",
      "Epoch: 163/200, Train Loss: 2.17312, Valid Loss: 2.32504, lr: 0.000117\n",
      "Epoch: 164/200, Train Loss: 2.16740, Valid Loss: 2.33813, lr: 0.000116\n",
      "Epoch: 165/200, Train Loss: 2.17760, Valid Loss: 2.34128, lr: 0.000115\n",
      "Epoch: 166/200, Train Loss: 2.17657, Valid Loss: 2.35069, lr: 0.000115\n",
      "Epoch: 167/200, Train Loss: 2.15584, Valid Loss: 2.31043, lr: 0.000114\n",
      "Epoch: 168/200, Train Loss: 2.18845, Valid Loss: 2.34558, lr: 0.000113\n",
      "Epoch: 169/200, Train Loss: 2.14207, Valid Loss: 2.34198, lr: 0.000112\n",
      "Epoch: 170/200, Train Loss: 2.15554, Valid Loss: 2.32486, lr: 0.000111\n",
      "Epoch: 171/200, Train Loss: 2.11238, Valid Loss: 2.35495, lr: 0.000111\n",
      "Epoch: 172/200, Train Loss: 2.11587, Valid Loss: 2.31568, lr: 0.000110\n",
      "Epoch: 173/200, Train Loss: 2.17215, Valid Loss: 2.34794, lr: 0.000109\n",
      "Epoch: 174/200, Train Loss: 2.14767, Valid Loss: 2.30700, lr: 0.000109\n",
      "Epoch: 175/200, Train Loss: 2.12737, Valid Loss: 2.32424, lr: 0.000108\n",
      "Epoch: 176/200, Train Loss: 2.16194, Valid Loss: 2.32037, lr: 0.000107\n",
      "Epoch: 177/200, Train Loss: 2.15126, Valid Loss: 2.34186, lr: 0.000107\n",
      "Epoch: 178/200, Train Loss: 2.13237, Valid Loss: 2.29482, lr: 0.000106\n",
      "Epoch: 179/200, Train Loss: 2.16100, Valid Loss: 2.31916, lr: 0.000106\n",
      "Epoch: 180/200, Train Loss: 2.15537, Valid Loss: 2.28962, lr: 0.000105\n",
      "Epoch: 181/200, Train Loss: 2.17570, Valid Loss: 2.32242, lr: 0.000105\n",
      "Epoch: 182/200, Train Loss: 2.17022, Valid Loss: 2.28980, lr: 0.000104\n",
      "Epoch: 183/200, Train Loss: 2.20600, Valid Loss: 2.30536, lr: 0.000104\n",
      "Epoch: 184/200, Train Loss: 2.20164, Valid Loss: 2.33437, lr: 0.000103\n",
      "Epoch: 185/200, Train Loss: 2.11446, Valid Loss: 2.28217, lr: 0.000103\n",
      "Epoch: 186/200, Train Loss: 2.10127, Valid Loss: 2.34257, lr: 0.000103\n",
      "Epoch: 187/200, Train Loss: 2.16873, Valid Loss: 2.28809, lr: 0.000102\n",
      "Epoch: 188/200, Train Loss: 2.13539, Valid Loss: 2.31783, lr: 0.000102\n",
      "Epoch: 189/200, Train Loss: 2.24473, Valid Loss: 2.38619, lr: 0.000102\n",
      "Epoch: 190/200, Train Loss: 2.12118, Valid Loss: 2.27709, lr: 0.000101\n",
      "Epoch: 191/200, Train Loss: 2.22601, Valid Loss: 2.40814, lr: 0.000101\n",
      "Epoch: 192/200, Train Loss: 2.16267, Valid Loss: 2.27813, lr: 0.000101\n",
      "Epoch: 193/200, Train Loss: 2.15663, Valid Loss: 2.33737, lr: 0.000101\n",
      "Epoch: 194/200, Train Loss: 2.15729, Valid Loss: 2.30501, lr: 0.000100\n",
      "Epoch: 195/200, Train Loss: 2.19267, Valid Loss: 2.30972, lr: 0.000100\n",
      "Epoch: 196/200, Train Loss: 2.10724, Valid Loss: 2.28682, lr: 0.000100\n",
      "Epoch: 197/200, Train Loss: 2.13065, Valid Loss: 2.35007, lr: 0.000100\n",
      "Epoch: 198/200, Train Loss: 2.14657, Valid Loss: 2.27611, lr: 0.000100\n",
      "Epoch: 199/200, Train Loss: 2.16163, Valid Loss: 2.29264, lr: 0.000100\n",
      "Epoch: 200/200, Train Loss: 2.16455, Valid Loss: 2.33198, lr: 0.000100\n",
      "\n",
      "===== Fold 5 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_11076\\2664917137.py:52: UnderReviewWarning: The feature LinearWarmupCosineAnnealingLR is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  scheduler = LinearWarmupCosineAnnealingLR(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/200, Train Loss: 75.59109, Valid Loss: 75.21866, lr: 0.000075\n",
      "Epoch: 2/200, Train Loss: 21.26216, Valid Loss: 22.69610, lr: 0.000150\n",
      "Epoch: 3/200, Train Loss: 6.77152, Valid Loss: 4.65440, lr: 0.000225\n",
      "Epoch: 4/200, Train Loss: 4.36677, Valid Loss: 4.84617, lr: 0.000300\n",
      "Epoch: 5/200, Train Loss: 3.89827, Valid Loss: 3.01521, lr: 0.000300\n",
      "Epoch: 6/200, Train Loss: 3.48341, Valid Loss: 2.96880, lr: 0.000300\n",
      "Epoch: 7/200, Train Loss: 3.31841, Valid Loss: 2.70459, lr: 0.000300\n",
      "Epoch: 8/200, Train Loss: 3.23554, Valid Loss: 2.70547, lr: 0.000300\n",
      "Epoch: 9/200, Train Loss: 3.26264, Valid Loss: 2.71600, lr: 0.000300\n",
      "Epoch: 10/200, Train Loss: 3.18672, Valid Loss: 2.68015, lr: 0.000300\n",
      "Epoch: 11/200, Train Loss: 3.18214, Valid Loss: 2.69120, lr: 0.000300\n",
      "Epoch: 12/200, Train Loss: 3.18861, Valid Loss: 2.66965, lr: 0.000299\n",
      "Epoch: 13/200, Train Loss: 3.18511, Valid Loss: 2.65529, lr: 0.000299\n",
      "Epoch: 14/200, Train Loss: 3.17665, Valid Loss: 2.64497, lr: 0.000299\n",
      "Epoch: 15/200, Train Loss: 3.13305, Valid Loss: 2.64007, lr: 0.000299\n",
      "Epoch: 16/200, Train Loss: 3.19418, Valid Loss: 2.62569, lr: 0.000298\n",
      "Epoch: 17/200, Train Loss: 3.14149, Valid Loss: 2.61826, lr: 0.000298\n",
      "Epoch: 18/200, Train Loss: 3.09428, Valid Loss: 2.60916, lr: 0.000298\n",
      "Epoch: 19/200, Train Loss: 3.12852, Valid Loss: 2.60058, lr: 0.000297\n",
      "Epoch: 20/200, Train Loss: 3.12177, Valid Loss: 2.59632, lr: 0.000297\n",
      "Epoch: 21/200, Train Loss: 3.09141, Valid Loss: 2.58599, lr: 0.000297\n",
      "Epoch: 22/200, Train Loss: 3.03426, Valid Loss: 2.58224, lr: 0.000296\n",
      "Epoch: 23/200, Train Loss: 3.02697, Valid Loss: 2.57726, lr: 0.000296\n",
      "Epoch: 24/200, Train Loss: 3.07789, Valid Loss: 2.56059, lr: 0.000295\n",
      "Epoch: 25/200, Train Loss: 3.04473, Valid Loss: 2.55372, lr: 0.000295\n",
      "Epoch: 26/200, Train Loss: 3.05516, Valid Loss: 2.54514, lr: 0.000294\n",
      "Epoch: 27/200, Train Loss: 3.02886, Valid Loss: 2.53712, lr: 0.000294\n",
      "Epoch: 28/200, Train Loss: 2.98294, Valid Loss: 2.53274, lr: 0.000293\n",
      "Epoch: 29/200, Train Loss: 2.98406, Valid Loss: 2.52116, lr: 0.000293\n",
      "Epoch: 30/200, Train Loss: 2.95636, Valid Loss: 2.52550, lr: 0.000292\n",
      "Epoch: 31/200, Train Loss: 3.00436, Valid Loss: 2.51197, lr: 0.000291\n",
      "Epoch: 32/200, Train Loss: 2.95483, Valid Loss: 2.52205, lr: 0.000291\n",
      "Epoch: 33/200, Train Loss: 2.92678, Valid Loss: 2.49503, lr: 0.000290\n",
      "Epoch: 34/200, Train Loss: 2.94794, Valid Loss: 2.48515, lr: 0.000289\n",
      "Epoch: 35/200, Train Loss: 2.89565, Valid Loss: 2.47649, lr: 0.000289\n",
      "Epoch: 36/200, Train Loss: 2.85205, Valid Loss: 2.48424, lr: 0.000288\n",
      "Epoch: 37/200, Train Loss: 2.86858, Valid Loss: 2.45946, lr: 0.000287\n",
      "Epoch: 38/200, Train Loss: 2.87796, Valid Loss: 2.46505, lr: 0.000286\n",
      "Epoch: 39/200, Train Loss: 2.86738, Valid Loss: 2.44932, lr: 0.000285\n",
      "Epoch: 40/200, Train Loss: 2.87646, Valid Loss: 2.45334, lr: 0.000285\n",
      "Epoch: 41/200, Train Loss: 2.84157, Valid Loss: 2.43141, lr: 0.000284\n",
      "Epoch: 42/200, Train Loss: 2.82216, Valid Loss: 2.45053, lr: 0.000283\n",
      "Epoch: 43/200, Train Loss: 2.86551, Valid Loss: 2.41438, lr: 0.000282\n",
      "Epoch: 44/200, Train Loss: 2.84811, Valid Loss: 2.40955, lr: 0.000281\n",
      "Epoch: 45/200, Train Loss: 2.82222, Valid Loss: 2.39959, lr: 0.000280\n",
      "Epoch: 46/200, Train Loss: 2.79538, Valid Loss: 2.39244, lr: 0.000279\n",
      "Epoch: 47/200, Train Loss: 2.79202, Valid Loss: 2.39346, lr: 0.000278\n",
      "Epoch: 48/200, Train Loss: 2.78449, Valid Loss: 2.37730, lr: 0.000277\n",
      "Epoch: 49/200, Train Loss: 2.78788, Valid Loss: 2.39995, lr: 0.000276\n",
      "Epoch: 50/200, Train Loss: 2.78074, Valid Loss: 2.36495, lr: 0.000275\n",
      "Epoch: 51/200, Train Loss: 2.72332, Valid Loss: 2.36334, lr: 0.000274\n",
      "Epoch: 52/200, Train Loss: 2.73820, Valid Loss: 2.34862, lr: 0.000273\n",
      "Epoch: 53/200, Train Loss: 2.78093, Valid Loss: 2.36290, lr: 0.000272\n",
      "Epoch: 54/200, Train Loss: 2.78634, Valid Loss: 2.33412, lr: 0.000270\n",
      "Epoch: 55/200, Train Loss: 2.71308, Valid Loss: 2.34752, lr: 0.000269\n",
      "Epoch: 56/200, Train Loss: 2.74916, Valid Loss: 2.31989, lr: 0.000268\n",
      "Epoch: 57/200, Train Loss: 2.72185, Valid Loss: 2.32151, lr: 0.000267\n",
      "Epoch: 58/200, Train Loss: 2.73805, Valid Loss: 2.30597, lr: 0.000266\n",
      "Epoch: 59/200, Train Loss: 2.75220, Valid Loss: 2.29910, lr: 0.000264\n",
      "Epoch: 60/200, Train Loss: 2.71856, Valid Loss: 2.29776, lr: 0.000263\n",
      "Epoch: 61/200, Train Loss: 2.67064, Valid Loss: 2.28503, lr: 0.000262\n",
      "Epoch: 62/200, Train Loss: 2.63575, Valid Loss: 2.27953, lr: 0.000261\n",
      "Epoch: 63/200, Train Loss: 2.62976, Valid Loss: 2.28285, lr: 0.000259\n",
      "Epoch: 64/200, Train Loss: 2.63484, Valid Loss: 2.27031, lr: 0.000258\n",
      "Epoch: 65/200, Train Loss: 2.67414, Valid Loss: 2.25929, lr: 0.000257\n",
      "Epoch: 66/200, Train Loss: 2.64084, Valid Loss: 2.25217, lr: 0.000255\n",
      "Epoch: 67/200, Train Loss: 2.63575, Valid Loss: 2.28277, lr: 0.000254\n",
      "Epoch: 68/200, Train Loss: 2.64796, Valid Loss: 2.23872, lr: 0.000253\n",
      "Epoch: 69/200, Train Loss: 2.60096, Valid Loss: 2.23296, lr: 0.000251\n",
      "Epoch: 70/200, Train Loss: 2.58172, Valid Loss: 2.22802, lr: 0.000250\n",
      "Epoch: 71/200, Train Loss: 2.59469, Valid Loss: 2.21737, lr: 0.000249\n",
      "Epoch: 72/200, Train Loss: 2.61534, Valid Loss: 2.22789, lr: 0.000247\n",
      "Epoch: 73/200, Train Loss: 2.67635, Valid Loss: 2.22762, lr: 0.000246\n",
      "Epoch: 74/200, Train Loss: 2.62038, Valid Loss: 2.36413, lr: 0.000244\n",
      "Epoch: 75/200, Train Loss: 2.67810, Valid Loss: 2.22132, lr: 0.000243\n",
      "Epoch: 76/200, Train Loss: 2.59959, Valid Loss: 2.23448, lr: 0.000241\n",
      "Epoch: 77/200, Train Loss: 2.48176, Valid Loss: 2.18384, lr: 0.000240\n",
      "Epoch: 78/200, Train Loss: 2.52750, Valid Loss: 2.18576, lr: 0.000238\n",
      "Epoch: 79/200, Train Loss: 2.51797, Valid Loss: 2.18080, lr: 0.000237\n",
      "Epoch: 80/200, Train Loss: 2.53701, Valid Loss: 2.16274, lr: 0.000235\n",
      "Epoch: 81/200, Train Loss: 2.59142, Valid Loss: 2.20742, lr: 0.000234\n",
      "Epoch: 82/200, Train Loss: 2.53304, Valid Loss: 2.15027, lr: 0.000232\n",
      "Epoch: 83/200, Train Loss: 2.54052, Valid Loss: 2.17737, lr: 0.000231\n",
      "Epoch: 84/200, Train Loss: 2.53889, Valid Loss: 2.15724, lr: 0.000229\n",
      "Epoch: 85/200, Train Loss: 2.46658, Valid Loss: 2.13328, lr: 0.000228\n",
      "Epoch: 86/200, Train Loss: 2.51376, Valid Loss: 2.15035, lr: 0.000226\n",
      "Epoch: 87/200, Train Loss: 2.47336, Valid Loss: 2.12419, lr: 0.000225\n",
      "Epoch: 88/200, Train Loss: 2.49437, Valid Loss: 2.11795, lr: 0.000223\n",
      "Epoch: 89/200, Train Loss: 2.44032, Valid Loss: 2.11188, lr: 0.000222\n",
      "Epoch: 90/200, Train Loss: 2.39091, Valid Loss: 2.11295, lr: 0.000220\n",
      "Epoch: 91/200, Train Loss: 2.41451, Valid Loss: 2.12107, lr: 0.000218\n",
      "Epoch: 92/200, Train Loss: 2.48089, Valid Loss: 2.11583, lr: 0.000217\n",
      "Epoch: 93/200, Train Loss: 2.42671, Valid Loss: 2.08884, lr: 0.000215\n",
      "Epoch: 94/200, Train Loss: 2.44071, Valid Loss: 2.09799, lr: 0.000214\n",
      "Epoch: 95/200, Train Loss: 2.44068, Valid Loss: 2.09617, lr: 0.000212\n",
      "Epoch: 96/200, Train Loss: 2.42915, Valid Loss: 2.07135, lr: 0.000210\n",
      "Epoch: 97/200, Train Loss: 2.45176, Valid Loss: 2.16499, lr: 0.000209\n",
      "Epoch: 98/200, Train Loss: 2.36475, Valid Loss: 2.06302, lr: 0.000207\n",
      "Epoch: 99/200, Train Loss: 2.37562, Valid Loss: 2.09885, lr: 0.000206\n",
      "Epoch: 100/200, Train Loss: 2.42194, Valid Loss: 2.05148, lr: 0.000204\n",
      "Epoch: 101/200, Train Loss: 2.41564, Valid Loss: 2.05579, lr: 0.000202\n",
      "Epoch: 102/200, Train Loss: 2.38480, Valid Loss: 2.05600, lr: 0.000201\n",
      "Epoch: 103/200, Train Loss: 2.36620, Valid Loss: 2.05717, lr: 0.000199\n",
      "Epoch: 104/200, Train Loss: 2.37694, Valid Loss: 2.03164, lr: 0.000198\n",
      "Epoch: 105/200, Train Loss: 2.41632, Valid Loss: 2.02792, lr: 0.000196\n",
      "Epoch: 106/200, Train Loss: 2.38844, Valid Loss: 2.07601, lr: 0.000194\n",
      "Epoch: 107/200, Train Loss: 2.42668, Valid Loss: 2.06906, lr: 0.000193\n",
      "Epoch: 108/200, Train Loss: 2.39575, Valid Loss: 2.02257, lr: 0.000191\n",
      "Epoch: 109/200, Train Loss: 2.37435, Valid Loss: 2.02150, lr: 0.000190\n",
      "Epoch: 110/200, Train Loss: 2.39247, Valid Loss: 2.07122, lr: 0.000188\n",
      "Epoch: 111/200, Train Loss: 2.35942, Valid Loss: 2.01235, lr: 0.000186\n",
      "Epoch: 112/200, Train Loss: 2.37300, Valid Loss: 2.01096, lr: 0.000185\n",
      "Epoch: 113/200, Train Loss: 2.37206, Valid Loss: 2.05059, lr: 0.000183\n",
      "Epoch: 114/200, Train Loss: 2.31163, Valid Loss: 2.00251, lr: 0.000182\n",
      "Epoch: 115/200, Train Loss: 2.36030, Valid Loss: 2.00668, lr: 0.000180\n",
      "Epoch: 116/200, Train Loss: 2.31807, Valid Loss: 2.12425, lr: 0.000178\n",
      "Epoch: 117/200, Train Loss: 2.33434, Valid Loss: 2.01026, lr: 0.000177\n",
      "Epoch: 118/200, Train Loss: 2.37068, Valid Loss: 2.00542, lr: 0.000175\n",
      "Epoch: 119/200, Train Loss: 2.36080, Valid Loss: 2.02629, lr: 0.000174\n",
      "Epoch: 120/200, Train Loss: 2.33445, Valid Loss: 2.00111, lr: 0.000172\n",
      "Epoch: 121/200, Train Loss: 2.30511, Valid Loss: 2.00657, lr: 0.000171\n",
      "Epoch: 122/200, Train Loss: 2.30072, Valid Loss: 1.99390, lr: 0.000169\n",
      "Epoch: 123/200, Train Loss: 2.34640, Valid Loss: 1.98964, lr: 0.000168\n",
      "Epoch: 124/200, Train Loss: 2.30013, Valid Loss: 2.03506, lr: 0.000166\n",
      "Epoch: 125/200, Train Loss: 2.37861, Valid Loss: 1.98566, lr: 0.000165\n",
      "Epoch: 126/200, Train Loss: 2.27224, Valid Loss: 2.01051, lr: 0.000163\n",
      "Epoch: 127/200, Train Loss: 2.27183, Valid Loss: 2.02543, lr: 0.000162\n",
      "Epoch: 128/200, Train Loss: 2.37690, Valid Loss: 1.98854, lr: 0.000160\n",
      "Epoch: 129/200, Train Loss: 2.38814, Valid Loss: 1.97292, lr: 0.000159\n",
      "Epoch: 130/200, Train Loss: 2.34146, Valid Loss: 1.97388, lr: 0.000157\n",
      "Epoch: 131/200, Train Loss: 2.32371, Valid Loss: 2.01471, lr: 0.000156\n",
      "Epoch: 132/200, Train Loss: 2.32383, Valid Loss: 2.02051, lr: 0.000154\n",
      "Epoch: 133/200, Train Loss: 2.31049, Valid Loss: 1.97098, lr: 0.000153\n",
      "Epoch: 134/200, Train Loss: 2.35699, Valid Loss: 1.96751, lr: 0.000151\n",
      "Epoch: 135/200, Train Loss: 2.31605, Valid Loss: 2.05610, lr: 0.000150\n",
      "Epoch: 136/200, Train Loss: 2.39781, Valid Loss: 2.00669, lr: 0.000149\n",
      "Epoch: 137/200, Train Loss: 2.28999, Valid Loss: 2.04484, lr: 0.000147\n",
      "Epoch: 138/200, Train Loss: 2.32833, Valid Loss: 1.98723, lr: 0.000146\n",
      "Epoch: 139/200, Train Loss: 2.31543, Valid Loss: 1.98655, lr: 0.000145\n",
      "Epoch: 140/200, Train Loss: 2.23271, Valid Loss: 1.97061, lr: 0.000143\n",
      "Epoch: 141/200, Train Loss: 2.26977, Valid Loss: 1.96873, lr: 0.000142\n",
      "Epoch: 142/200, Train Loss: 2.26407, Valid Loss: 2.05257, lr: 0.000141\n",
      "Epoch: 143/200, Train Loss: 2.29847, Valid Loss: 1.96165, lr: 0.000139\n",
      "Epoch: 144/200, Train Loss: 2.28092, Valid Loss: 2.00033, lr: 0.000138\n",
      "Epoch: 145/200, Train Loss: 2.28062, Valid Loss: 2.00228, lr: 0.000137\n",
      "Epoch: 146/200, Train Loss: 2.28264, Valid Loss: 1.95646, lr: 0.000136\n",
      "Epoch: 147/200, Train Loss: 2.25486, Valid Loss: 1.99164, lr: 0.000134\n",
      "Epoch: 148/200, Train Loss: 2.21884, Valid Loss: 1.96279, lr: 0.000133\n",
      "Epoch: 149/200, Train Loss: 2.29595, Valid Loss: 1.98143, lr: 0.000132\n",
      "Epoch: 150/200, Train Loss: 2.27011, Valid Loss: 1.95527, lr: 0.000131\n",
      "Epoch: 151/200, Train Loss: 2.28156, Valid Loss: 2.05613, lr: 0.000130\n",
      "Epoch: 152/200, Train Loss: 2.30380, Valid Loss: 1.96771, lr: 0.000128\n",
      "Epoch: 153/200, Train Loss: 2.29088, Valid Loss: 2.03749, lr: 0.000127\n",
      "Epoch: 154/200, Train Loss: 2.23053, Valid Loss: 1.94861, lr: 0.000126\n",
      "Epoch: 155/200, Train Loss: 2.29949, Valid Loss: 1.97942, lr: 0.000125\n",
      "Epoch: 156/200, Train Loss: 2.23685, Valid Loss: 1.94708, lr: 0.000124\n",
      "Epoch: 157/200, Train Loss: 2.28014, Valid Loss: 1.97653, lr: 0.000123\n",
      "Epoch: 158/200, Train Loss: 2.24537, Valid Loss: 1.95607, lr: 0.000122\n",
      "Epoch: 159/200, Train Loss: 2.24281, Valid Loss: 1.95776, lr: 0.000121\n",
      "Epoch: 160/200, Train Loss: 2.25169, Valid Loss: 1.94459, lr: 0.000120\n",
      "Epoch: 161/200, Train Loss: 2.29744, Valid Loss: 2.01963, lr: 0.000119\n",
      "Epoch: 162/200, Train Loss: 2.21268, Valid Loss: 1.94244, lr: 0.000118\n",
      "Epoch: 163/200, Train Loss: 2.28021, Valid Loss: 2.03240, lr: 0.000117\n",
      "Epoch: 164/200, Train Loss: 2.32270, Valid Loss: 1.99131, lr: 0.000116\n",
      "Epoch: 165/200, Train Loss: 2.28555, Valid Loss: 1.94061, lr: 0.000115\n",
      "Epoch: 166/200, Train Loss: 2.26408, Valid Loss: 1.97421, lr: 0.000115\n",
      "Epoch: 167/200, Train Loss: 2.23832, Valid Loss: 1.95473, lr: 0.000114\n",
      "Epoch: 168/200, Train Loss: 2.25892, Valid Loss: 1.99217, lr: 0.000113\n",
      "Epoch: 169/200, Train Loss: 2.24259, Valid Loss: 1.94791, lr: 0.000112\n",
      "Epoch: 170/200, Train Loss: 2.25794, Valid Loss: 1.94889, lr: 0.000111\n",
      "Epoch: 171/200, Train Loss: 2.25044, Valid Loss: 1.99337, lr: 0.000111\n",
      "Epoch: 172/200, Train Loss: 2.20811, Valid Loss: 1.94217, lr: 0.000110\n",
      "Epoch: 173/200, Train Loss: 2.22560, Valid Loss: 1.96133, lr: 0.000109\n",
      "Epoch: 174/200, Train Loss: 2.23982, Valid Loss: 1.99218, lr: 0.000109\n",
      "Epoch: 175/200, Train Loss: 2.22342, Valid Loss: 1.93452, lr: 0.000108\n",
      "Epoch: 176/200, Train Loss: 2.28153, Valid Loss: 2.00321, lr: 0.000107\n",
      "Epoch: 177/200, Train Loss: 2.25150, Valid Loss: 1.93349, lr: 0.000107\n",
      "Epoch: 178/200, Train Loss: 2.18062, Valid Loss: 2.02411, lr: 0.000106\n",
      "Epoch: 179/200, Train Loss: 2.24261, Valid Loss: 1.93184, lr: 0.000106\n",
      "Epoch: 180/200, Train Loss: 2.23414, Valid Loss: 2.01787, lr: 0.000105\n",
      "Epoch: 181/200, Train Loss: 2.25091, Valid Loss: 1.93078, lr: 0.000105\n",
      "Epoch: 182/200, Train Loss: 2.20722, Valid Loss: 1.97270, lr: 0.000104\n",
      "Epoch: 183/200, Train Loss: 2.21760, Valid Loss: 1.92979, lr: 0.000104\n",
      "Epoch: 184/200, Train Loss: 2.26770, Valid Loss: 2.04293, lr: 0.000103\n",
      "Epoch: 185/200, Train Loss: 2.25984, Valid Loss: 1.93984, lr: 0.000103\n",
      "Epoch: 186/200, Train Loss: 2.20884, Valid Loss: 1.92923, lr: 0.000103\n",
      "Epoch: 187/200, Train Loss: 2.22353, Valid Loss: 1.99056, lr: 0.000102\n",
      "Epoch: 188/200, Train Loss: 2.21404, Valid Loss: 1.93400, lr: 0.000102\n",
      "Epoch: 189/200, Train Loss: 2.26551, Valid Loss: 2.16904, lr: 0.000102\n",
      "Epoch: 190/200, Train Loss: 2.23794, Valid Loss: 1.94669, lr: 0.000101\n",
      "Epoch: 191/200, Train Loss: 2.26778, Valid Loss: 2.02138, lr: 0.000101\n",
      "Epoch: 192/200, Train Loss: 2.21792, Valid Loss: 1.92829, lr: 0.000101\n",
      "Epoch: 193/200, Train Loss: 2.24550, Valid Loss: 1.95385, lr: 0.000101\n",
      "Epoch: 194/200, Train Loss: 2.17240, Valid Loss: 1.94018, lr: 0.000100\n",
      "Epoch: 195/200, Train Loss: 2.19295, Valid Loss: 1.95742, lr: 0.000100\n",
      "Epoch: 196/200, Train Loss: 2.24111, Valid Loss: 1.95101, lr: 0.000100\n",
      "Epoch: 197/200, Train Loss: 2.21857, Valid Loss: 1.93424, lr: 0.000100\n",
      "Epoch: 198/200, Train Loss: 2.27504, Valid Loss: 1.98979, lr: 0.000100\n",
      "Epoch: 199/200, Train Loss: 2.17272, Valid Loss: 1.93748, lr: 0.000100\n",
      "Epoch: 200/200, Train Loss: 2.23124, Valid Loss: 1.96439, lr: 0.000100\n",
      "\n",
      "MLP CV scores: [np.float64(1.561617797088365), np.float64(1.459438635814292), np.float64(1.50515830953188), np.float64(1.5270821529617453), np.float64(1.4015666965280433)]\n",
      "MLP 평균 RMSE: 1.4909727183848651\n"
     ]
    }
   ],
   "source": [
    "# K-fold CV\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# final_dataset에서 feature / target 분리\n",
    "train_df = final_dataset.drop(['smiles', 'IC50_nM'], axis=1)\n",
    "# Fingerprint 벡터는 2D numpy 배열로 변환\n",
    "fp_array = np.vstack(train_df['Fingerprint'].values).astype(np.float32)  # (n_samples, fp_dim)\n",
    "# Fingerprint 컬럼 제외한 나머지 feature 추출\n",
    "other_features = train_df.drop(columns=['Fingerprint', 'pIC50']).copy()\n",
    "# bool 타입은 float32로 변환\n",
    "for col in other_features.columns:\n",
    "    if other_features[col].dtype == 'bool':\n",
    "        other_features[col] = other_features[col].astype(np.float32)\n",
    "other_array = other_features.values.astype(np.float32)  # (n_samples, other_dim)\n",
    "\n",
    "# 최종 feature 합치기\n",
    "X_np = np.hstack([fp_array, other_array])  # (n_samples, total_features)\n",
    "y_np = train_df['pIC50'].values.astype(np.float32)\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "mlp_cv_scores = []\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(kf.split(X_np)):\n",
    "    print(f\"\\n===== Fold {fold+1} =====\")\n",
    "    \n",
    "    # fold별 train/valid 나누기\n",
    "    X_train, X_valid = X_np[train_idx], X_np[valid_idx]\n",
    "    y_train, y_valid = y_np[train_idx], y_np[valid_idx]\n",
    "\n",
    "    # VarianceThreshold 적용 (train에 fit, valid는 transform)\n",
    "    transform = VarianceThreshold(threshold=0.05)\n",
    "    X_train = transform.fit_transform(X_train)\n",
    "    X_valid = transform.transform(X_valid)\n",
    "    \n",
    "    # Tensor 변환\n",
    "    train_dataset = KfoldCustomDataset(X_train, y_train, is_test=False)\n",
    "    valid_dataset = KfoldCustomDataset(X_valid, y_valid, is_test=False)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=CFG_model['BATCH_SIZE'], shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=CFG_model['BATCH_SIZE'], shuffle=False)\n",
    "\n",
    "    # 모델 초기화 (Net은 MLP 모델 클래스)\n",
    "    model = Net(\n",
    "        input_size=X_train.shape[1], \n",
    "        hidden_size=CFG_model['HIDDEN_SIZE'], \n",
    "        dropout_rate=CFG_model['DROPOUT_RATE'], \n",
    "        out_size=CFG_model['OUTPUT_SIZE']\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=CFG_model['LEARNING_RATE'], weight_decay=1e-4)\n",
    "    scheduler = LinearWarmupCosineAnnealingLR(\n",
    "        optimizer,\n",
    "        warmup_epochs=CFG_model['WARMUP_EPOCHS'],\n",
    "        max_epochs=CFG_model['MAX_EPOCHS'],\n",
    "        eta_min=1e-4\n",
    "    )\n",
    "    \n",
    "    # 학습\n",
    "    model_train, train_losses, valid_losses, learning_rates= train(\n",
    "        train_loader, valid_loader, model, criterion, optimizer, scheduler,\n",
    "        epochs=CFG_model['EPOCHS']\n",
    "    )\n",
    "\n",
    "    # fold별 valid score 저장 (RMSE)\n",
    "    rmse = np.sqrt(valid_losses[-1])  # 마지막 epoch의 valid loss 사용\n",
    "    mlp_cv_scores.append(rmse)\n",
    "\n",
    "print(\"\\nMLP CV scores:\", mlp_cv_scores)\n",
    "print(\"MLP 평균 RMSE:\", np.mean(mlp_cv_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "997dbe62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_11076\\2424086588.py:48: UnderReviewWarning: The feature LinearWarmupCosineAnnealingLR is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  scheduler = LinearWarmupCosineAnnealingLR(\n"
     ]
    }
   ],
   "source": [
    "# k-fold 끝난 뒤, 단독 학습 시작 전에 반드시 환경 초기화 및 새 객체 생성\n",
    "\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "def reset_all(seed=42):\n",
    "    import random\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    import os\n",
    "    \n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "reset_all(CFG['SEED'])\n",
    "\n",
    "# 단독 학습용 데이터셋, DataLoader 다시 만들기\n",
    "X_train = prepare_features(train_df_split)\n",
    "X_valid = prepare_features(valid_df_split)\n",
    "y_train = train_df_split['pIC50'].values.astype(np.float32)\n",
    "y_valid = valid_df_split['pIC50'].values.astype(np.float32)\n",
    "transform = VarianceThreshold(threshold=0.05)\n",
    "X_train_tr = transform.fit_transform(X_train)\n",
    "X_valid_tr = transform.transform(X_valid)\n",
    "train_dataset = CustomDataset(X_train_tr, y_train)\n",
    "valid_dataset = CustomDataset(X_valid_tr, y_valid)\n",
    "train_loader = DataLoader(train_dataset, batch_size=CFG_model['BATCH_SIZE'], shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=CFG_model['BATCH_SIZE'], shuffle=False)\n",
    "\n",
    "# 단독 학습용 모델, optimizer, scheduler 새로 생성\n",
    "model = Net(\n",
    "    input_size=train_dataset.X.shape[1],\n",
    "    hidden_size=CFG_model['HIDDEN_SIZE'],\n",
    "    dropout_rate=CFG_model['DROPOUT_RATE'],\n",
    "    out_size=CFG_model['OUTPUT_SIZE']\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CFG_model['LEARNING_RATE'], weight_decay=1e-4)\n",
    "scheduler = LinearWarmupCosineAnnealingLR(\n",
    "    optimizer,\n",
    "    warmup_epochs=CFG_model['WARMUP_EPOCHS'],\n",
    "    max_epochs=CFG_model['MAX_EPOCHS'],\n",
    "    eta_min=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1b25967e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Start:\n",
      "Epoch: 1/200, Train Loss: 2.91653, Valid Loss: 2.87985, lr: 0.000488\n",
      "Epoch: 2/200, Train Loss: 2.93323, Valid Loss: 2.72428, lr: 0.000452\n",
      "Epoch: 3/200, Train Loss: 2.84392, Valid Loss: 2.71664, lr: 0.000397\n",
      "Epoch: 4/200, Train Loss: 2.78239, Valid Loss: 2.71135, lr: 0.000328\n",
      "Epoch: 5/200, Train Loss: 2.76081, Valid Loss: 2.75902, lr: 0.000251\n",
      "Epoch: 6/200, Train Loss: 2.75414, Valid Loss: 2.71856, lr: 0.000173\n",
      "Epoch: 7/200, Train Loss: 2.79026, Valid Loss: 2.73674, lr: 0.000104\n",
      "Epoch: 8/200, Train Loss: 2.81033, Valid Loss: 2.71968, lr: 0.000049\n",
      "Epoch: 9/200, Train Loss: 2.75841, Valid Loss: 2.71315, lr: 0.000013\n",
      "Epoch: 10/200, Train Loss: 2.78749, Valid Loss: 2.71474, lr: 0.000500\n",
      "Epoch: 11/200, Train Loss: 2.77693, Valid Loss: 2.70279, lr: 0.000488\n",
      "Epoch: 12/200, Train Loss: 2.76141, Valid Loss: 2.72232, lr: 0.000452\n",
      "Epoch: 13/200, Train Loss: 2.83134, Valid Loss: 2.95691, lr: 0.000397\n",
      "Epoch: 14/200, Train Loss: 3.28802, Valid Loss: 2.85353, lr: 0.000328\n",
      "Epoch: 15/200, Train Loss: 2.85853, Valid Loss: 2.72119, lr: 0.000251\n",
      "Epoch: 16/200, Train Loss: 2.76228, Valid Loss: 2.69322, lr: 0.000173\n",
      "Epoch: 17/200, Train Loss: 2.78874, Valid Loss: 2.69344, lr: 0.000104\n",
      "Epoch: 18/200, Train Loss: 2.68468, Valid Loss: 2.69372, lr: 0.000049\n",
      "Epoch: 19/200, Train Loss: 2.78287, Valid Loss: 2.69172, lr: 0.000013\n",
      "Epoch: 20/200, Train Loss: 2.73583, Valid Loss: 2.69137, lr: 0.000500\n",
      "Epoch: 21/200, Train Loss: 2.83714, Valid Loss: 2.69782, lr: 0.000488\n",
      "Epoch: 22/200, Train Loss: 2.76431, Valid Loss: 2.71921, lr: 0.000452\n",
      "Epoch: 23/200, Train Loss: 2.76323, Valid Loss: 2.72223, lr: 0.000397\n",
      "Epoch: 24/200, Train Loss: 2.76729, Valid Loss: 2.72215, lr: 0.000328\n",
      "Epoch: 25/200, Train Loss: 2.82631, Valid Loss: 2.69774, lr: 0.000251\n",
      "Epoch: 26/200, Train Loss: 2.77339, Valid Loss: 2.71013, lr: 0.000173\n",
      "Epoch: 27/200, Train Loss: 2.85646, Valid Loss: 2.67937, lr: 0.000104\n",
      "Epoch: 28/200, Train Loss: 2.70366, Valid Loss: 2.69457, lr: 0.000049\n",
      "Epoch: 29/200, Train Loss: 2.71910, Valid Loss: 2.68881, lr: 0.000013\n",
      "Epoch: 30/200, Train Loss: 2.71687, Valid Loss: 2.68975, lr: 0.000500\n",
      "Epoch: 31/200, Train Loss: 2.79704, Valid Loss: 2.67694, lr: 0.000488\n",
      "Epoch: 32/200, Train Loss: 2.97056, Valid Loss: 2.94696, lr: 0.000452\n",
      "Epoch: 33/200, Train Loss: 2.91536, Valid Loss: 3.15779, lr: 0.000397\n",
      "Epoch: 34/200, Train Loss: 3.01367, Valid Loss: 3.16505, lr: 0.000328\n",
      "Epoch: 35/200, Train Loss: 2.80513, Valid Loss: 2.69639, lr: 0.000251\n",
      "Epoch: 36/200, Train Loss: 2.80521, Valid Loss: 2.66938, lr: 0.000173\n",
      "Epoch: 37/200, Train Loss: 2.77082, Valid Loss: 2.66722, lr: 0.000104\n",
      "Epoch: 38/200, Train Loss: 2.73823, Valid Loss: 2.68843, lr: 0.000049\n",
      "Epoch: 39/200, Train Loss: 2.73522, Valid Loss: 2.67710, lr: 0.000013\n",
      "Epoch: 40/200, Train Loss: 2.74218, Valid Loss: 2.67286, lr: 0.000500\n",
      "Epoch: 41/200, Train Loss: 2.92194, Valid Loss: 2.69679, lr: 0.000488\n",
      "Epoch: 42/200, Train Loss: 3.14462, Valid Loss: 2.84882, lr: 0.000452\n",
      "Epoch: 43/200, Train Loss: 2.96050, Valid Loss: 2.77946, lr: 0.000397\n",
      "Epoch: 44/200, Train Loss: 2.70162, Valid Loss: 2.77561, lr: 0.000328\n",
      "Epoch: 45/200, Train Loss: 2.81614, Valid Loss: 2.67193, lr: 0.000251\n",
      "Epoch: 46/200, Train Loss: 2.84994, Valid Loss: 2.66636, lr: 0.000173\n",
      "Epoch: 47/200, Train Loss: 2.72047, Valid Loss: 2.65501, lr: 0.000104\n",
      "Epoch: 48/200, Train Loss: 2.68461, Valid Loss: 2.66063, lr: 0.000049\n",
      "Epoch: 49/200, Train Loss: 2.73462, Valid Loss: 2.66493, lr: 0.000013\n",
      "Epoch: 50/200, Train Loss: 2.76677, Valid Loss: 2.66404, lr: 0.000500\n",
      "Epoch: 51/200, Train Loss: 2.79451, Valid Loss: 2.65122, lr: 0.000488\n",
      "Epoch: 52/200, Train Loss: 2.73596, Valid Loss: 2.66426, lr: 0.000452\n",
      "Epoch: 53/200, Train Loss: 2.70190, Valid Loss: 2.99276, lr: 0.000397\n",
      "Epoch: 54/200, Train Loss: 2.92905, Valid Loss: 2.90407, lr: 0.000328\n",
      "Epoch: 55/200, Train Loss: 2.77585, Valid Loss: 2.64080, lr: 0.000251\n",
      "Epoch: 56/200, Train Loss: 2.79982, Valid Loss: 2.66260, lr: 0.000173\n",
      "Epoch: 57/200, Train Loss: 2.78373, Valid Loss: 2.67102, lr: 0.000104\n",
      "Epoch: 58/200, Train Loss: 2.73025, Valid Loss: 2.65677, lr: 0.000049\n",
      "Epoch: 59/200, Train Loss: 2.66220, Valid Loss: 2.64249, lr: 0.000013\n",
      "Epoch: 60/200, Train Loss: 2.62546, Valid Loss: 2.64205, lr: 0.000500\n",
      "Epoch: 61/200, Train Loss: 2.64420, Valid Loss: 2.73284, lr: 0.000488\n",
      "Epoch: 62/200, Train Loss: 2.74858, Valid Loss: 2.66485, lr: 0.000452\n",
      "Epoch: 63/200, Train Loss: 2.66078, Valid Loss: 2.65532, lr: 0.000397\n",
      "Epoch: 64/200, Train Loss: 2.70057, Valid Loss: 2.65626, lr: 0.000328\n",
      "Epoch: 65/200, Train Loss: 2.66459, Valid Loss: 2.61818, lr: 0.000251\n",
      "Epoch: 66/200, Train Loss: 2.65291, Valid Loss: 2.63144, lr: 0.000173\n",
      "Epoch: 67/200, Train Loss: 2.65762, Valid Loss: 2.66270, lr: 0.000104\n",
      "Epoch: 68/200, Train Loss: 2.75088, Valid Loss: 2.60712, lr: 0.000049\n",
      "Epoch: 69/200, Train Loss: 2.59517, Valid Loss: 2.62588, lr: 0.000013\n",
      "Epoch: 70/200, Train Loss: 2.64357, Valid Loss: 2.61725, lr: 0.000500\n",
      "Epoch: 71/200, Train Loss: 2.64192, Valid Loss: 2.78919, lr: 0.000488\n",
      "Epoch: 72/200, Train Loss: 2.92078, Valid Loss: 3.28481, lr: 0.000452\n",
      "Epoch: 73/200, Train Loss: 2.97092, Valid Loss: 3.39235, lr: 0.000397\n",
      "Epoch: 74/200, Train Loss: 2.86859, Valid Loss: 2.97545, lr: 0.000328\n",
      "Epoch: 75/200, Train Loss: 2.78638, Valid Loss: 2.80043, lr: 0.000251\n",
      "Epoch: 76/200, Train Loss: 2.65524, Valid Loss: 2.59448, lr: 0.000173\n",
      "Epoch: 77/200, Train Loss: 2.61164, Valid Loss: 2.59233, lr: 0.000104\n",
      "Epoch: 78/200, Train Loss: 2.65635, Valid Loss: 2.59434, lr: 0.000049\n",
      "Epoch: 79/200, Train Loss: 2.58549, Valid Loss: 2.59749, lr: 0.000013\n",
      "Epoch: 80/200, Train Loss: 2.62541, Valid Loss: 2.59940, lr: 0.000500\n",
      "Epoch: 81/200, Train Loss: 2.65409, Valid Loss: 2.58397, lr: 0.000488\n",
      "Epoch: 82/200, Train Loss: 2.62640, Valid Loss: 3.00624, lr: 0.000452\n",
      "Epoch: 83/200, Train Loss: 2.78343, Valid Loss: 2.61702, lr: 0.000397\n",
      "Epoch: 84/200, Train Loss: 2.58221, Valid Loss: 2.68683, lr: 0.000328\n",
      "Epoch: 85/200, Train Loss: 2.62536, Valid Loss: 2.56021, lr: 0.000251\n",
      "Epoch: 86/200, Train Loss: 2.61757, Valid Loss: 2.56232, lr: 0.000173\n",
      "Epoch: 87/200, Train Loss: 2.52407, Valid Loss: 2.57733, lr: 0.000104\n",
      "Epoch: 88/200, Train Loss: 2.53215, Valid Loss: 2.55810, lr: 0.000049\n",
      "Epoch: 89/200, Train Loss: 2.53441, Valid Loss: 2.55676, lr: 0.000013\n",
      "Epoch: 90/200, Train Loss: 2.49619, Valid Loss: 2.55547, lr: 0.000500\n",
      "Epoch: 91/200, Train Loss: 2.52345, Valid Loss: 2.62408, lr: 0.000488\n",
      "Epoch: 92/200, Train Loss: 2.67935, Valid Loss: 4.74823, lr: 0.000452\n",
      "Epoch: 93/200, Train Loss: 3.74303, Valid Loss: 4.03383, lr: 0.000397\n",
      "Epoch: 94/200, Train Loss: 3.01035, Valid Loss: 3.57483, lr: 0.000328\n",
      "Epoch: 95/200, Train Loss: 2.92874, Valid Loss: 2.80911, lr: 0.000251\n",
      "Epoch: 96/200, Train Loss: 2.69904, Valid Loss: 2.57231, lr: 0.000173\n",
      "Epoch: 97/200, Train Loss: 2.58162, Valid Loss: 2.57993, lr: 0.000104\n",
      "Epoch: 98/200, Train Loss: 2.60526, Valid Loss: 2.56895, lr: 0.000049\n",
      "Epoch: 99/200, Train Loss: 2.57213, Valid Loss: 2.57397, lr: 0.000013\n",
      "Epoch: 100/200, Train Loss: 2.55852, Valid Loss: 2.57236, lr: 0.000500\n",
      "Epoch: 101/200, Train Loss: 2.55304, Valid Loss: 2.56411, lr: 0.000488\n",
      "Epoch: 102/200, Train Loss: 2.57945, Valid Loss: 2.65521, lr: 0.000452\n",
      "Epoch: 103/200, Train Loss: 2.69683, Valid Loss: 2.87738, lr: 0.000397\n",
      "Epoch: 104/200, Train Loss: 2.64839, Valid Loss: 2.55289, lr: 0.000328\n",
      "Epoch: 105/200, Train Loss: 2.50980, Valid Loss: 2.58381, lr: 0.000251\n",
      "Epoch: 106/200, Train Loss: 2.57003, Valid Loss: 2.53193, lr: 0.000173\n",
      "Epoch: 107/200, Train Loss: 2.50869, Valid Loss: 2.55340, lr: 0.000104\n",
      "Epoch: 108/200, Train Loss: 2.56908, Valid Loss: 2.53149, lr: 0.000049\n",
      "Epoch: 109/200, Train Loss: 2.53285, Valid Loss: 2.52858, lr: 0.000013\n",
      "Epoch: 110/200, Train Loss: 2.53380, Valid Loss: 2.52907, lr: 0.000500\n",
      "Epoch: 111/200, Train Loss: 2.47467, Valid Loss: 2.52433, lr: 0.000488\n",
      "Epoch: 112/200, Train Loss: 2.65672, Valid Loss: 2.57037, lr: 0.000452\n",
      "Epoch: 113/200, Train Loss: 2.59178, Valid Loss: 2.52568, lr: 0.000397\n",
      "Epoch: 114/200, Train Loss: 2.50915, Valid Loss: 2.51845, lr: 0.000328\n",
      "Epoch: 115/200, Train Loss: 2.46823, Valid Loss: 2.48941, lr: 0.000251\n",
      "Epoch: 116/200, Train Loss: 2.44191, Valid Loss: 2.49252, lr: 0.000173\n",
      "Epoch: 117/200, Train Loss: 2.47358, Valid Loss: 2.48636, lr: 0.000104\n",
      "Epoch: 118/200, Train Loss: 2.46169, Valid Loss: 2.49431, lr: 0.000049\n",
      "Epoch: 119/200, Train Loss: 2.49309, Valid Loss: 2.48582, lr: 0.000013\n",
      "Epoch: 120/200, Train Loss: 2.49078, Valid Loss: 2.48386, lr: 0.000500\n",
      "Epoch: 121/200, Train Loss: 2.59306, Valid Loss: 2.49119, lr: 0.000488\n",
      "Epoch: 122/200, Train Loss: 2.42590, Valid Loss: 2.47542, lr: 0.000452\n",
      "Epoch: 123/200, Train Loss: 2.45593, Valid Loss: 2.49977, lr: 0.000397\n",
      "Epoch: 124/200, Train Loss: 2.44700, Valid Loss: 2.49294, lr: 0.000328\n",
      "Epoch: 125/200, Train Loss: 2.49477, Valid Loss: 2.45129, lr: 0.000251\n",
      "Epoch: 126/200, Train Loss: 2.44876, Valid Loss: 2.45249, lr: 0.000173\n",
      "Epoch: 127/200, Train Loss: 2.39327, Valid Loss: 2.50804, lr: 0.000104\n",
      "Epoch: 128/200, Train Loss: 2.47185, Valid Loss: 2.47352, lr: 0.000049\n",
      "Epoch: 129/200, Train Loss: 2.41008, Valid Loss: 2.50629, lr: 0.000013\n",
      "Epoch: 130/200, Train Loss: 2.41424, Valid Loss: 2.48292, lr: 0.000500\n",
      "Epoch: 131/200, Train Loss: 2.78324, Valid Loss: 2.62096, lr: 0.000488\n",
      "Epoch: 132/200, Train Loss: 2.53969, Valid Loss: 2.46355, lr: 0.000452\n",
      "Epoch: 133/200, Train Loss: 2.43595, Valid Loss: 2.46543, lr: 0.000397\n",
      "Epoch: 134/200, Train Loss: 2.52767, Valid Loss: 3.80396, lr: 0.000328\n",
      "Epoch: 135/200, Train Loss: 2.93356, Valid Loss: 2.46521, lr: 0.000251\n",
      "Epoch: 136/200, Train Loss: 2.65262, Valid Loss: 2.45157, lr: 0.000173\n",
      "Epoch: 137/200, Train Loss: 2.61121, Valid Loss: 2.53500, lr: 0.000104\n",
      "Epoch: 138/200, Train Loss: 2.63142, Valid Loss: 2.45910, lr: 0.000049\n",
      "Epoch: 139/200, Train Loss: 2.46164, Valid Loss: 2.44706, lr: 0.000013\n",
      "Epoch: 140/200, Train Loss: 2.43977, Valid Loss: 2.44989, lr: 0.000500\n",
      "Epoch: 141/200, Train Loss: 2.36950, Valid Loss: 2.43819, lr: 0.000488\n",
      "Epoch: 142/200, Train Loss: 2.37874, Valid Loss: 2.73911, lr: 0.000452\n",
      "Epoch: 143/200, Train Loss: 2.42512, Valid Loss: 2.64072, lr: 0.000397\n",
      "Epoch: 144/200, Train Loss: 2.48486, Valid Loss: 2.62737, lr: 0.000328\n",
      "Epoch: 145/200, Train Loss: 2.51898, Valid Loss: 2.53636, lr: 0.000251\n",
      "Epoch: 146/200, Train Loss: 2.43707, Valid Loss: 2.44660, lr: 0.000173\n",
      "Epoch: 147/200, Train Loss: 2.48384, Valid Loss: 2.43250, lr: 0.000104\n",
      "Epoch: 148/200, Train Loss: 2.37424, Valid Loss: 2.41265, lr: 0.000049\n",
      "Epoch: 149/200, Train Loss: 2.37385, Valid Loss: 2.41108, lr: 0.000013\n",
      "Epoch: 150/200, Train Loss: 2.34319, Valid Loss: 2.41179, lr: 0.000500\n",
      "Epoch: 151/200, Train Loss: 2.43711, Valid Loss: 2.40320, lr: 0.000488\n",
      "Epoch: 152/200, Train Loss: 2.42613, Valid Loss: 2.47623, lr: 0.000452\n",
      "Epoch: 153/200, Train Loss: 2.44965, Valid Loss: 2.49683, lr: 0.000397\n",
      "Epoch: 154/200, Train Loss: 2.46714, Valid Loss: 2.49886, lr: 0.000328\n",
      "Epoch: 155/200, Train Loss: 2.44415, Valid Loss: 2.44352, lr: 0.000251\n",
      "Epoch: 156/200, Train Loss: 2.41628, Valid Loss: 2.38080, lr: 0.000173\n",
      "Epoch: 157/200, Train Loss: 2.42333, Valid Loss: 2.42180, lr: 0.000104\n",
      "Epoch: 158/200, Train Loss: 2.35387, Valid Loss: 2.38876, lr: 0.000049\n",
      "Epoch: 159/200, Train Loss: 2.38311, Valid Loss: 2.37542, lr: 0.000013\n",
      "Epoch: 160/200, Train Loss: 2.37679, Valid Loss: 2.38014, lr: 0.000500\n",
      "Epoch: 161/200, Train Loss: 2.37264, Valid Loss: 2.55363, lr: 0.000488\n",
      "Epoch: 162/200, Train Loss: 2.46575, Valid Loss: 2.91989, lr: 0.000452\n",
      "Epoch: 163/200, Train Loss: 2.64473, Valid Loss: 2.98435, lr: 0.000397\n",
      "Epoch: 164/200, Train Loss: 2.60235, Valid Loss: 2.54346, lr: 0.000328\n",
      "Epoch: 165/200, Train Loss: 2.41378, Valid Loss: 2.42167, lr: 0.000251\n",
      "Epoch: 166/200, Train Loss: 2.38399, Valid Loss: 2.37184, lr: 0.000173\n",
      "Epoch: 167/200, Train Loss: 2.32162, Valid Loss: 2.39489, lr: 0.000104\n",
      "Epoch: 168/200, Train Loss: 2.34531, Valid Loss: 2.38788, lr: 0.000049\n",
      "Epoch: 169/200, Train Loss: 2.30942, Valid Loss: 2.37777, lr: 0.000013\n",
      "Epoch: 170/200, Train Loss: 2.39186, Valid Loss: 2.37013, lr: 0.000500\n",
      "Epoch: 171/200, Train Loss: 2.39381, Valid Loss: 2.37001, lr: 0.000488\n",
      "Epoch: 172/200, Train Loss: 2.32833, Valid Loss: 2.37704, lr: 0.000452\n",
      "Epoch: 173/200, Train Loss: 2.36793, Valid Loss: 2.71895, lr: 0.000397\n",
      "Epoch: 174/200, Train Loss: 2.49911, Valid Loss: 2.41826, lr: 0.000328\n",
      "Epoch: 175/200, Train Loss: 2.40240, Valid Loss: 2.67906, lr: 0.000251\n",
      "Epoch: 176/200, Train Loss: 2.33066, Valid Loss: 2.35806, lr: 0.000173\n",
      "Epoch: 177/200, Train Loss: 2.37414, Valid Loss: 2.36257, lr: 0.000104\n",
      "Epoch: 178/200, Train Loss: 2.30074, Valid Loss: 2.35450, lr: 0.000049\n",
      "Epoch: 179/200, Train Loss: 2.31847, Valid Loss: 2.37196, lr: 0.000013\n",
      "Epoch: 180/200, Train Loss: 2.25764, Valid Loss: 2.37552, lr: 0.000500\n",
      "Epoch: 181/200, Train Loss: 2.32610, Valid Loss: 2.37948, lr: 0.000488\n",
      "Epoch: 182/200, Train Loss: 2.33488, Valid Loss: 2.57927, lr: 0.000452\n",
      "Epoch: 183/200, Train Loss: 2.41900, Valid Loss: 2.42779, lr: 0.000397\n",
      "Epoch: 184/200, Train Loss: 2.41252, Valid Loss: 2.44809, lr: 0.000328\n",
      "Epoch: 185/200, Train Loss: 2.35594, Valid Loss: 2.58070, lr: 0.000251\n",
      "Epoch: 186/200, Train Loss: 2.29910, Valid Loss: 2.34370, lr: 0.000173\n",
      "Epoch: 187/200, Train Loss: 2.26961, Valid Loss: 2.44994, lr: 0.000104\n",
      "Epoch: 188/200, Train Loss: 2.40335, Valid Loss: 2.41701, lr: 0.000049\n",
      "Epoch: 189/200, Train Loss: 2.30764, Valid Loss: 2.34779, lr: 0.000013\n",
      "Epoch: 190/200, Train Loss: 2.27104, Valid Loss: 2.34401, lr: 0.000500\n",
      "Epoch: 191/200, Train Loss: 2.25568, Valid Loss: 2.40522, lr: 0.000488\n",
      "Epoch: 192/200, Train Loss: 2.38891, Valid Loss: 2.51202, lr: 0.000452\n",
      "Epoch: 193/200, Train Loss: 2.36351, Valid Loss: 2.48164, lr: 0.000397\n",
      "Epoch: 194/200, Train Loss: 2.48757, Valid Loss: 2.65301, lr: 0.000328\n",
      "Epoch: 195/200, Train Loss: 2.39146, Valid Loss: 2.34280, lr: 0.000251\n",
      "Epoch: 196/200, Train Loss: 2.32377, Valid Loss: 2.37348, lr: 0.000173\n",
      "Epoch: 197/200, Train Loss: 2.58206, Valid Loss: 2.36080, lr: 0.000104\n",
      "Epoch: 198/200, Train Loss: 2.36965, Valid Loss: 2.33685, lr: 0.000049\n",
      "Epoch: 199/200, Train Loss: 2.29846, Valid Loss: 2.33781, lr: 0.000013\n",
      "Epoch: 200/200, Train Loss: 2.23529, Valid Loss: 2.33905, lr: 0.000500\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Start:\")\n",
    "model.to(device)\n",
    "model_train, train_losses, valid_losses, learning_rates = train(train_loader, valid_loader, model, criterion, optimizer, scheduler, epochs=CFG_model['EPOCHS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "da59a9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rmse(model, valid_loader, criterion):\n",
    "    model.eval()\n",
    "    valid_running_loss = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in valid_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs).squeeze(dim=1)\n",
    "            loss = criterion(outputs, targets)\n",
    "            batch_size = inputs.size(0)\n",
    "            valid_running_loss += loss.item() * batch_size\n",
    "            total_samples += batch_size\n",
    "    \n",
    "    avg_loss = valid_running_loss / total_samples\n",
    "    rmse = math.sqrt(avg_loss)\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "41f80899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최종 Validation RMSE: 1.5293937247293872\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "# 최종 RMSE 계산\n",
    "final_rmse = evaluate_rmse(model, valid_loader, criterion)\n",
    "print(\"최종 Validation RMSE:\", final_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f051efd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP CV mean score saved: 1.4870013834694793\n"
     ]
    }
   ],
   "source": [
    "# 앙상블을 위한 가중치 파일 저장\n",
    "# mlp_cv_scores\n",
    "# [np.float64(1.4735811248471444),\n",
    "#  np.float64(1.4590740801849447),\n",
    "#  np.float64(1.4521791715262042),\n",
    "#  np.float64(1.5729801947246587),\n",
    "#  np.float64(1.4046790089843888)]\n",
    "\n",
    "import json\n",
    "\n",
    "# 평균 점수 계산\n",
    "mlp_mean_score = np.mean(mlp_cv_scores)\n",
    "\n",
    "# JSON 형식으로 저장 (가중치 계산용)\n",
    "with open('mlp_cv_score.json', 'w') as f:\n",
    "    json.dump({'mean_score': float(mlp_mean_score)}, f)\n",
    "\n",
    "print(f\"MLP CV mean score saved: {mlp_mean_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa5ca4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_curves(train_loss, valid_loss, lrs):\n",
    "    epochs = list(range(1, len(train_loss) + 1))\n",
    "    \n",
    "    # Loss\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_loss, label='Train Loss')\n",
    "    plt.plot(epochs, valid_loss, label='Valid Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Train vs Valid Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # LR\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, lrs, label='Learning Rate', color='orange')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.title('Learning Rate Schedule')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "21e4773a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train/Validation loss 및 learning rate 시각화\n",
    "plot_training_curves(train_losses, valid_losses, learning_rates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ad663b",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "45bd3d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"C:/Users/user/Desktop/dacon_drug_development/dataset/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "430d423e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:14:09] DEPRECATION WARNING: please use MorganGenerator\n"
     ]
    }
   ],
   "source": [
    "# test dataset preprocessing\n",
    "test['Fingerprint'] = test['Smiles'].apply(smiles_to_fingerprint)\n",
    "test = test[test['Fingerprint'].notnull()]\n",
    "\n",
    "test_features_df = pd.DataFrame([descriptors(s) for s in test['Smiles']])\n",
    "test_final_dataset = pd.concat([test, test_features_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4886eea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading for test dataset\n",
    "\n",
    "test_dataset = test_final_dataset.drop(['ID', 'Smiles'], axis=1)\n",
    "# test 데이터 feature 준비\n",
    "X_test = prepare_features(test_dataset)\n",
    "# transform; train에서 fit, test에는 transform만.\n",
    "X_test_tr = transform.transform(X_test)\n",
    "# CustomDataset\n",
    "test_dataset = CustomDataset(X_test_tr, y=None)\n",
    "# DataLoader\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=CFG_model['BATCH_SIZE'], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "368d71c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(test_loader, model):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            output = model(inputs).squeeze(dim=1)  # torch.Size([batch_size])\n",
    "            # 이거 shape 확인해봐야함. \n",
    "            preds.extend(output.cpu().numpy().flatten().tolist())\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5681ad28",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = inference(test_loader, model_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1db601ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앙상블을 위한 예측값 저장\n",
    "np.save('mlp_test_preds.npy', np.array(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2854abfb",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ce8bfe5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('C:/Users/user/Desktop/dacon_drug_development/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8f1e97b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>ASK1_IC50_nM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_000</td>\n",
       "      <td>8.270438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_001</td>\n",
       "      <td>8.416474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_002</td>\n",
       "      <td>8.416474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_003</td>\n",
       "      <td>8.829744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_004</td>\n",
       "      <td>8.360770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>TEST_122</td>\n",
       "      <td>8.134489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>TEST_123</td>\n",
       "      <td>8.330828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>TEST_124</td>\n",
       "      <td>8.157233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>TEST_125</td>\n",
       "      <td>8.167944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>TEST_126</td>\n",
       "      <td>7.977779</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>127 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID  ASK1_IC50_nM\n",
       "0    TEST_000      8.270438\n",
       "1    TEST_001      8.416474\n",
       "2    TEST_002      8.416474\n",
       "3    TEST_003      8.829744\n",
       "4    TEST_004      8.360770\n",
       "..        ...           ...\n",
       "122  TEST_122      8.134489\n",
       "123  TEST_123      8.330828\n",
       "124  TEST_124      8.157233\n",
       "125  TEST_125      8.167944\n",
       "126  TEST_126      7.977779\n",
       "\n",
       "[127 rows x 2 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission['ASK1_IC50_nM'] = predictions\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1593e1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('C:/Users/user/Desktop/dacon_drug_development/MLP_3_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
