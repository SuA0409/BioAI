{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d1a03e6",
   "metadata": {},
   "source": [
    "### Installation & Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39c26167",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pl_bolts\\__init__.py:11: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(numpy, tp_name):\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\lightning_fabric\\__init__.py:29: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __import__(\"pkg_resources\").declare_namespace(__name__)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pl_bolts\\models\\self_supervised\\amdim\\amdim_module.py:34: UnderReviewWarning: The feature generate_power_seq is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  \"lr_options\": generate_power_seq(LEARNING_RATE_CIFAR, 11),\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pl_bolts\\models\\self_supervised\\amdim\\amdim_module.py:92: UnderReviewWarning: The feature FeatureMapContrastiveTask is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  contrastive_task: Union[FeatureMapContrastiveTask] = FeatureMapContrastiveTask(\"01, 02, 11\"),\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pl_bolts\\losses\\self_supervised_learning.py:228: UnderReviewWarning: The feature AmdimNCELoss is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  self.nce_loss = AmdimNCELoss(tclip)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, Descriptors, Lipinski\n",
    "from rdkit import DataStructs\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from pl_bolts.optimizers.lr_scheduler import LinearWarmupCosineAnnealingLR\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "785f170c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'NBITS': 2048,\n",
    "    'SEED': 42\n",
    "}\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True  # 재현성\n",
    "    torch.backends.cudnn.benchmark = False   #안정성\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "seed_everything(CFG['SEED'])\n",
    "\n",
    "# SMILES 데이터를 분자 지문으로 변환\n",
    "def smiles_to_fingerprint(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is not None:\n",
    "        fp = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=CFG['NBITS'])\n",
    "        return np.array(fp)\n",
    "    else:\n",
    "        return np.zeros((CFG['NBITS'],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c6221a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def IC50_to_pIC50(ic50_nM):\n",
    "    ic50_nM = np.clip(ic50_nM, 1e-10, None)\n",
    "    return 9 - np.log10(ic50_nM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f46b689",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pIC50_to_IC50(pIC50):\n",
    "    return 10 ** (9 - pIC50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189fd78c",
   "metadata": {},
   "source": [
    "### Data loading & Molecular Descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "f49f4aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:07] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n"
     ]
    }
   ],
   "source": [
    "IC50_dataset = pd.read_csv(\"C:/Users/user/Desktop/dacon_drug_development/IC50_dataset.csv\")\n",
    "IC50_dataset['Fingerprint'] = IC50_dataset['smiles'].apply(smiles_to_fingerprint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354f96c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def descriptors(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    f = {}\n",
    "    # 1. functional groups (IC50 낮춤)\n",
    "    tetrazole_smarts = ['c1nn[n+](n1)[O-]', 'c1[nH]nnn1']\n",
    "    f['has_tetrazole'] = any(mol.HasSubstructMatch(Chem.MolFromSmarts(s)) for s in tetrazole_smarts)\n",
    "    f['tetrazole_effect'] = 1 if f['has_tetrazole'] else 0\n",
    "    #\n",
    "    f['has_triazole'] = mol.HasSubstructMatch(Chem.MolFromSmarts('c1nnc(n1)'))\n",
    "    f['triazole_effect'] = 1 if f['has_triazole'] else 0\n",
    "    #\n",
    "    f['has_sulfoxide'] = mol.HasSubstructMatch(Chem.MolFromSmarts('S(=O)(C)'))\n",
    "    f['sulfoxide_effect'] = 1 if f['has_sulfoxide'] else 0\n",
    "    #\n",
    "    amide_smarts = ['C(=O)N', 'NC(=O)']\n",
    "    f['has_amide'] = any(mol.HasSubstructMatch(Chem.MolFromSmarts(s)) for s in amide_smarts)\n",
    "    f['amide_effect'] = 1 if f['has_amide'] else 0\n",
    "    #\n",
    "    f['has_sulfonamide'] = mol.HasSubstructMatch(Chem.MolFromSmarts('S(=O)(=O)N'))\n",
    "    f['sulfonamide_effect'] = 1 if f['has_sulfonamide'] else 0\n",
    "\n",
    "    # 2. 분자량 (IC50 낮춤)\n",
    "    mw = Descriptors.MolWt(mol)\n",
    "    f['mw'] = mw\n",
    "    f['mw_effect'] = 1 if 300 <= mw <= 500 else -1\n",
    "    \n",
    "\n",
    "    # 3. logP (IC50 낮춤)\n",
    "    logp = Descriptors.MolLogP(mol)\n",
    "    f['logP'] = logp\n",
    "    if 2 <= logp <= 4:\n",
    "        f['logP_effect'] = 1\n",
    "    else:\n",
    "        f['logP_effect'] = -1\n",
    "\n",
    "    f['logP_too_low'] = logp < 1\n",
    "    f['logP_too_high'] = logp > 5\n",
    "    f['logP_too_low_effect'] = -1 if f['logP_too_low'] else 0\n",
    "    f['logP_too_high_effect'] = -1 if f['logP_too_high'] else 0\n",
    "\n",
    "    # 4. TPSA (IC50 낮춤)\n",
    "    tpsa = Descriptors.TPSA(mol)\n",
    "    f['TPSA'] = tpsa\n",
    "    if 60 <= tpsa <= 120:\n",
    "        f['TPSA_effect'] = 1\n",
    "    else:\n",
    "        f['TPSA_effect'] = -1\n",
    "\n",
    "    # 5. Rotatable bonds (IC50 높임)\n",
    "    rot = Lipinski.NumRotatableBonds(mol)\n",
    "    f['rotatable'] = rot\n",
    "    f['rot_effect'] = -1 if rot > 7 else 1\n",
    "\n",
    "    # 6. Rings & aromatic rings (IC50 낮춤)\n",
    "    num_rings = mol.GetRingInfo().NumRings()\n",
    "    f['num_rings'] = num_rings\n",
    "    f['ring_count_ge_2'] = num_rings >= 2\n",
    "    f['ring_count_ge_2_effect'] = 1 if f['ring_count_ge_2'] else 0\n",
    "    #\n",
    "    ssr = Chem.GetSymmSSSR(mol)\n",
    "    aromatic_ring = any(all(mol.GetAtomWithIdx(idx).GetIsAromatic() for idx in ring) for ring in ssr)\n",
    "    f['has_aromatic_ring'] = aromatic_ring\n",
    "    f['has_aromatic_ring_effect'] = 1 if aromatic_ring else 0\n",
    "    #\n",
    "    f['ring_ge2_and_aromatic'] = f['ring_count_ge_2'] and f['has_aromatic_ring']\n",
    "    f['ring_ge2_and_aromatic_effect'] = 1 if f['ring_ge2_and_aromatic'] else 0\n",
    "\n",
    "    # 7. pKa 관련 작용기 (amine, imidazole, pyridine)\n",
    "    amine_smarts = ['[NX3;H2,H1;!$(NC=O)]', '[NX3;H0;!$(NC=O)]']\n",
    "    f['has_amine'] = any(mol.HasSubstructMatch(Chem.MolFromSmarts(s)) for s in amine_smarts)\n",
    "    f['has_imidazole'] = mol.HasSubstructMatch(Chem.MolFromSmarts('c1cnc[nH]1'))\n",
    "    f['has_pyridine'] = mol.HasSubstructMatch(Chem.MolFromSmarts('c1ccncc1'))\n",
    "\n",
    "    f['pKa_related_effect'] = 1 if (f['has_amine'] or f['has_imidazole'] or f['has_pyridine']) else 0\n",
    "\n",
    "    # 8. H-bond acceptor 음의 partial charge 작용기 count & 효과\n",
    "    sulfoxide_oxygen_smarts = '[O]=S'\n",
    "    f['sulfoxide_oxygen_count'] = len(mol.GetSubstructMatches(Chem.MolFromSmarts(sulfoxide_oxygen_smarts)))\n",
    "    f['tetrazole_nitrogen_count'] = len(mol.GetSubstructMatches(Chem.MolFromSmarts('n'))) if f['has_tetrazole'] else 0\n",
    "    f['hbond_acceptor_effect'] = 1 if (f['sulfoxide_oxygen_count'] + f['tetrazole_nitrogen_count'] > 0) else 0\n",
    "\n",
    "    # 9. 중극성 지표 (ATP-binding pocket 유사성)\n",
    "    f['mid_polarity'] = 1 if (60 <= tpsa <= 120 and 2 <= logp <= 4) else 0\n",
    "    f['mid_polarity_effect'] = 1 if f['mid_polarity'] == 1 else -1\n",
    "\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2434b7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수정1\n",
    "def descriptors(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    f = {}\n",
    "    # 1. functional groups (IC50 낮춤)\n",
    "    tetrazole_smarts = ['c1nn[n+](n1)[O-]', 'c1[nH]nnn1']\n",
    "    f['has_tetrazole'] = any(mol.HasSubstructMatch(Chem.MolFromSmarts(s)) for s in tetrazole_smarts)\n",
    "    f['tetrazole_effect'] = 1 if f['has_tetrazole'] else 0\n",
    "    #\n",
    "    f['has_triazole'] = mol.HasSubstructMatch(Chem.MolFromSmarts('c1nnc(n1)'))\n",
    "    f['triazole_effect'] = 1 if f['has_triazole'] else 0\n",
    "    #\n",
    "    f['has_sulfoxide'] = mol.HasSubstructMatch(Chem.MolFromSmarts('S(=O)(C)'))\n",
    "    f['sulfoxide_effect'] = 1 if f['has_sulfoxide'] else 0\n",
    "    #\n",
    "    amide_smarts = ['C(=O)N', 'NC(=O)']\n",
    "    f['has_amide'] = any(mol.HasSubstructMatch(Chem.MolFromSmarts(s)) for s in amide_smarts)\n",
    "    f['amide_effect'] = 1 if f['has_amide'] else 0\n",
    "    #\n",
    "    f['has_sulfonamide'] = mol.HasSubstructMatch(Chem.MolFromSmarts('S(=O)(=O)N'))\n",
    "    f['sulfonamide_effect'] = 1 if f['has_sulfonamide'] else 0\n",
    "\n",
    "    # 2. 분자량 (IC50 낮춤)\n",
    "    mw = Descriptors.MolWt(mol)\n",
    "    f['mw'] = mw\n",
    "    # 수정1\n",
    "    f['HA'] = Lipinski.HeavyAtomCount(mol)\n",
    "    f['MW_per_HA_sqrt'] = (f['mw'] / f['HA'])**0.5 if f['HA'] > 0 else 0\n",
    "    \n",
    "    # 3. logP (IC50 낮춤)\n",
    "    logp = Descriptors.MolLogP(mol)\n",
    "    f['logP'] = logp\n",
    "    if 2 <= logp <= 4: \n",
    "        f['logP_effect'] = 1 \n",
    "    else: \n",
    "        f['logP_effect'] = -1\n",
    "\n",
    "    f['logP_too_low'] = logp < 1\n",
    "    f['logP_too_high'] = logp > 5\n",
    "    f['logP_too_low_effect'] = -1 if f['logP_too_low'] else 0\n",
    "    f['logP_too_high_effect'] = -1 if f['logP_too_high'] else 0\n",
    "\n",
    "    # 4. TPSA (IC50 낮춤)\n",
    "    tpsa = Descriptors.TPSA(mol)\n",
    "    f['TPSA'] = tpsa\n",
    "    if 60 <= tpsa <= 120: \n",
    "        f['TPSA_effect'] = 1 \n",
    "    else: \n",
    "        f['TPSA_effect'] = -1\n",
    "\n",
    "    # 5. Rotatable bonds (IC50 높임)\n",
    "    rot = Lipinski.NumRotatableBonds(mol)\n",
    "    f['rotatable'] = rot\n",
    "    # 수정2\n",
    "    f['RotB_per_HA_sqrt'] = (f['rotatable'] / f['HA'])**0.5 if f['HA'] > 0 else 0\n",
    "\n",
    "    # 6. Rings & aromatic rings (IC50 낮춤)\n",
    "    num_rings = mol.GetRingInfo().NumRings()\n",
    "    f['num_rings'] = num_rings\n",
    "    f['ring_count_ge_2'] = num_rings >= 2\n",
    "    # 수정3\n",
    "    f['Rings_per_HA_sqrt'] = (f['num_rings'] / f['HA'])**0.5 if f['HA'] > 0 else 0\n",
    "    #\n",
    "    ssr = Chem.GetSymmSSSR(mol)\n",
    "    aromatic_ring = any(all(mol.GetAtomWithIdx(idx).GetIsAromatic() for idx in ring) for ring in ssr)\n",
    "    f['has_aromatic_ring'] = aromatic_ring\n",
    "    f['has_aromatic_ring_effect'] = 1 if aromatic_ring else 0\n",
    "    #\n",
    "    f['ring_ge2_and_aromatic'] = f['ring_count_ge_2'] and f['has_aromatic_ring']\n",
    "    # 수정4 : 삭제\n",
    "\n",
    "    # 7. pKa 관련 작용기 (amine, imidazole, pyridine)\n",
    "    amine_smarts = ['[NX3;H2,H1;!$(NC=O)]', '[NX3;H0;!$(NC=O)]']\n",
    "    f['has_amine'] = any(mol.HasSubstructMatch(Chem.MolFromSmarts(s)) for s in amine_smarts)\n",
    "    f['has_imidazole'] = mol.HasSubstructMatch(Chem.MolFromSmarts('c1cnc[nH]1'))\n",
    "    f['has_pyridine'] = mol.HasSubstructMatch(Chem.MolFromSmarts('c1ccncc1'))\n",
    "\n",
    "    # 8. H-bond acceptor 음의 partial charge 작용기 count & 효과\n",
    "    # 수정5\n",
    "    f['HBD'] = Lipinski.NumHDonors(mol)\n",
    "    f['HBA'] = Lipinski.NumHAcceptors(mol)\n",
    "    f['HBD_density_sq'] = (f['HBD'] / f['HA'])**2 if f['HA'] > 0 else 0\n",
    "    f['HBA_density_sq'] = (f['HBA'] / f['HA'])**2 if f['HA'] > 0 else 0\n",
    "    f['Flex_Polarity_sqrt'] = ((rot / f['HA']) * f['TPSA'])**0.5 if f['HA'] > 0 else 0\n",
    "    #\n",
    "    sulfoxide_oxygen_smarts = '[O]=S'\n",
    "    f['sulfoxide_oxygen_count'] = len(mol.GetSubstructMatches(Chem.MolFromSmarts(sulfoxide_oxygen_smarts)))\n",
    "    f['tetrazole_nitrogen_count'] = len(mol.GetSubstructMatches(Chem.MolFromSmarts('n'))) if f['has_tetrazole'] else 0\n",
    "    f['hbond_acceptor_effect'] = 1 if (f['sulfoxide_oxygen_count'] + f['tetrazole_nitrogen_count'] > 0) else 0\n",
    "\n",
    "    # 9. 중극성 지표 (ATP-binding pocket 유사성)\n",
    "    f['mid_polarity'] = 1 if (60 <= tpsa <= 120 and 2 <= logp <= 4) else 0\n",
    "    f['mid_polarity_effect'] = 1 if f['mid_polarity'] == 1 else -1\n",
    "\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "bedfcd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수정2\n",
    "def descriptors(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    f = {}\n",
    "    # # 1. Functional groups (IC50 낮춤)\n",
    "    tetrazole_smarts = ['c1nn[n+](n1)[O-]', 'c1[nH]nnn1']\n",
    "    # f['has_tetrazole'] = any(mol.HasSubstructMatch(Chem.MolFromSmarts(s)) for s in tetrazole_smarts)\n",
    "    # f['tetrazole_effect'] = 1 if f['has_tetrazole'] else 0\n",
    "    f['tetrazole_count'] = sum(len(mol.GetSubstructMatches(Chem.MolFromSmarts(s))) for s in tetrazole_smarts)\n",
    "    # #\n",
    "    # f['has_triazole'] = mol.HasSubstructMatch(Chem.MolFromSmarts('c1nnc(n1)'))\n",
    "    # f['triazole_effect'] = 1 if f['has_triazole'] else 0\n",
    "    # f['triazole_count'] = len(mol.GetSubstructMatches(Chem.MolFromSmarts('c1nnc(n1)')))\n",
    "    # #\n",
    "    # f['has_sulfoxide'] = mol.HasSubstructMatch(Chem.MolFromSmarts('S(=O)(C)'))\n",
    "    # f['sulfoxide_effect'] = 1 if f['has_sulfoxide'] else 0\n",
    "    # #\n",
    "    # amide_smarts = ['C(=O)N', 'NC(=O)']\n",
    "    # f['has_amide'] = any(mol.HasSubstructMatch(Chem.MolFromSmarts(s)) for s in amide_smarts)\n",
    "    # f['amide_effect'] = 1 if f['has_amide'] else 0\n",
    "    # #\n",
    "    # f['has_sulfonamide'] = mol.HasSubstructMatch(Chem.MolFromSmarts('S(=O)(=O)N'))\n",
    "    # f['sulfonamide_effect'] = 1 if f['has_sulfonamide'] else 0\n",
    "\n",
    "    # # 2. 분자량 / 크기\n",
    "    # mw = Descriptors.MolWt(mol)\n",
    "    # f['mw'] = mw\n",
    "    f['HA'] = Lipinski.HeavyAtomCount(mol)\n",
    "    # f['MW_per_HA_sqrt'] = (f['mw'] / f['HA'])**0.5 if f['HA'] > 0 else 0\n",
    "    # f['MW_per_HA_log'] = np.log1p(f['mw'] / f['HA']) if f['HA'] > 0 else 0\n",
    "\n",
    "    # 3. logP\n",
    "    logp = Descriptors.MolLogP(mol)\n",
    "    f['logP'] = logp\n",
    "    f['logP_effect'] = 1 if 2 <= logp <= 4 else -1\n",
    "    f['logP_too_low'] = logp < 1\n",
    "    f['logP_too_high'] = logp > 5\n",
    "    f['logP_too_low_effect'] = -1 if f['logP_too_low'] else 0\n",
    "    f['logP_too_high_effect'] = -1 if f['logP_too_high'] else 0\n",
    "\n",
    "    # 4. TPSA\n",
    "    tpsa = Descriptors.TPSA(mol)\n",
    "    f['TPSA'] = tpsa\n",
    "    f['TPSA_effect'] = 1 if 60 <= tpsa <= 120 else -1\n",
    "\n",
    "    # 5. Rotatable bonds\n",
    "    rot = Lipinski.NumRotatableBonds(mol)\n",
    "    f['rotatable'] = rot\n",
    "    f['RotB_per_HA_sqrt'] = (rot / f['HA'])**0.5 if f['HA'] > 0 else 0\n",
    "    f['RotB_per_HA_log'] = np.log1p(rot / f['HA']) if f['HA'] > 0 else 0\n",
    "\n",
    "    # # 6. Rings & aromatic rings\n",
    "    # num_rings = mol.GetRingInfo().NumRings()\n",
    "    # f['num_rings'] = num_rings\n",
    "    # f['ring_count_ge_2'] = num_rings >= 2\n",
    "    # f['Rings_per_HA_sqrt'] = (num_rings / f['HA'])**0.5 if f['HA'] > 0 else 0\n",
    "    # f['Rings_per_HA_log'] = np.log1p(num_rings / f['HA']) if f['HA'] > 0 else 0\n",
    "    # #\n",
    "    # ssr = Chem.GetSymmSSSR(mol)\n",
    "    # aromatic_ring = any(all(mol.GetAtomWithIdx(idx).GetIsAromatic() for idx in ring) for ring in ssr)\n",
    "    # f['has_aromatic_ring'] = aromatic_ring\n",
    "    # f['has_aromatic_ring_effect'] = 1 if aromatic_ring else 0\n",
    "    # f['ring_ge2_and_aromatic'] = f['ring_count_ge_2'] and f['has_aromatic_ring']\n",
    "\n",
    "    # # 7. pKa 관련 작용기\n",
    "    # amine_smarts = ['[NX3;H2,H1;!$(NC=O)]', '[NX3;H0;!$(NC=O)]']\n",
    "    # f['has_amine'] = any(mol.HasSubstructMatch(Chem.MolFromSmarts(s)) for s in amine_smarts)\n",
    "    # f['has_amine_count'] = sum(len(mol.GetSubstructMatches(Chem.MolFromSmarts(s))) for s in amine_smarts)\n",
    "    # f['has_imidazole'] = mol.HasSubstructMatch(Chem.MolFromSmarts('c1cnc[nH]1'))\n",
    "    # f['has_pyridine'] = mol.HasSubstructMatch(Chem.MolFromSmarts('c1ccncc1'))\n",
    "\n",
    "    # # 8. H-bond / Flexibility\n",
    "    # f['HBD'] = Lipinski.NumHDonors(mol)\n",
    "    # f['HBA'] = Lipinski.NumHAcceptors(mol)\n",
    "    # f['HBD_density_sq'] = (f['HBD'] / f['HA'])**2 if f['HA'] > 0 else 0\n",
    "    # f['HBA_density_sq'] = (f['HBA'] / f['HA'])**2 if f['HA'] > 0 else 0\n",
    "    # f['Flex_Polarity_sqrt'] = ((rot / f['HA']) * tpsa)**0.5 if f['HA'] > 0 else 0\n",
    "    # #\n",
    "    # sulfoxide_oxygen_smarts = '[O]=S'\n",
    "    # f['sulfoxide_oxygen_count'] = len(mol.GetSubstructMatches(Chem.MolFromSmarts(sulfoxide_oxygen_smarts)))\n",
    "    # f['tetrazole_nitrogen_count'] = f['tetrazole_count']\n",
    "    # f['hbond_acceptor_effect'] = 1 if (f['sulfoxide_oxygen_count'] + f['tetrazole_nitrogen_count'] > 0) else 0\n",
    "\n",
    "    # 9. 중극성 지표\n",
    "    f['mid_polarity'] = 1 if (60 <= tpsa <= 120 and 2 <= logp <= 4) else 0\n",
    "    f['mid_polarity_effect'] = 1 if f['mid_polarity'] == 1 else -1\n",
    "\n",
    "    # # 10. 상호작용 항 (추가)\n",
    "    # f['logP_TPSA_interaction'] = logp * tpsa\n",
    "    # f['RotB_TPSA_interaction'] = rot * tpsa\n",
    "\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "680739f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = pd.DataFrame([descriptors(s) for s in IC50_dataset['smiles']])\n",
    "final_dataset = pd.concat([IC50_dataset, features_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e37e2e9",
   "metadata": {},
   "source": [
    "### Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "d2b4df43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        features = torch.tensor(self.X[idx]).float()\n",
    "        if self.y is not None:\n",
    "            target = torch.tensor(self.y[idx]).float()\n",
    "            return features, target\n",
    "        else:\n",
    "            return features\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "82858876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature 준비 함수 (Fingerprint 등 합치기)\n",
    "def prepare_features(df):\n",
    "    fp_array = np.stack(df['Fingerprint']).astype(np.float32)\n",
    "    feature_cols = [col for col in df.columns if col not in ['pIC50', 'Fingerprint']]\n",
    "    feature_df = df[feature_cols].copy()\n",
    "    for c in feature_df.columns:\n",
    "        if feature_df[c].dtype == 'bool':\n",
    "            feature_df[c] = feature_df[c].astype(int)\n",
    "    feature_array = feature_df.values.astype(np.float32)\n",
    "    X = np.hstack([fp_array, feature_array])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "216d1524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/valid split\n",
    "train = final_dataset.drop(['smiles', 'IC50_nM'], axis=1)\n",
    "train_df_split, valid_df_split = train_test_split(train, test_size=0.3, random_state=42)\n",
    "\n",
    "# feature 추출\n",
    "X_train = prepare_features(train_df_split)\n",
    "X_valid = prepare_features(valid_df_split)\n",
    "\n",
    "y_train = train_df_split['pIC50'].values.astype(np.float32)\n",
    "y_valid = valid_df_split['pIC50'].values.astype(np.float32)\n",
    "\n",
    "# transform은 train에만 fit, valid는 transform만\n",
    "transform = VarianceThreshold(threshold=0.05)\n",
    "X_train_tr = transform.fit_transform(X_train)\n",
    "X_valid_tr = transform.transform(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "c94eb350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_size: 200\n"
     ]
    }
   ],
   "source": [
    "# Dataset\n",
    "train_dataset = CustomDataset(X_train_tr, y_train)\n",
    "valid_dataset = CustomDataset(X_valid_tr, y_valid)\n",
    "\n",
    "# input_size\n",
    "input_size = train_dataset.X.shape[1]\n",
    "print(\"input_size:\", input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "id": "2953daac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "CFG_model = {\n",
    "    'BATCH_SIZE': 256,\n",
    "    'EPOCHS': 200,\n",
    "    'INPUT_SIZE': input_size,\n",
    "    'HIDDEN_SIZE': 2048,\n",
    "    'OUTPUT_SIZE': 1,\n",
    "    'DROPOUT_RATE': 0.15,\n",
    "    'LEARNING_RATE': 0.0005\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "id": "78bb10ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=CFG_model['BATCH_SIZE'], shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=CFG_model['BATCH_SIZE'], shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43b5799",
   "metadata": {},
   "source": [
    "### Building MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "a0ec20cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_rate, out_size):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        bottleneck_size = hidden_size // 2\n",
    "\n",
    "        # fc 레이어 3개와 출력 레이어 (bottleneck 구조)\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)   # (input_size, 2048)\n",
    "        self.fc2 = nn.Linear(hidden_size, bottleneck_size)   # (2048, 1024)\n",
    "        self.fc3 = nn.Linear(bottleneck_size, hidden_size)   # (1024, 2048)\n",
    "        self.fc_out = nn.Linear(hidden_size, out_size)\n",
    "\n",
    "        # 정규화\n",
    "        self.ln1 = nn.LayerNorm(hidden_size)\n",
    "        self.ln2 = nn.LayerNorm(bottleneck_size)\n",
    "        self.ln3 = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        # 활성화 함수\n",
    "        self.activation = nn.LeakyReLU()\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)   # 0.15\n",
    "        self.dropout2 = nn.Dropout(dropout_rate+0.1)   # 0.25\n",
    "        self.dropout3 = nn.Dropout(dropout_rate+0.2)   # 0.35\n",
    "     \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.activation(out)\n",
    "        out = self.ln1(out)\n",
    "        out1 = self.dropout1(out)\n",
    "\n",
    "        out = self.fc2(out1)\n",
    "        out = self.activation(out)\n",
    "        out = self.ln2(out)\n",
    "        out = self.dropout2(out)\n",
    "        \n",
    "        out = self.fc3(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.ln3(out)\n",
    "        out2 = self.dropout3(out)\n",
    "        out2 = out1 + out2\n",
    "\n",
    "        out = self.fc_out(out2)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "ba72e00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(CFG_model['INPUT_SIZE'],CFG_model['HIDDEN_SIZE'],CFG_model['DROPOUT_RATE'],CFG_model['OUTPUT_SIZE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "6961ef59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CFG_model hyperparameters 추가\n",
    "num_samples = len(train_dataset)\n",
    "batch_size = CFG_model['BATCH_SIZE']\n",
    "num_epochs = CFG_model['EPOCHS']\n",
    "\n",
    "# step 단위 변수 대신 epoch 단위로 변환\n",
    "warmup_epochs = int(num_epochs * 0.02)  # warmup epochs\n",
    "\n",
    "# CFG_model에 epoch 단위 key 파라미터 추가/업데이트\n",
    "CFG_model['WARMUP_EPOCHS'] = warmup_epochs\n",
    "CFG_model['MAX_EPOCHS'] = num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "a751cf7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5968\\4145934217.py:3: UnderReviewWarning: The feature LinearWarmupCosineAnnealingLR is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  scheduler = LinearWarmupCosineAnnealingLR(\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CFG_model['LEARNING_RATE'], weight_decay=1e-4)\n",
    "scheduler = LinearWarmupCosineAnnealingLR(\n",
    "    optimizer,\n",
    "    warmup_epochs=CFG_model['WARMUP_EPOCHS'],\n",
    "    max_epochs=CFG_model['MAX_EPOCHS'],\n",
    "    eta_min=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa67beb1",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "80658212",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train(train_loader, valid_loader, model, criterion, optimizer, scheduler, epochs):\n",
    "    train_loss = []\n",
    "    valid_loss = []\n",
    "    lrs = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(inputs).squeeze(dim=1)  # torch.Size([128, 1])\n",
    "            # target shape은 항상 torch.Size([128])\n",
    "            loss = criterion(output, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)  # 배치 크기만큼 loss를 누적\n",
    "        \n",
    "        avg_train_loss = running_loss / len(train_loader.dataset)  # 전체 샘플 수로 나누기\n",
    "        train_loss.append(avg_train_loss)\n",
    "\n",
    "        # Validatation\n",
    "        model.eval()\n",
    "        valid_running_loss = 0\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in valid_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                output = model(inputs).squeeze(dim=1)\n",
    "                loss = criterion(output, targets)\n",
    "                valid_running_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "        avg_valid_loss = valid_running_loss / len(valid_loader.dataset)\n",
    "        valid_loss.append(avg_valid_loss)\n",
    "\n",
    "        scheduler.step()\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        lrs.append(lr)\n",
    "\n",
    "        print(f'Epoch: {epoch+1}/{epochs}, '\n",
    "                  f'Train Loss: {avg_train_loss:.5f}, '\n",
    "                  f'Valid Loss: {avg_valid_loss:.5f}, '\n",
    "                  f'lr: {lr:.6f}'\n",
    "                  )\n",
    "            \n",
    "        model.train()\n",
    "    \n",
    "    return model, train_loss, valid_loss, lrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d908e83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KfoldCustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y=None, is_test=False):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            self.X = X.values.astype(np.float32)\n",
    "        else:\n",
    "            self.X = X.astype(np.float32)\n",
    "\n",
    "        if y is not None:\n",
    "            if isinstance(y, pd.Series):\n",
    "                self.y = y.values.astype(np.float32)\n",
    "            else:\n",
    "                self.y = y.astype(np.float32)\n",
    "        else:\n",
    "            self.y = None\n",
    "\n",
    "        self.is_test = is_test\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]\n",
    "        if self.is_test or self.y is None:\n",
    "            return torch.tensor(x, dtype=torch.float32)\n",
    "        else:\n",
    "            y = self.y[idx]\n",
    "            return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "62721964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Fold 1 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_10732\\2960053717.py:52: UnderReviewWarning: The feature LinearWarmupCosineAnnealingLR is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  scheduler = LinearWarmupCosineAnnealingLR(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/200, Train Loss: 63.94691, Valid Loss: 61.08895, lr: 0.000167\n",
      "Epoch: 2/200, Train Loss: 22.26517, Valid Loss: 6.49247, lr: 0.000333\n",
      "Epoch: 3/200, Train Loss: 7.46321, Valid Loss: 3.65725, lr: 0.000500\n",
      "Epoch: 4/200, Train Loss: 5.75480, Valid Loss: 3.27088, lr: 0.000500\n",
      "Epoch: 5/200, Train Loss: 3.83466, Valid Loss: 3.55046, lr: 0.000500\n",
      "Epoch: 6/200, Train Loss: 3.57300, Valid Loss: 3.91306, lr: 0.000500\n",
      "Epoch: 7/200, Train Loss: 3.51439, Valid Loss: 2.91696, lr: 0.000500\n",
      "Epoch: 8/200, Train Loss: 3.23595, Valid Loss: 2.90210, lr: 0.000500\n",
      "Epoch: 9/200, Train Loss: 3.14181, Valid Loss: 3.16140, lr: 0.000499\n",
      "Epoch: 10/200, Train Loss: 3.09201, Valid Loss: 2.89197, lr: 0.000499\n",
      "Epoch: 11/200, Train Loss: 3.02585, Valid Loss: 2.90617, lr: 0.000499\n",
      "Epoch: 12/200, Train Loss: 2.95994, Valid Loss: 2.92759, lr: 0.000498\n",
      "Epoch: 13/200, Train Loss: 3.12499, Valid Loss: 2.88802, lr: 0.000498\n",
      "Epoch: 14/200, Train Loss: 2.99401, Valid Loss: 2.86096, lr: 0.000497\n",
      "Epoch: 15/200, Train Loss: 3.00227, Valid Loss: 2.89246, lr: 0.000497\n",
      "Epoch: 16/200, Train Loss: 2.97130, Valid Loss: 2.85480, lr: 0.000496\n",
      "Epoch: 17/200, Train Loss: 2.90589, Valid Loss: 2.85465, lr: 0.000496\n",
      "Epoch: 18/200, Train Loss: 2.94088, Valid Loss: 2.86181, lr: 0.000495\n",
      "Epoch: 19/200, Train Loss: 2.93598, Valid Loss: 2.81491, lr: 0.000494\n",
      "Epoch: 20/200, Train Loss: 2.95230, Valid Loss: 2.83413, lr: 0.000493\n",
      "Epoch: 21/200, Train Loss: 2.95793, Valid Loss: 2.81294, lr: 0.000493\n",
      "Epoch: 22/200, Train Loss: 2.85622, Valid Loss: 2.80196, lr: 0.000492\n",
      "Epoch: 23/200, Train Loss: 2.89494, Valid Loss: 2.77160, lr: 0.000491\n",
      "Epoch: 24/200, Train Loss: 2.90619, Valid Loss: 2.80212, lr: 0.000490\n",
      "Epoch: 25/200, Train Loss: 2.85112, Valid Loss: 2.76239, lr: 0.000489\n",
      "Epoch: 26/200, Train Loss: 2.82968, Valid Loss: 2.76433, lr: 0.000488\n",
      "Epoch: 27/200, Train Loss: 2.86436, Valid Loss: 2.76307, lr: 0.000487\n",
      "Epoch: 28/200, Train Loss: 2.80911, Valid Loss: 2.75231, lr: 0.000485\n",
      "Epoch: 29/200, Train Loss: 2.86587, Valid Loss: 2.75684, lr: 0.000484\n",
      "Epoch: 30/200, Train Loss: 2.81174, Valid Loss: 2.76154, lr: 0.000483\n",
      "Epoch: 31/200, Train Loss: 2.75555, Valid Loss: 2.71137, lr: 0.000482\n",
      "Epoch: 32/200, Train Loss: 2.76565, Valid Loss: 2.74230, lr: 0.000480\n",
      "Epoch: 33/200, Train Loss: 2.78370, Valid Loss: 2.71289, lr: 0.000479\n",
      "Epoch: 34/200, Train Loss: 2.77909, Valid Loss: 2.68465, lr: 0.000477\n",
      "Epoch: 35/200, Train Loss: 2.74859, Valid Loss: 2.73883, lr: 0.000476\n",
      "Epoch: 36/200, Train Loss: 2.83418, Valid Loss: 2.67144, lr: 0.000474\n",
      "Epoch: 37/200, Train Loss: 2.85129, Valid Loss: 2.65671, lr: 0.000473\n",
      "Epoch: 38/200, Train Loss: 2.76770, Valid Loss: 2.73108, lr: 0.000471\n",
      "Epoch: 39/200, Train Loss: 2.76466, Valid Loss: 2.62883, lr: 0.000469\n",
      "Epoch: 40/200, Train Loss: 2.70881, Valid Loss: 2.70772, lr: 0.000468\n",
      "Epoch: 41/200, Train Loss: 2.68260, Valid Loss: 2.62067, lr: 0.000466\n",
      "Epoch: 42/200, Train Loss: 2.67596, Valid Loss: 2.70268, lr: 0.000464\n",
      "Epoch: 43/200, Train Loss: 2.68395, Valid Loss: 2.61625, lr: 0.000462\n",
      "Epoch: 44/200, Train Loss: 2.61144, Valid Loss: 2.64616, lr: 0.000460\n",
      "Epoch: 45/200, Train Loss: 2.68009, Valid Loss: 2.63283, lr: 0.000458\n",
      "Epoch: 46/200, Train Loss: 2.65960, Valid Loss: 2.59723, lr: 0.000456\n",
      "Epoch: 47/200, Train Loss: 2.60680, Valid Loss: 2.61513, lr: 0.000454\n",
      "Epoch: 48/200, Train Loss: 2.60371, Valid Loss: 2.60987, lr: 0.000452\n",
      "Epoch: 49/200, Train Loss: 2.56184, Valid Loss: 2.60060, lr: 0.000450\n",
      "Epoch: 50/200, Train Loss: 2.65395, Valid Loss: 2.63584, lr: 0.000448\n",
      "Epoch: 51/200, Train Loss: 2.58871, Valid Loss: 2.56450, lr: 0.000446\n",
      "Epoch: 52/200, Train Loss: 2.56858, Valid Loss: 2.62924, lr: 0.000444\n",
      "Epoch: 53/200, Train Loss: 2.56739, Valid Loss: 2.56373, lr: 0.000441\n",
      "Epoch: 54/200, Train Loss: 2.53484, Valid Loss: 2.58590, lr: 0.000439\n",
      "Epoch: 55/200, Train Loss: 2.50899, Valid Loss: 2.54032, lr: 0.000437\n",
      "Epoch: 56/200, Train Loss: 2.55169, Valid Loss: 2.57766, lr: 0.000434\n",
      "Epoch: 57/200, Train Loss: 2.53145, Valid Loss: 2.54981, lr: 0.000432\n",
      "Epoch: 58/200, Train Loss: 2.52548, Valid Loss: 2.57759, lr: 0.000430\n",
      "Epoch: 59/200, Train Loss: 2.51932, Valid Loss: 2.51761, lr: 0.000427\n",
      "Epoch: 60/200, Train Loss: 2.52238, Valid Loss: 2.53851, lr: 0.000425\n",
      "Epoch: 61/200, Train Loss: 2.47938, Valid Loss: 2.53865, lr: 0.000422\n",
      "Epoch: 62/200, Train Loss: 2.50741, Valid Loss: 2.50903, lr: 0.000420\n",
      "Epoch: 63/200, Train Loss: 2.42504, Valid Loss: 2.53699, lr: 0.000417\n",
      "Epoch: 64/200, Train Loss: 2.47146, Valid Loss: 2.49788, lr: 0.000414\n",
      "Epoch: 65/200, Train Loss: 2.41417, Valid Loss: 2.49736, lr: 0.000412\n",
      "Epoch: 66/200, Train Loss: 2.47757, Valid Loss: 2.52336, lr: 0.000409\n",
      "Epoch: 67/200, Train Loss: 2.49680, Valid Loss: 2.45721, lr: 0.000406\n",
      "Epoch: 68/200, Train Loss: 2.41869, Valid Loss: 2.56582, lr: 0.000404\n",
      "Epoch: 69/200, Train Loss: 2.42185, Valid Loss: 2.47342, lr: 0.000401\n",
      "Epoch: 70/200, Train Loss: 2.39374, Valid Loss: 2.49921, lr: 0.000398\n",
      "Epoch: 71/200, Train Loss: 2.42429, Valid Loss: 2.47929, lr: 0.000395\n",
      "Epoch: 72/200, Train Loss: 2.44504, Valid Loss: 2.46049, lr: 0.000393\n",
      "Epoch: 73/200, Train Loss: 2.37808, Valid Loss: 2.49122, lr: 0.000390\n",
      "Epoch: 74/200, Train Loss: 2.45599, Valid Loss: 2.46775, lr: 0.000387\n",
      "Epoch: 75/200, Train Loss: 2.37105, Valid Loss: 2.45702, lr: 0.000384\n",
      "Epoch: 76/200, Train Loss: 2.34957, Valid Loss: 2.44611, lr: 0.000381\n",
      "Epoch: 77/200, Train Loss: 2.45809, Valid Loss: 2.44176, lr: 0.000378\n",
      "Epoch: 78/200, Train Loss: 2.35505, Valid Loss: 2.44178, lr: 0.000375\n",
      "Epoch: 79/200, Train Loss: 2.33081, Valid Loss: 2.45560, lr: 0.000372\n",
      "Epoch: 80/200, Train Loss: 2.36585, Valid Loss: 2.44852, lr: 0.000369\n",
      "Epoch: 81/200, Train Loss: 2.34103, Valid Loss: 2.41039, lr: 0.000366\n",
      "Epoch: 82/200, Train Loss: 2.30758, Valid Loss: 2.45996, lr: 0.000363\n",
      "Epoch: 83/200, Train Loss: 2.27739, Valid Loss: 2.44950, lr: 0.000360\n",
      "Epoch: 84/200, Train Loss: 2.40537, Valid Loss: 2.39628, lr: 0.000357\n",
      "Epoch: 85/200, Train Loss: 2.37797, Valid Loss: 2.51353, lr: 0.000354\n",
      "Epoch: 86/200, Train Loss: 2.35457, Valid Loss: 2.37259, lr: 0.000351\n",
      "Epoch: 87/200, Train Loss: 2.31611, Valid Loss: 2.46554, lr: 0.000348\n",
      "Epoch: 88/200, Train Loss: 2.36265, Valid Loss: 2.37597, lr: 0.000345\n",
      "Epoch: 89/200, Train Loss: 2.36123, Valid Loss: 2.46320, lr: 0.000341\n",
      "Epoch: 90/200, Train Loss: 2.32944, Valid Loss: 2.38542, lr: 0.000338\n",
      "Epoch: 91/200, Train Loss: 2.31209, Valid Loss: 2.42611, lr: 0.000335\n",
      "Epoch: 92/200, Train Loss: 2.32592, Valid Loss: 2.38249, lr: 0.000332\n",
      "Epoch: 93/200, Train Loss: 2.28597, Valid Loss: 2.39684, lr: 0.000329\n",
      "Epoch: 94/200, Train Loss: 2.34489, Valid Loss: 2.41471, lr: 0.000326\n",
      "Epoch: 95/200, Train Loss: 2.30617, Valid Loss: 2.39505, lr: 0.000322\n",
      "Epoch: 96/200, Train Loss: 2.32949, Valid Loss: 2.39666, lr: 0.000319\n",
      "Epoch: 97/200, Train Loss: 2.25448, Valid Loss: 2.38914, lr: 0.000316\n",
      "Epoch: 98/200, Train Loss: 2.27026, Valid Loss: 2.40870, lr: 0.000313\n",
      "Epoch: 99/200, Train Loss: 2.25631, Valid Loss: 2.38764, lr: 0.000310\n",
      "Epoch: 100/200, Train Loss: 2.24341, Valid Loss: 2.40930, lr: 0.000306\n",
      "Epoch: 101/200, Train Loss: 2.27520, Valid Loss: 2.38472, lr: 0.000303\n",
      "Epoch: 102/200, Train Loss: 2.27893, Valid Loss: 2.41023, lr: 0.000300\n",
      "Epoch: 103/200, Train Loss: 2.24225, Valid Loss: 2.39361, lr: 0.000297\n",
      "Epoch: 104/200, Train Loss: 2.30294, Valid Loss: 2.38594, lr: 0.000294\n",
      "Epoch: 105/200, Train Loss: 2.26143, Valid Loss: 2.39638, lr: 0.000290\n",
      "Epoch: 106/200, Train Loss: 2.26950, Valid Loss: 2.39126, lr: 0.000287\n",
      "Epoch: 107/200, Train Loss: 2.24869, Valid Loss: 2.40499, lr: 0.000284\n",
      "Epoch: 108/200, Train Loss: 2.24329, Valid Loss: 2.37358, lr: 0.000281\n",
      "Epoch: 109/200, Train Loss: 2.23476, Valid Loss: 2.39460, lr: 0.000278\n",
      "Epoch: 110/200, Train Loss: 2.22017, Valid Loss: 2.36315, lr: 0.000274\n",
      "Epoch: 111/200, Train Loss: 2.19666, Valid Loss: 2.39065, lr: 0.000271\n",
      "Epoch: 112/200, Train Loss: 2.23816, Valid Loss: 2.37053, lr: 0.000268\n",
      "Epoch: 113/200, Train Loss: 2.23812, Valid Loss: 2.37907, lr: 0.000265\n",
      "Epoch: 114/200, Train Loss: 2.21598, Valid Loss: 2.37566, lr: 0.000262\n",
      "Epoch: 115/200, Train Loss: 2.22948, Valid Loss: 2.41352, lr: 0.000259\n",
      "Epoch: 116/200, Train Loss: 2.22087, Valid Loss: 2.36604, lr: 0.000255\n",
      "Epoch: 117/200, Train Loss: 2.23235, Valid Loss: 2.36634, lr: 0.000252\n",
      "Epoch: 118/200, Train Loss: 2.21902, Valid Loss: 2.38728, lr: 0.000249\n",
      "Epoch: 119/200, Train Loss: 2.20057, Valid Loss: 2.37605, lr: 0.000246\n",
      "Epoch: 120/200, Train Loss: 2.22283, Valid Loss: 2.34894, lr: 0.000243\n",
      "Epoch: 121/200, Train Loss: 2.19574, Valid Loss: 2.39358, lr: 0.000240\n",
      "Epoch: 122/200, Train Loss: 2.22877, Valid Loss: 2.35555, lr: 0.000237\n",
      "Epoch: 123/200, Train Loss: 2.21067, Valid Loss: 2.38254, lr: 0.000234\n",
      "Epoch: 124/200, Train Loss: 2.16743, Valid Loss: 2.35379, lr: 0.000231\n",
      "Epoch: 125/200, Train Loss: 2.23065, Valid Loss: 2.37238, lr: 0.000228\n",
      "Epoch: 126/200, Train Loss: 2.19690, Valid Loss: 2.36883, lr: 0.000225\n",
      "Epoch: 127/200, Train Loss: 2.25291, Valid Loss: 2.36482, lr: 0.000222\n",
      "Epoch: 128/200, Train Loss: 2.26016, Valid Loss: 2.36788, lr: 0.000219\n",
      "Epoch: 129/200, Train Loss: 2.18846, Valid Loss: 2.33968, lr: 0.000216\n",
      "Epoch: 130/200, Train Loss: 2.20329, Valid Loss: 2.36885, lr: 0.000213\n",
      "Epoch: 131/200, Train Loss: 2.16913, Valid Loss: 2.34519, lr: 0.000210\n",
      "Epoch: 132/200, Train Loss: 2.14715, Valid Loss: 2.41099, lr: 0.000207\n",
      "Epoch: 133/200, Train Loss: 2.17486, Valid Loss: 2.34290, lr: 0.000205\n",
      "Epoch: 134/200, Train Loss: 2.17289, Valid Loss: 2.40207, lr: 0.000202\n",
      "Epoch: 135/200, Train Loss: 2.17099, Valid Loss: 2.34132, lr: 0.000199\n",
      "Epoch: 136/200, Train Loss: 2.18110, Valid Loss: 2.39806, lr: 0.000196\n",
      "Epoch: 137/200, Train Loss: 2.22310, Valid Loss: 2.32997, lr: 0.000194\n",
      "Epoch: 138/200, Train Loss: 2.18580, Valid Loss: 2.41398, lr: 0.000191\n",
      "Epoch: 139/200, Train Loss: 2.17489, Valid Loss: 2.33644, lr: 0.000188\n",
      "Epoch: 140/200, Train Loss: 2.19619, Valid Loss: 2.39048, lr: 0.000186\n",
      "Epoch: 141/200, Train Loss: 2.13739, Valid Loss: 2.33387, lr: 0.000183\n",
      "Epoch: 142/200, Train Loss: 2.14709, Valid Loss: 2.35102, lr: 0.000180\n",
      "Epoch: 143/200, Train Loss: 2.18260, Valid Loss: 2.36023, lr: 0.000178\n",
      "Epoch: 144/200, Train Loss: 2.14173, Valid Loss: 2.35320, lr: 0.000175\n",
      "Epoch: 145/200, Train Loss: 2.16189, Valid Loss: 2.35663, lr: 0.000173\n",
      "Epoch: 146/200, Train Loss: 2.18805, Valid Loss: 2.38345, lr: 0.000170\n",
      "Epoch: 147/200, Train Loss: 2.16274, Valid Loss: 2.34892, lr: 0.000168\n",
      "Epoch: 148/200, Train Loss: 2.17132, Valid Loss: 2.35621, lr: 0.000166\n",
      "Epoch: 149/200, Train Loss: 2.14410, Valid Loss: 2.37831, lr: 0.000163\n",
      "Epoch: 150/200, Train Loss: 2.17739, Valid Loss: 2.32758, lr: 0.000161\n",
      "Epoch: 151/200, Train Loss: 2.16929, Valid Loss: 2.40338, lr: 0.000159\n",
      "Epoch: 152/200, Train Loss: 2.15480, Valid Loss: 2.34352, lr: 0.000156\n",
      "Epoch: 153/200, Train Loss: 2.14006, Valid Loss: 2.35012, lr: 0.000154\n",
      "Epoch: 154/200, Train Loss: 2.14539, Valid Loss: 2.37289, lr: 0.000152\n",
      "Epoch: 155/200, Train Loss: 2.17516, Valid Loss: 2.33300, lr: 0.000150\n",
      "Epoch: 156/200, Train Loss: 2.14167, Valid Loss: 2.38000, lr: 0.000148\n",
      "Epoch: 157/200, Train Loss: 2.20711, Valid Loss: 2.32440, lr: 0.000146\n",
      "Epoch: 158/200, Train Loss: 2.13883, Valid Loss: 2.40182, lr: 0.000144\n",
      "Epoch: 159/200, Train Loss: 2.17818, Valid Loss: 2.33025, lr: 0.000142\n",
      "Epoch: 160/200, Train Loss: 2.13244, Valid Loss: 2.38256, lr: 0.000140\n",
      "Epoch: 161/200, Train Loss: 2.16362, Valid Loss: 2.35597, lr: 0.000138\n",
      "Epoch: 162/200, Train Loss: 2.13299, Valid Loss: 2.35615, lr: 0.000136\n",
      "Epoch: 163/200, Train Loss: 2.17073, Valid Loss: 2.35335, lr: 0.000134\n",
      "Epoch: 164/200, Train Loss: 2.12793, Valid Loss: 2.37711, lr: 0.000132\n",
      "Epoch: 165/200, Train Loss: 2.16016, Valid Loss: 2.32939, lr: 0.000131\n",
      "Epoch: 166/200, Train Loss: 2.13695, Valid Loss: 2.35649, lr: 0.000129\n",
      "Epoch: 167/200, Train Loss: 2.11058, Valid Loss: 2.36605, lr: 0.000127\n",
      "Epoch: 168/200, Train Loss: 2.12698, Valid Loss: 2.34590, lr: 0.000126\n",
      "Epoch: 169/200, Train Loss: 2.13686, Valid Loss: 2.34686, lr: 0.000124\n",
      "Epoch: 170/200, Train Loss: 2.17275, Valid Loss: 2.35043, lr: 0.000123\n",
      "Epoch: 171/200, Train Loss: 2.14781, Valid Loss: 2.36683, lr: 0.000121\n",
      "Epoch: 172/200, Train Loss: 2.14024, Valid Loss: 2.33796, lr: 0.000120\n",
      "Epoch: 173/200, Train Loss: 2.12932, Valid Loss: 2.38116, lr: 0.000118\n",
      "Epoch: 174/200, Train Loss: 2.13228, Valid Loss: 2.34274, lr: 0.000117\n",
      "Epoch: 175/200, Train Loss: 2.14304, Valid Loss: 2.33367, lr: 0.000116\n",
      "Epoch: 176/200, Train Loss: 2.13904, Valid Loss: 2.39063, lr: 0.000115\n",
      "Epoch: 177/200, Train Loss: 2.13652, Valid Loss: 2.34790, lr: 0.000113\n",
      "Epoch: 178/200, Train Loss: 2.16182, Valid Loss: 2.35060, lr: 0.000112\n",
      "Epoch: 179/200, Train Loss: 2.12181, Valid Loss: 2.37329, lr: 0.000111\n",
      "Epoch: 180/200, Train Loss: 2.12481, Valid Loss: 2.34932, lr: 0.000110\n",
      "Epoch: 181/200, Train Loss: 2.13765, Valid Loss: 2.33592, lr: 0.000109\n",
      "Epoch: 182/200, Train Loss: 2.08375, Valid Loss: 2.38076, lr: 0.000108\n",
      "Epoch: 183/200, Train Loss: 2.13941, Valid Loss: 2.33996, lr: 0.000107\n",
      "Epoch: 184/200, Train Loss: 2.10334, Valid Loss: 2.35222, lr: 0.000107\n",
      "Epoch: 185/200, Train Loss: 2.15531, Valid Loss: 2.35051, lr: 0.000106\n",
      "Epoch: 186/200, Train Loss: 2.12605, Valid Loss: 2.36332, lr: 0.000105\n",
      "Epoch: 187/200, Train Loss: 2.14298, Valid Loss: 2.34025, lr: 0.000104\n",
      "Epoch: 188/200, Train Loss: 2.18686, Valid Loss: 2.38069, lr: 0.000104\n",
      "Epoch: 189/200, Train Loss: 2.10453, Valid Loss: 2.36664, lr: 0.000103\n",
      "Epoch: 190/200, Train Loss: 2.11328, Valid Loss: 2.34075, lr: 0.000103\n",
      "Epoch: 191/200, Train Loss: 2.09270, Valid Loss: 2.34752, lr: 0.000102\n",
      "Epoch: 192/200, Train Loss: 2.08406, Valid Loss: 2.37641, lr: 0.000102\n",
      "Epoch: 193/200, Train Loss: 2.11566, Valid Loss: 2.34425, lr: 0.000101\n",
      "Epoch: 194/200, Train Loss: 2.13581, Valid Loss: 2.35464, lr: 0.000101\n",
      "Epoch: 195/200, Train Loss: 2.08525, Valid Loss: 2.36073, lr: 0.000101\n",
      "Epoch: 196/200, Train Loss: 2.12222, Valid Loss: 2.36693, lr: 0.000100\n",
      "Epoch: 197/200, Train Loss: 2.11484, Valid Loss: 2.35575, lr: 0.000100\n",
      "Epoch: 198/200, Train Loss: 2.15095, Valid Loss: 2.33692, lr: 0.000100\n",
      "Epoch: 199/200, Train Loss: 2.11204, Valid Loss: 2.38553, lr: 0.000100\n",
      "Epoch: 200/200, Train Loss: 2.13409, Valid Loss: 2.36504, lr: 0.000100\n",
      "\n",
      "===== Fold 2 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_10732\\2960053717.py:52: UnderReviewWarning: The feature LinearWarmupCosineAnnealingLR is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  scheduler = LinearWarmupCosineAnnealingLR(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/200, Train Loss: 64.15905, Valid Loss: 69.00572, lr: 0.000167\n",
      "Epoch: 2/200, Train Loss: 19.52054, Valid Loss: 3.38431, lr: 0.000333\n",
      "Epoch: 3/200, Train Loss: 6.90595, Valid Loss: 4.51275, lr: 0.000500\n",
      "Epoch: 4/200, Train Loss: 5.73903, Valid Loss: 3.25979, lr: 0.000500\n",
      "Epoch: 5/200, Train Loss: 4.10969, Valid Loss: 3.12689, lr: 0.000500\n",
      "Epoch: 6/200, Train Loss: 3.51866, Valid Loss: 3.22914, lr: 0.000500\n",
      "Epoch: 7/200, Train Loss: 3.21628, Valid Loss: 3.17379, lr: 0.000500\n",
      "Epoch: 8/200, Train Loss: 3.23592, Valid Loss: 2.83611, lr: 0.000500\n",
      "Epoch: 9/200, Train Loss: 3.23097, Valid Loss: 2.84649, lr: 0.000499\n",
      "Epoch: 10/200, Train Loss: 3.05836, Valid Loss: 2.94256, lr: 0.000499\n",
      "Epoch: 11/200, Train Loss: 3.08740, Valid Loss: 2.79896, lr: 0.000499\n",
      "Epoch: 12/200, Train Loss: 3.05256, Valid Loss: 2.78211, lr: 0.000498\n",
      "Epoch: 13/200, Train Loss: 3.05279, Valid Loss: 2.82621, lr: 0.000498\n",
      "Epoch: 14/200, Train Loss: 2.95783, Valid Loss: 2.76723, lr: 0.000497\n",
      "Epoch: 15/200, Train Loss: 2.96568, Valid Loss: 2.76310, lr: 0.000497\n",
      "Epoch: 16/200, Train Loss: 2.94726, Valid Loss: 2.75035, lr: 0.000496\n",
      "Epoch: 17/200, Train Loss: 3.04382, Valid Loss: 2.73357, lr: 0.000496\n",
      "Epoch: 18/200, Train Loss: 2.84051, Valid Loss: 2.74402, lr: 0.000495\n",
      "Epoch: 19/200, Train Loss: 2.97716, Valid Loss: 2.70627, lr: 0.000494\n",
      "Epoch: 20/200, Train Loss: 2.95579, Valid Loss: 2.71436, lr: 0.000493\n",
      "Epoch: 21/200, Train Loss: 2.94365, Valid Loss: 2.70665, lr: 0.000493\n",
      "Epoch: 22/200, Train Loss: 2.94154, Valid Loss: 2.67981, lr: 0.000492\n",
      "Epoch: 23/200, Train Loss: 2.89185, Valid Loss: 2.67006, lr: 0.000491\n",
      "Epoch: 24/200, Train Loss: 2.84439, Valid Loss: 2.68640, lr: 0.000490\n",
      "Epoch: 25/200, Train Loss: 2.88543, Valid Loss: 2.64427, lr: 0.000489\n",
      "Epoch: 26/200, Train Loss: 2.86759, Valid Loss: 2.66330, lr: 0.000488\n",
      "Epoch: 27/200, Train Loss: 2.85010, Valid Loss: 2.62706, lr: 0.000487\n",
      "Epoch: 28/200, Train Loss: 2.84762, Valid Loss: 2.65280, lr: 0.000485\n",
      "Epoch: 29/200, Train Loss: 2.80475, Valid Loss: 2.60414, lr: 0.000484\n",
      "Epoch: 30/200, Train Loss: 2.73609, Valid Loss: 2.61859, lr: 0.000483\n",
      "Epoch: 31/200, Train Loss: 2.77700, Valid Loss: 2.59350, lr: 0.000482\n",
      "Epoch: 32/200, Train Loss: 2.77435, Valid Loss: 2.60141, lr: 0.000480\n",
      "Epoch: 33/200, Train Loss: 2.76223, Valid Loss: 2.56850, lr: 0.000479\n",
      "Epoch: 34/200, Train Loss: 2.72524, Valid Loss: 2.58351, lr: 0.000477\n",
      "Epoch: 35/200, Train Loss: 2.70162, Valid Loss: 2.56468, lr: 0.000476\n",
      "Epoch: 36/200, Train Loss: 2.73431, Valid Loss: 2.54947, lr: 0.000474\n",
      "Epoch: 37/200, Train Loss: 2.74261, Valid Loss: 2.55433, lr: 0.000473\n",
      "Epoch: 38/200, Train Loss: 2.69645, Valid Loss: 2.52242, lr: 0.000471\n",
      "Epoch: 39/200, Train Loss: 2.71414, Valid Loss: 2.54470, lr: 0.000469\n",
      "Epoch: 40/200, Train Loss: 2.70645, Valid Loss: 2.51033, lr: 0.000468\n",
      "Epoch: 41/200, Train Loss: 2.61546, Valid Loss: 2.52447, lr: 0.000466\n",
      "Epoch: 42/200, Train Loss: 2.65019, Valid Loss: 2.50470, lr: 0.000464\n",
      "Epoch: 43/200, Train Loss: 2.64951, Valid Loss: 2.48082, lr: 0.000462\n",
      "Epoch: 44/200, Train Loss: 2.63948, Valid Loss: 2.49466, lr: 0.000460\n",
      "Epoch: 45/200, Train Loss: 2.61210, Valid Loss: 2.47051, lr: 0.000458\n",
      "Epoch: 46/200, Train Loss: 2.62318, Valid Loss: 2.49835, lr: 0.000456\n",
      "Epoch: 47/200, Train Loss: 2.58866, Valid Loss: 2.45798, lr: 0.000454\n",
      "Epoch: 48/200, Train Loss: 2.59139, Valid Loss: 2.45149, lr: 0.000452\n",
      "Epoch: 49/200, Train Loss: 2.60602, Valid Loss: 2.44294, lr: 0.000450\n",
      "Epoch: 50/200, Train Loss: 2.51032, Valid Loss: 2.44086, lr: 0.000448\n",
      "Epoch: 51/200, Train Loss: 2.56297, Valid Loss: 2.43605, lr: 0.000446\n",
      "Epoch: 52/200, Train Loss: 2.61045, Valid Loss: 2.42150, lr: 0.000444\n",
      "Epoch: 53/200, Train Loss: 2.55134, Valid Loss: 2.42022, lr: 0.000441\n",
      "Epoch: 54/200, Train Loss: 2.49895, Valid Loss: 2.40221, lr: 0.000439\n",
      "Epoch: 55/200, Train Loss: 2.52081, Valid Loss: 2.39731, lr: 0.000437\n",
      "Epoch: 56/200, Train Loss: 2.49056, Valid Loss: 2.39311, lr: 0.000434\n",
      "Epoch: 57/200, Train Loss: 2.50494, Valid Loss: 2.40392, lr: 0.000432\n",
      "Epoch: 58/200, Train Loss: 2.48240, Valid Loss: 2.38451, lr: 0.000430\n",
      "Epoch: 59/200, Train Loss: 2.49798, Valid Loss: 2.36676, lr: 0.000427\n",
      "Epoch: 60/200, Train Loss: 2.50020, Valid Loss: 2.37226, lr: 0.000425\n",
      "Epoch: 61/200, Train Loss: 2.49739, Valid Loss: 2.35697, lr: 0.000422\n",
      "Epoch: 62/200, Train Loss: 2.46955, Valid Loss: 2.38076, lr: 0.000420\n",
      "Epoch: 63/200, Train Loss: 2.44569, Valid Loss: 2.35578, lr: 0.000417\n",
      "Epoch: 64/200, Train Loss: 2.45344, Valid Loss: 2.35400, lr: 0.000414\n",
      "Epoch: 65/200, Train Loss: 2.47826, Valid Loss: 2.33383, lr: 0.000412\n",
      "Epoch: 66/200, Train Loss: 2.43217, Valid Loss: 2.32577, lr: 0.000409\n",
      "Epoch: 67/200, Train Loss: 2.49012, Valid Loss: 2.31892, lr: 0.000406\n",
      "Epoch: 68/200, Train Loss: 2.44466, Valid Loss: 2.34564, lr: 0.000404\n",
      "Epoch: 69/200, Train Loss: 2.40804, Valid Loss: 2.30517, lr: 0.000401\n",
      "Epoch: 70/200, Train Loss: 2.37427, Valid Loss: 2.31483, lr: 0.000398\n",
      "Epoch: 71/200, Train Loss: 2.39414, Valid Loss: 2.30423, lr: 0.000395\n",
      "Epoch: 72/200, Train Loss: 2.39270, Valid Loss: 2.31734, lr: 0.000393\n",
      "Epoch: 73/200, Train Loss: 2.41568, Valid Loss: 2.28979, lr: 0.000390\n",
      "Epoch: 74/200, Train Loss: 2.34586, Valid Loss: 2.30022, lr: 0.000387\n",
      "Epoch: 75/200, Train Loss: 2.41360, Valid Loss: 2.29183, lr: 0.000384\n",
      "Epoch: 76/200, Train Loss: 2.36653, Valid Loss: 2.27531, lr: 0.000381\n",
      "Epoch: 77/200, Train Loss: 2.35721, Valid Loss: 2.27861, lr: 0.000378\n",
      "Epoch: 78/200, Train Loss: 2.37295, Valid Loss: 2.26696, lr: 0.000375\n",
      "Epoch: 79/200, Train Loss: 2.35929, Valid Loss: 2.25944, lr: 0.000372\n",
      "Epoch: 80/200, Train Loss: 2.36259, Valid Loss: 2.26104, lr: 0.000369\n",
      "Epoch: 81/200, Train Loss: 2.39918, Valid Loss: 2.26601, lr: 0.000366\n",
      "Epoch: 82/200, Train Loss: 2.35277, Valid Loss: 2.25056, lr: 0.000363\n",
      "Epoch: 83/200, Train Loss: 2.30589, Valid Loss: 2.25218, lr: 0.000360\n",
      "Epoch: 84/200, Train Loss: 2.29974, Valid Loss: 2.24803, lr: 0.000357\n",
      "Epoch: 85/200, Train Loss: 2.32821, Valid Loss: 2.24562, lr: 0.000354\n",
      "Epoch: 86/200, Train Loss: 2.36093, Valid Loss: 2.22574, lr: 0.000351\n",
      "Epoch: 87/200, Train Loss: 2.35968, Valid Loss: 2.26060, lr: 0.000348\n",
      "Epoch: 88/200, Train Loss: 2.35207, Valid Loss: 2.21829, lr: 0.000345\n",
      "Epoch: 89/200, Train Loss: 2.34106, Valid Loss: 2.27174, lr: 0.000341\n",
      "Epoch: 90/200, Train Loss: 2.26284, Valid Loss: 2.22693, lr: 0.000338\n",
      "Epoch: 91/200, Train Loss: 2.26683, Valid Loss: 2.21954, lr: 0.000335\n",
      "Epoch: 92/200, Train Loss: 2.26714, Valid Loss: 2.23287, lr: 0.000332\n",
      "Epoch: 93/200, Train Loss: 2.27346, Valid Loss: 2.22337, lr: 0.000329\n",
      "Epoch: 94/200, Train Loss: 2.30098, Valid Loss: 2.23203, lr: 0.000326\n",
      "Epoch: 95/200, Train Loss: 2.22821, Valid Loss: 2.19738, lr: 0.000322\n",
      "Epoch: 96/200, Train Loss: 2.26151, Valid Loss: 2.24561, lr: 0.000319\n",
      "Epoch: 97/200, Train Loss: 2.31450, Valid Loss: 2.19135, lr: 0.000316\n",
      "Epoch: 98/200, Train Loss: 2.28839, Valid Loss: 2.24513, lr: 0.000313\n",
      "Epoch: 99/200, Train Loss: 2.29162, Valid Loss: 2.18640, lr: 0.000310\n",
      "Epoch: 100/200, Train Loss: 2.21351, Valid Loss: 2.21337, lr: 0.000306\n",
      "Epoch: 101/200, Train Loss: 2.30265, Valid Loss: 2.18109, lr: 0.000303\n",
      "Epoch: 102/200, Train Loss: 2.28363, Valid Loss: 2.22085, lr: 0.000300\n",
      "Epoch: 103/200, Train Loss: 2.28487, Valid Loss: 2.17698, lr: 0.000297\n",
      "Epoch: 104/200, Train Loss: 2.24130, Valid Loss: 2.23178, lr: 0.000294\n",
      "Epoch: 105/200, Train Loss: 2.24841, Valid Loss: 2.19402, lr: 0.000290\n",
      "Epoch: 106/200, Train Loss: 2.22011, Valid Loss: 2.18543, lr: 0.000287\n",
      "Epoch: 107/200, Train Loss: 2.28794, Valid Loss: 2.18105, lr: 0.000284\n",
      "Epoch: 108/200, Train Loss: 2.22975, Valid Loss: 2.18921, lr: 0.000281\n",
      "Epoch: 109/200, Train Loss: 2.25151, Valid Loss: 2.16850, lr: 0.000278\n",
      "Epoch: 110/200, Train Loss: 2.21375, Valid Loss: 2.19469, lr: 0.000274\n",
      "Epoch: 111/200, Train Loss: 2.27550, Valid Loss: 2.16654, lr: 0.000271\n",
      "Epoch: 112/200, Train Loss: 2.23063, Valid Loss: 2.17758, lr: 0.000268\n",
      "Epoch: 113/200, Train Loss: 2.16357, Valid Loss: 2.17375, lr: 0.000265\n",
      "Epoch: 114/200, Train Loss: 2.26738, Valid Loss: 2.17653, lr: 0.000262\n",
      "Epoch: 115/200, Train Loss: 2.19289, Valid Loss: 2.17789, lr: 0.000259\n",
      "Epoch: 116/200, Train Loss: 2.24410, Valid Loss: 2.16922, lr: 0.000255\n",
      "Epoch: 117/200, Train Loss: 2.19287, Valid Loss: 2.17291, lr: 0.000252\n",
      "Epoch: 118/200, Train Loss: 2.23615, Valid Loss: 2.15917, lr: 0.000249\n",
      "Epoch: 119/200, Train Loss: 2.25435, Valid Loss: 2.17535, lr: 0.000246\n",
      "Epoch: 120/200, Train Loss: 2.26327, Valid Loss: 2.15644, lr: 0.000243\n",
      "Epoch: 121/200, Train Loss: 2.24969, Valid Loss: 2.18318, lr: 0.000240\n",
      "Epoch: 122/200, Train Loss: 2.20457, Valid Loss: 2.15189, lr: 0.000237\n",
      "Epoch: 123/200, Train Loss: 2.16694, Valid Loss: 2.16563, lr: 0.000234\n",
      "Epoch: 124/200, Train Loss: 2.20158, Valid Loss: 2.14587, lr: 0.000231\n",
      "Epoch: 125/200, Train Loss: 2.21029, Valid Loss: 2.16452, lr: 0.000228\n",
      "Epoch: 126/200, Train Loss: 2.20483, Valid Loss: 2.16558, lr: 0.000225\n",
      "Epoch: 127/200, Train Loss: 2.19272, Valid Loss: 2.16119, lr: 0.000222\n",
      "Epoch: 128/200, Train Loss: 2.23023, Valid Loss: 2.14621, lr: 0.000219\n",
      "Epoch: 129/200, Train Loss: 2.22985, Valid Loss: 2.17169, lr: 0.000216\n",
      "Epoch: 130/200, Train Loss: 2.26161, Valid Loss: 2.14111, lr: 0.000213\n",
      "Epoch: 131/200, Train Loss: 2.23510, Valid Loss: 2.15502, lr: 0.000210\n",
      "Epoch: 132/200, Train Loss: 2.19547, Valid Loss: 2.15658, lr: 0.000207\n",
      "Epoch: 133/200, Train Loss: 2.23521, Valid Loss: 2.14570, lr: 0.000205\n",
      "Epoch: 134/200, Train Loss: 2.17408, Valid Loss: 2.17353, lr: 0.000202\n",
      "Epoch: 135/200, Train Loss: 2.20216, Valid Loss: 2.13341, lr: 0.000199\n",
      "Epoch: 136/200, Train Loss: 2.22746, Valid Loss: 2.18405, lr: 0.000196\n",
      "Epoch: 137/200, Train Loss: 2.20136, Valid Loss: 2.13535, lr: 0.000194\n",
      "Epoch: 138/200, Train Loss: 2.17308, Valid Loss: 2.15932, lr: 0.000191\n",
      "Epoch: 139/200, Train Loss: 2.20849, Valid Loss: 2.15180, lr: 0.000188\n",
      "Epoch: 140/200, Train Loss: 2.20547, Valid Loss: 2.15004, lr: 0.000186\n",
      "Epoch: 141/200, Train Loss: 2.17468, Valid Loss: 2.13959, lr: 0.000183\n",
      "Epoch: 142/200, Train Loss: 2.14855, Valid Loss: 2.16987, lr: 0.000180\n",
      "Epoch: 143/200, Train Loss: 2.20786, Valid Loss: 2.14334, lr: 0.000178\n",
      "Epoch: 144/200, Train Loss: 2.20601, Valid Loss: 2.14540, lr: 0.000175\n",
      "Epoch: 145/200, Train Loss: 2.20060, Valid Loss: 2.13672, lr: 0.000173\n",
      "Epoch: 146/200, Train Loss: 2.17193, Valid Loss: 2.13146, lr: 0.000170\n",
      "Epoch: 147/200, Train Loss: 2.18703, Valid Loss: 2.15856, lr: 0.000168\n",
      "Epoch: 148/200, Train Loss: 2.21947, Valid Loss: 2.14022, lr: 0.000166\n",
      "Epoch: 149/200, Train Loss: 2.17722, Valid Loss: 2.13611, lr: 0.000163\n",
      "Epoch: 150/200, Train Loss: 2.19384, Valid Loss: 2.14498, lr: 0.000161\n",
      "Epoch: 151/200, Train Loss: 2.16713, Valid Loss: 2.14973, lr: 0.000159\n",
      "Epoch: 152/200, Train Loss: 2.14639, Valid Loss: 2.12597, lr: 0.000156\n",
      "Epoch: 153/200, Train Loss: 2.18801, Valid Loss: 2.16144, lr: 0.000154\n",
      "Epoch: 154/200, Train Loss: 2.17329, Valid Loss: 2.13269, lr: 0.000152\n",
      "Epoch: 155/200, Train Loss: 2.13923, Valid Loss: 2.13565, lr: 0.000150\n",
      "Epoch: 156/200, Train Loss: 2.13979, Valid Loss: 2.15134, lr: 0.000148\n",
      "Epoch: 157/200, Train Loss: 2.18570, Valid Loss: 2.13413, lr: 0.000146\n",
      "Epoch: 158/200, Train Loss: 2.17359, Valid Loss: 2.13774, lr: 0.000144\n",
      "Epoch: 159/200, Train Loss: 2.16213, Valid Loss: 2.12708, lr: 0.000142\n",
      "Epoch: 160/200, Train Loss: 2.15518, Valid Loss: 2.15265, lr: 0.000140\n",
      "Epoch: 161/200, Train Loss: 2.20190, Valid Loss: 2.13532, lr: 0.000138\n",
      "Epoch: 162/200, Train Loss: 2.19394, Valid Loss: 2.14056, lr: 0.000136\n",
      "Epoch: 163/200, Train Loss: 2.18371, Valid Loss: 2.14108, lr: 0.000134\n",
      "Epoch: 164/200, Train Loss: 2.14952, Valid Loss: 2.14410, lr: 0.000132\n",
      "Epoch: 165/200, Train Loss: 2.14468, Valid Loss: 2.12626, lr: 0.000131\n",
      "Epoch: 166/200, Train Loss: 2.21529, Valid Loss: 2.14279, lr: 0.000129\n",
      "Epoch: 167/200, Train Loss: 2.13139, Valid Loss: 2.12770, lr: 0.000127\n",
      "Epoch: 168/200, Train Loss: 2.15592, Valid Loss: 2.14251, lr: 0.000126\n",
      "Epoch: 169/200, Train Loss: 2.14560, Valid Loss: 2.13905, lr: 0.000124\n",
      "Epoch: 170/200, Train Loss: 2.17111, Valid Loss: 2.12656, lr: 0.000123\n",
      "Epoch: 171/200, Train Loss: 2.12845, Valid Loss: 2.15292, lr: 0.000121\n",
      "Epoch: 172/200, Train Loss: 2.17815, Valid Loss: 2.12093, lr: 0.000120\n",
      "Epoch: 173/200, Train Loss: 2.16266, Valid Loss: 2.14554, lr: 0.000118\n",
      "Epoch: 174/200, Train Loss: 2.17573, Valid Loss: 2.12695, lr: 0.000117\n",
      "Epoch: 175/200, Train Loss: 2.18470, Valid Loss: 2.12119, lr: 0.000116\n",
      "Epoch: 176/200, Train Loss: 2.15531, Valid Loss: 2.15163, lr: 0.000115\n",
      "Epoch: 177/200, Train Loss: 2.14142, Valid Loss: 2.12279, lr: 0.000113\n",
      "Epoch: 178/200, Train Loss: 2.16588, Valid Loss: 2.13364, lr: 0.000112\n",
      "Epoch: 179/200, Train Loss: 2.16786, Valid Loss: 2.12350, lr: 0.000111\n",
      "Epoch: 180/200, Train Loss: 2.16747, Valid Loss: 2.12463, lr: 0.000110\n",
      "Epoch: 181/200, Train Loss: 2.17094, Valid Loss: 2.12946, lr: 0.000109\n",
      "Epoch: 182/200, Train Loss: 2.14173, Valid Loss: 2.13666, lr: 0.000108\n",
      "Epoch: 183/200, Train Loss: 2.15401, Valid Loss: 2.12074, lr: 0.000107\n",
      "Epoch: 184/200, Train Loss: 2.14480, Valid Loss: 2.13850, lr: 0.000107\n",
      "Epoch: 185/200, Train Loss: 2.14391, Valid Loss: 2.13254, lr: 0.000106\n",
      "Epoch: 186/200, Train Loss: 2.15182, Valid Loss: 2.12111, lr: 0.000105\n",
      "Epoch: 187/200, Train Loss: 2.12980, Valid Loss: 2.13066, lr: 0.000104\n",
      "Epoch: 188/200, Train Loss: 2.16285, Valid Loss: 2.13197, lr: 0.000104\n",
      "Epoch: 189/200, Train Loss: 2.14185, Valid Loss: 2.12840, lr: 0.000103\n",
      "Epoch: 190/200, Train Loss: 2.13893, Valid Loss: 2.13197, lr: 0.000103\n",
      "Epoch: 191/200, Train Loss: 2.16113, Valid Loss: 2.12633, lr: 0.000102\n",
      "Epoch: 192/200, Train Loss: 2.11100, Valid Loss: 2.12472, lr: 0.000102\n",
      "Epoch: 193/200, Train Loss: 2.13858, Valid Loss: 2.13625, lr: 0.000101\n",
      "Epoch: 194/200, Train Loss: 2.14017, Valid Loss: 2.13085, lr: 0.000101\n",
      "Epoch: 195/200, Train Loss: 2.14549, Valid Loss: 2.11313, lr: 0.000101\n",
      "Epoch: 196/200, Train Loss: 2.12921, Valid Loss: 2.12643, lr: 0.000100\n",
      "Epoch: 197/200, Train Loss: 2.16460, Valid Loss: 2.13804, lr: 0.000100\n",
      "Epoch: 198/200, Train Loss: 2.14712, Valid Loss: 2.11184, lr: 0.000100\n",
      "Epoch: 199/200, Train Loss: 2.14855, Valid Loss: 2.14979, lr: 0.000100\n",
      "Epoch: 200/200, Train Loss: 2.14230, Valid Loss: 2.11205, lr: 0.000100\n",
      "\n",
      "===== Fold 3 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_10732\\2960053717.py:52: UnderReviewWarning: The feature LinearWarmupCosineAnnealingLR is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  scheduler = LinearWarmupCosineAnnealingLR(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/200, Train Loss: 62.06833, Valid Loss: 59.11424, lr: 0.000167\n",
      "Epoch: 2/200, Train Loss: 18.70842, Valid Loss: 4.62172, lr: 0.000333\n",
      "Epoch: 3/200, Train Loss: 5.74924, Valid Loss: 4.80489, lr: 0.000500\n",
      "Epoch: 4/200, Train Loss: 4.67781, Valid Loss: 3.11528, lr: 0.000500\n",
      "Epoch: 5/200, Train Loss: 3.90189, Valid Loss: 2.95230, lr: 0.000500\n",
      "Epoch: 6/200, Train Loss: 3.41033, Valid Loss: 3.04804, lr: 0.000500\n",
      "Epoch: 7/200, Train Loss: 3.27869, Valid Loss: 2.82880, lr: 0.000500\n",
      "Epoch: 8/200, Train Loss: 3.18208, Valid Loss: 2.96050, lr: 0.000500\n",
      "Epoch: 9/200, Train Loss: 3.18669, Valid Loss: 2.77635, lr: 0.000499\n",
      "Epoch: 10/200, Train Loss: 3.14916, Valid Loss: 2.78349, lr: 0.000499\n",
      "Epoch: 11/200, Train Loss: 3.10954, Valid Loss: 2.80513, lr: 0.000499\n",
      "Epoch: 12/200, Train Loss: 3.00574, Valid Loss: 2.74911, lr: 0.000498\n",
      "Epoch: 13/200, Train Loss: 3.05428, Valid Loss: 2.73231, lr: 0.000498\n",
      "Epoch: 14/200, Train Loss: 2.93928, Valid Loss: 2.77517, lr: 0.000497\n",
      "Epoch: 15/200, Train Loss: 3.04925, Valid Loss: 2.70672, lr: 0.000497\n",
      "Epoch: 16/200, Train Loss: 2.94785, Valid Loss: 2.77563, lr: 0.000496\n",
      "Epoch: 17/200, Train Loss: 2.95825, Valid Loss: 2.68131, lr: 0.000496\n",
      "Epoch: 18/200, Train Loss: 2.94946, Valid Loss: 2.71174, lr: 0.000495\n",
      "Epoch: 19/200, Train Loss: 2.96828, Valid Loss: 2.68420, lr: 0.000494\n",
      "Epoch: 20/200, Train Loss: 2.87011, Valid Loss: 2.66851, lr: 0.000493\n",
      "Epoch: 21/200, Train Loss: 2.91914, Valid Loss: 2.68911, lr: 0.000493\n",
      "Epoch: 22/200, Train Loss: 2.88550, Valid Loss: 2.64171, lr: 0.000492\n",
      "Epoch: 23/200, Train Loss: 2.88299, Valid Loss: 2.68692, lr: 0.000491\n",
      "Epoch: 24/200, Train Loss: 2.85182, Valid Loss: 2.62392, lr: 0.000490\n",
      "Epoch: 25/200, Train Loss: 2.90937, Valid Loss: 2.65529, lr: 0.000489\n",
      "Epoch: 26/200, Train Loss: 2.88417, Valid Loss: 2.61875, lr: 0.000488\n",
      "Epoch: 27/200, Train Loss: 2.83951, Valid Loss: 2.63850, lr: 0.000487\n",
      "Epoch: 28/200, Train Loss: 2.82808, Valid Loss: 2.58684, lr: 0.000485\n",
      "Epoch: 29/200, Train Loss: 2.87505, Valid Loss: 2.63713, lr: 0.000484\n",
      "Epoch: 30/200, Train Loss: 2.80050, Valid Loss: 2.56472, lr: 0.000483\n",
      "Epoch: 31/200, Train Loss: 2.78030, Valid Loss: 2.59073, lr: 0.000482\n",
      "Epoch: 32/200, Train Loss: 2.79797, Valid Loss: 2.54312, lr: 0.000480\n",
      "Epoch: 33/200, Train Loss: 2.72023, Valid Loss: 2.53999, lr: 0.000479\n",
      "Epoch: 34/200, Train Loss: 2.72525, Valid Loss: 2.56787, lr: 0.000477\n",
      "Epoch: 35/200, Train Loss: 2.78510, Valid Loss: 2.51294, lr: 0.000476\n",
      "Epoch: 36/200, Train Loss: 2.79410, Valid Loss: 2.58404, lr: 0.000474\n",
      "Epoch: 37/200, Train Loss: 2.71624, Valid Loss: 2.49244, lr: 0.000473\n",
      "Epoch: 38/200, Train Loss: 2.74813, Valid Loss: 2.60340, lr: 0.000471\n",
      "Epoch: 39/200, Train Loss: 2.71130, Valid Loss: 2.49030, lr: 0.000469\n",
      "Epoch: 40/200, Train Loss: 2.67933, Valid Loss: 2.51553, lr: 0.000468\n",
      "Epoch: 41/200, Train Loss: 2.65464, Valid Loss: 2.51944, lr: 0.000466\n",
      "Epoch: 42/200, Train Loss: 2.60469, Valid Loss: 2.46120, lr: 0.000464\n",
      "Epoch: 43/200, Train Loss: 2.59693, Valid Loss: 2.48821, lr: 0.000462\n",
      "Epoch: 44/200, Train Loss: 2.63589, Valid Loss: 2.45696, lr: 0.000460\n",
      "Epoch: 45/200, Train Loss: 2.53900, Valid Loss: 2.50500, lr: 0.000458\n",
      "Epoch: 46/200, Train Loss: 2.61927, Valid Loss: 2.49325, lr: 0.000456\n",
      "Epoch: 47/200, Train Loss: 2.61358, Valid Loss: 2.42512, lr: 0.000454\n",
      "Epoch: 48/200, Train Loss: 2.56125, Valid Loss: 2.48245, lr: 0.000452\n",
      "Epoch: 49/200, Train Loss: 2.57085, Valid Loss: 2.42078, lr: 0.000450\n",
      "Epoch: 50/200, Train Loss: 2.56280, Valid Loss: 2.43794, lr: 0.000448\n",
      "Epoch: 51/200, Train Loss: 2.56800, Valid Loss: 2.46767, lr: 0.000446\n",
      "Epoch: 52/200, Train Loss: 2.51382, Valid Loss: 2.39357, lr: 0.000444\n",
      "Epoch: 53/200, Train Loss: 2.58502, Valid Loss: 2.42307, lr: 0.000441\n",
      "Epoch: 54/200, Train Loss: 2.54175, Valid Loss: 2.46644, lr: 0.000439\n",
      "Epoch: 55/200, Train Loss: 2.52595, Valid Loss: 2.41101, lr: 0.000437\n",
      "Epoch: 56/200, Train Loss: 2.50825, Valid Loss: 2.40343, lr: 0.000434\n",
      "Epoch: 57/200, Train Loss: 2.44717, Valid Loss: 2.42360, lr: 0.000432\n",
      "Epoch: 58/200, Train Loss: 2.49040, Valid Loss: 2.38167, lr: 0.000430\n",
      "Epoch: 59/200, Train Loss: 2.49961, Valid Loss: 2.37023, lr: 0.000427\n",
      "Epoch: 60/200, Train Loss: 2.50108, Valid Loss: 2.39610, lr: 0.000425\n",
      "Epoch: 61/200, Train Loss: 2.49171, Valid Loss: 2.33332, lr: 0.000422\n",
      "Epoch: 62/200, Train Loss: 2.47359, Valid Loss: 2.43755, lr: 0.000420\n",
      "Epoch: 63/200, Train Loss: 2.47904, Valid Loss: 2.32744, lr: 0.000417\n",
      "Epoch: 64/200, Train Loss: 2.43508, Valid Loss: 2.40467, lr: 0.000414\n",
      "Epoch: 65/200, Train Loss: 2.44684, Valid Loss: 2.31464, lr: 0.000412\n",
      "Epoch: 66/200, Train Loss: 2.36745, Valid Loss: 2.38484, lr: 0.000409\n",
      "Epoch: 67/200, Train Loss: 2.39774, Valid Loss: 2.35825, lr: 0.000406\n",
      "Epoch: 68/200, Train Loss: 2.40410, Valid Loss: 2.35077, lr: 0.000404\n",
      "Epoch: 69/200, Train Loss: 2.36427, Valid Loss: 2.34675, lr: 0.000401\n",
      "Epoch: 70/200, Train Loss: 2.38205, Valid Loss: 2.32754, lr: 0.000398\n",
      "Epoch: 71/200, Train Loss: 2.35560, Valid Loss: 2.35394, lr: 0.000395\n",
      "Epoch: 72/200, Train Loss: 2.36060, Valid Loss: 2.31976, lr: 0.000393\n",
      "Epoch: 73/200, Train Loss: 2.35131, Valid Loss: 2.32446, lr: 0.000390\n",
      "Epoch: 74/200, Train Loss: 2.33836, Valid Loss: 2.31617, lr: 0.000387\n",
      "Epoch: 75/200, Train Loss: 2.35217, Valid Loss: 2.35804, lr: 0.000384\n",
      "Epoch: 76/200, Train Loss: 2.33677, Valid Loss: 2.28525, lr: 0.000381\n",
      "Epoch: 77/200, Train Loss: 2.36836, Valid Loss: 2.31749, lr: 0.000378\n",
      "Epoch: 78/200, Train Loss: 2.36614, Valid Loss: 2.35534, lr: 0.000375\n",
      "Epoch: 79/200, Train Loss: 2.33521, Valid Loss: 2.27288, lr: 0.000372\n",
      "Epoch: 80/200, Train Loss: 2.28936, Valid Loss: 2.34702, lr: 0.000369\n",
      "Epoch: 81/200, Train Loss: 2.31435, Valid Loss: 2.29244, lr: 0.000366\n",
      "Epoch: 82/200, Train Loss: 2.33400, Valid Loss: 2.33855, lr: 0.000363\n",
      "Epoch: 83/200, Train Loss: 2.28289, Valid Loss: 2.26048, lr: 0.000360\n",
      "Epoch: 84/200, Train Loss: 2.27043, Valid Loss: 2.34130, lr: 0.000357\n",
      "Epoch: 85/200, Train Loss: 2.28381, Valid Loss: 2.26160, lr: 0.000354\n",
      "Epoch: 86/200, Train Loss: 2.29297, Valid Loss: 2.34818, lr: 0.000351\n",
      "Epoch: 87/200, Train Loss: 2.32475, Valid Loss: 2.26696, lr: 0.000348\n",
      "Epoch: 88/200, Train Loss: 2.31328, Valid Loss: 2.28858, lr: 0.000345\n",
      "Epoch: 89/200, Train Loss: 2.25489, Valid Loss: 2.28178, lr: 0.000341\n",
      "Epoch: 90/200, Train Loss: 2.24148, Valid Loss: 2.27160, lr: 0.000338\n",
      "Epoch: 91/200, Train Loss: 2.23558, Valid Loss: 2.28541, lr: 0.000335\n",
      "Epoch: 92/200, Train Loss: 2.26240, Valid Loss: 2.32766, lr: 0.000332\n",
      "Epoch: 93/200, Train Loss: 2.23954, Valid Loss: 2.27276, lr: 0.000329\n",
      "Epoch: 94/200, Train Loss: 2.28955, Valid Loss: 2.28982, lr: 0.000326\n",
      "Epoch: 95/200, Train Loss: 2.25425, Valid Loss: 2.27512, lr: 0.000322\n",
      "Epoch: 96/200, Train Loss: 2.23128, Valid Loss: 2.31714, lr: 0.000319\n",
      "Epoch: 97/200, Train Loss: 2.25028, Valid Loss: 2.26614, lr: 0.000316\n",
      "Epoch: 98/200, Train Loss: 2.21862, Valid Loss: 2.27661, lr: 0.000313\n",
      "Epoch: 99/200, Train Loss: 2.24916, Valid Loss: 2.35921, lr: 0.000310\n",
      "Epoch: 100/200, Train Loss: 2.24628, Valid Loss: 2.23494, lr: 0.000306\n",
      "Epoch: 101/200, Train Loss: 2.19648, Valid Loss: 2.38653, lr: 0.000303\n",
      "Epoch: 102/200, Train Loss: 2.26078, Valid Loss: 2.23928, lr: 0.000300\n",
      "Epoch: 103/200, Train Loss: 2.21187, Valid Loss: 2.27671, lr: 0.000297\n",
      "Epoch: 104/200, Train Loss: 2.23798, Valid Loss: 2.27846, lr: 0.000294\n",
      "Epoch: 105/200, Train Loss: 2.21188, Valid Loss: 2.27635, lr: 0.000290\n",
      "Epoch: 106/200, Train Loss: 2.19895, Valid Loss: 2.26512, lr: 0.000287\n",
      "Epoch: 107/200, Train Loss: 2.16945, Valid Loss: 2.33736, lr: 0.000284\n",
      "Epoch: 108/200, Train Loss: 2.25507, Valid Loss: 2.22165, lr: 0.000281\n",
      "Epoch: 109/200, Train Loss: 2.22505, Valid Loss: 2.38672, lr: 0.000278\n",
      "Epoch: 110/200, Train Loss: 2.23430, Valid Loss: 2.22488, lr: 0.000274\n",
      "Epoch: 111/200, Train Loss: 2.25842, Valid Loss: 2.33227, lr: 0.000271\n",
      "Epoch: 112/200, Train Loss: 2.19182, Valid Loss: 2.24275, lr: 0.000268\n",
      "Epoch: 113/200, Train Loss: 2.20259, Valid Loss: 2.24369, lr: 0.000265\n",
      "Epoch: 114/200, Train Loss: 2.21665, Valid Loss: 2.30029, lr: 0.000262\n",
      "Epoch: 115/200, Train Loss: 2.21720, Valid Loss: 2.25538, lr: 0.000259\n",
      "Epoch: 116/200, Train Loss: 2.18546, Valid Loss: 2.28919, lr: 0.000255\n",
      "Epoch: 117/200, Train Loss: 2.23520, Valid Loss: 2.24218, lr: 0.000252\n",
      "Epoch: 118/200, Train Loss: 2.25149, Valid Loss: 2.31652, lr: 0.000249\n",
      "Epoch: 119/200, Train Loss: 2.21513, Valid Loss: 2.22809, lr: 0.000246\n",
      "Epoch: 120/200, Train Loss: 2.23067, Valid Loss: 2.30936, lr: 0.000243\n",
      "Epoch: 121/200, Train Loss: 2.16428, Valid Loss: 2.26131, lr: 0.000240\n",
      "Epoch: 122/200, Train Loss: 2.19443, Valid Loss: 2.26524, lr: 0.000237\n",
      "Epoch: 123/200, Train Loss: 2.19529, Valid Loss: 2.27221, lr: 0.000234\n",
      "Epoch: 124/200, Train Loss: 2.18526, Valid Loss: 2.25385, lr: 0.000231\n",
      "Epoch: 125/200, Train Loss: 2.16480, Valid Loss: 2.28984, lr: 0.000228\n",
      "Epoch: 126/200, Train Loss: 2.15302, Valid Loss: 2.23618, lr: 0.000225\n",
      "Epoch: 127/200, Train Loss: 2.12903, Valid Loss: 2.29914, lr: 0.000222\n",
      "Epoch: 128/200, Train Loss: 2.14343, Valid Loss: 2.25282, lr: 0.000219\n",
      "Epoch: 129/200, Train Loss: 2.12685, Valid Loss: 2.29179, lr: 0.000216\n",
      "Epoch: 130/200, Train Loss: 2.17309, Valid Loss: 2.25583, lr: 0.000213\n",
      "Epoch: 131/200, Train Loss: 2.17117, Valid Loss: 2.25759, lr: 0.000210\n",
      "Epoch: 132/200, Train Loss: 2.16006, Valid Loss: 2.28122, lr: 0.000207\n",
      "Epoch: 133/200, Train Loss: 2.11675, Valid Loss: 2.28685, lr: 0.000205\n",
      "Epoch: 134/200, Train Loss: 2.13140, Valid Loss: 2.24033, lr: 0.000202\n",
      "Epoch: 135/200, Train Loss: 2.12516, Valid Loss: 2.28267, lr: 0.000199\n",
      "Epoch: 136/200, Train Loss: 2.13222, Valid Loss: 2.28347, lr: 0.000196\n",
      "Epoch: 137/200, Train Loss: 2.10880, Valid Loss: 2.24783, lr: 0.000194\n",
      "Epoch: 138/200, Train Loss: 2.13420, Valid Loss: 2.26734, lr: 0.000191\n",
      "Epoch: 139/200, Train Loss: 2.10357, Valid Loss: 2.24826, lr: 0.000188\n",
      "Epoch: 140/200, Train Loss: 2.15241, Valid Loss: 2.27740, lr: 0.000186\n",
      "Epoch: 141/200, Train Loss: 2.12529, Valid Loss: 2.23136, lr: 0.000183\n",
      "Epoch: 142/200, Train Loss: 2.11403, Valid Loss: 2.28378, lr: 0.000180\n",
      "Epoch: 143/200, Train Loss: 2.11672, Valid Loss: 2.25235, lr: 0.000178\n",
      "Epoch: 144/200, Train Loss: 2.16482, Valid Loss: 2.27889, lr: 0.000175\n",
      "Epoch: 145/200, Train Loss: 2.10796, Valid Loss: 2.25649, lr: 0.000173\n",
      "Epoch: 146/200, Train Loss: 2.16332, Valid Loss: 2.26413, lr: 0.000170\n",
      "Epoch: 147/200, Train Loss: 2.12575, Valid Loss: 2.24189, lr: 0.000168\n",
      "Epoch: 148/200, Train Loss: 2.12888, Valid Loss: 2.26890, lr: 0.000166\n",
      "Epoch: 149/200, Train Loss: 2.12141, Valid Loss: 2.25063, lr: 0.000163\n",
      "Epoch: 150/200, Train Loss: 2.08578, Valid Loss: 2.27326, lr: 0.000161\n",
      "Epoch: 151/200, Train Loss: 2.12489, Valid Loss: 2.26453, lr: 0.000159\n",
      "Epoch: 152/200, Train Loss: 2.14919, Valid Loss: 2.26573, lr: 0.000156\n",
      "Epoch: 153/200, Train Loss: 2.10143, Valid Loss: 2.25380, lr: 0.000154\n",
      "Epoch: 154/200, Train Loss: 2.13604, Valid Loss: 2.28384, lr: 0.000152\n",
      "Epoch: 155/200, Train Loss: 2.10769, Valid Loss: 2.24318, lr: 0.000150\n",
      "Epoch: 156/200, Train Loss: 2.11176, Valid Loss: 2.27887, lr: 0.000148\n",
      "Epoch: 157/200, Train Loss: 2.07986, Valid Loss: 2.31043, lr: 0.000146\n",
      "Epoch: 158/200, Train Loss: 2.10352, Valid Loss: 2.23049, lr: 0.000144\n",
      "Epoch: 159/200, Train Loss: 2.11927, Valid Loss: 2.28116, lr: 0.000142\n",
      "Epoch: 160/200, Train Loss: 2.12387, Valid Loss: 2.27592, lr: 0.000140\n",
      "Epoch: 161/200, Train Loss: 2.11470, Valid Loss: 2.25693, lr: 0.000138\n",
      "Epoch: 162/200, Train Loss: 2.11470, Valid Loss: 2.25480, lr: 0.000136\n",
      "Epoch: 163/200, Train Loss: 2.13172, Valid Loss: 2.29287, lr: 0.000134\n",
      "Epoch: 164/200, Train Loss: 2.14346, Valid Loss: 2.26672, lr: 0.000132\n",
      "Epoch: 165/200, Train Loss: 2.11699, Valid Loss: 2.25417, lr: 0.000131\n",
      "Epoch: 166/200, Train Loss: 2.09384, Valid Loss: 2.28673, lr: 0.000129\n",
      "Epoch: 167/200, Train Loss: 2.10244, Valid Loss: 2.26439, lr: 0.000127\n",
      "Epoch: 168/200, Train Loss: 2.09900, Valid Loss: 2.25163, lr: 0.000126\n",
      "Epoch: 169/200, Train Loss: 2.10232, Valid Loss: 2.30668, lr: 0.000124\n",
      "Epoch: 170/200, Train Loss: 2.12990, Valid Loss: 2.23172, lr: 0.000123\n",
      "Epoch: 171/200, Train Loss: 2.11198, Valid Loss: 2.29685, lr: 0.000121\n",
      "Epoch: 172/200, Train Loss: 2.10902, Valid Loss: 2.28289, lr: 0.000120\n",
      "Epoch: 173/200, Train Loss: 2.11544, Valid Loss: 2.22110, lr: 0.000118\n",
      "Epoch: 174/200, Train Loss: 2.09465, Valid Loss: 2.34286, lr: 0.000117\n",
      "Epoch: 175/200, Train Loss: 2.07648, Valid Loss: 2.24186, lr: 0.000116\n",
      "Epoch: 176/200, Train Loss: 2.08969, Valid Loss: 2.24184, lr: 0.000115\n",
      "Epoch: 177/200, Train Loss: 2.08559, Valid Loss: 2.32732, lr: 0.000113\n",
      "Epoch: 178/200, Train Loss: 2.11953, Valid Loss: 2.23934, lr: 0.000112\n",
      "Epoch: 179/200, Train Loss: 2.11151, Valid Loss: 2.27807, lr: 0.000111\n",
      "Epoch: 180/200, Train Loss: 2.14224, Valid Loss: 2.27185, lr: 0.000110\n",
      "Epoch: 181/200, Train Loss: 2.05121, Valid Loss: 2.22470, lr: 0.000109\n",
      "Epoch: 182/200, Train Loss: 2.06352, Valid Loss: 2.31536, lr: 0.000108\n",
      "Epoch: 183/200, Train Loss: 2.07957, Valid Loss: 2.22715, lr: 0.000107\n",
      "Epoch: 184/200, Train Loss: 2.07441, Valid Loss: 2.27739, lr: 0.000107\n",
      "Epoch: 185/200, Train Loss: 2.12391, Valid Loss: 2.27192, lr: 0.000106\n",
      "Epoch: 186/200, Train Loss: 2.09395, Valid Loss: 2.25231, lr: 0.000105\n",
      "Epoch: 187/200, Train Loss: 2.09762, Valid Loss: 2.25935, lr: 0.000104\n",
      "Epoch: 188/200, Train Loss: 2.10345, Valid Loss: 2.25459, lr: 0.000104\n",
      "Epoch: 189/200, Train Loss: 2.08025, Valid Loss: 2.27746, lr: 0.000103\n",
      "Epoch: 190/200, Train Loss: 2.08805, Valid Loss: 2.25847, lr: 0.000103\n",
      "Epoch: 191/200, Train Loss: 2.05079, Valid Loss: 2.25421, lr: 0.000102\n",
      "Epoch: 192/200, Train Loss: 2.08174, Valid Loss: 2.28338, lr: 0.000102\n",
      "Epoch: 193/200, Train Loss: 2.06935, Valid Loss: 2.23901, lr: 0.000101\n",
      "Epoch: 194/200, Train Loss: 2.12368, Valid Loss: 2.29752, lr: 0.000101\n",
      "Epoch: 195/200, Train Loss: 2.07216, Valid Loss: 2.25832, lr: 0.000101\n",
      "Epoch: 196/200, Train Loss: 2.04501, Valid Loss: 2.25130, lr: 0.000100\n",
      "Epoch: 197/200, Train Loss: 2.07646, Valid Loss: 2.30885, lr: 0.000100\n",
      "Epoch: 198/200, Train Loss: 2.13052, Valid Loss: 2.23030, lr: 0.000100\n",
      "Epoch: 199/200, Train Loss: 2.09149, Valid Loss: 2.27913, lr: 0.000100\n",
      "Epoch: 200/200, Train Loss: 2.07584, Valid Loss: 2.24512, lr: 0.000100\n",
      "\n",
      "===== Fold 4 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_10732\\2960053717.py:52: UnderReviewWarning: The feature LinearWarmupCosineAnnealingLR is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  scheduler = LinearWarmupCosineAnnealingLR(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/200, Train Loss: 52.22253, Valid Loss: 54.17975, lr: 0.000167\n",
      "Epoch: 2/200, Train Loss: 17.80206, Valid Loss: 3.47324, lr: 0.000333\n",
      "Epoch: 3/200, Train Loss: 6.11123, Valid Loss: 4.41919, lr: 0.000500\n",
      "Epoch: 4/200, Train Loss: 4.83951, Valid Loss: 4.12231, lr: 0.000500\n",
      "Epoch: 5/200, Train Loss: 3.90520, Valid Loss: 3.36918, lr: 0.000500\n",
      "Epoch: 6/200, Train Loss: 3.36815, Valid Loss: 3.31636, lr: 0.000500\n",
      "Epoch: 7/200, Train Loss: 3.12028, Valid Loss: 3.76015, lr: 0.000500\n",
      "Epoch: 8/200, Train Loss: 3.04243, Valid Loss: 3.23215, lr: 0.000500\n",
      "Epoch: 9/200, Train Loss: 3.02084, Valid Loss: 3.28598, lr: 0.000499\n",
      "Epoch: 10/200, Train Loss: 2.95312, Valid Loss: 3.34055, lr: 0.000499\n",
      "Epoch: 11/200, Train Loss: 2.94827, Valid Loss: 3.24102, lr: 0.000499\n",
      "Epoch: 12/200, Train Loss: 2.87678, Valid Loss: 3.27465, lr: 0.000498\n",
      "Epoch: 13/200, Train Loss: 2.93206, Valid Loss: 3.20745, lr: 0.000498\n",
      "Epoch: 14/200, Train Loss: 2.92757, Valid Loss: 3.22280, lr: 0.000497\n",
      "Epoch: 15/200, Train Loss: 2.83199, Valid Loss: 3.18614, lr: 0.000497\n",
      "Epoch: 16/200, Train Loss: 2.93053, Valid Loss: 3.19619, lr: 0.000496\n",
      "Epoch: 17/200, Train Loss: 2.80456, Valid Loss: 3.16552, lr: 0.000496\n",
      "Epoch: 18/200, Train Loss: 2.84707, Valid Loss: 3.13896, lr: 0.000495\n",
      "Epoch: 19/200, Train Loss: 2.85649, Valid Loss: 3.15335, lr: 0.000494\n",
      "Epoch: 20/200, Train Loss: 2.78776, Valid Loss: 3.15007, lr: 0.000493\n",
      "Epoch: 21/200, Train Loss: 2.72793, Valid Loss: 3.15403, lr: 0.000493\n",
      "Epoch: 22/200, Train Loss: 2.83554, Valid Loss: 3.12711, lr: 0.000492\n",
      "Epoch: 23/200, Train Loss: 2.77138, Valid Loss: 3.08638, lr: 0.000491\n",
      "Epoch: 24/200, Train Loss: 2.72300, Valid Loss: 3.12130, lr: 0.000490\n",
      "Epoch: 25/200, Train Loss: 2.73645, Valid Loss: 3.06827, lr: 0.000489\n",
      "Epoch: 26/200, Train Loss: 2.75045, Valid Loss: 3.11466, lr: 0.000488\n",
      "Epoch: 27/200, Train Loss: 2.73132, Valid Loss: 3.02720, lr: 0.000487\n",
      "Epoch: 28/200, Train Loss: 2.73196, Valid Loss: 3.05627, lr: 0.000485\n",
      "Epoch: 29/200, Train Loss: 2.69435, Valid Loss: 3.03497, lr: 0.000484\n",
      "Epoch: 30/200, Train Loss: 2.70176, Valid Loss: 3.02239, lr: 0.000483\n",
      "Epoch: 31/200, Train Loss: 2.65561, Valid Loss: 2.99105, lr: 0.000482\n",
      "Epoch: 32/200, Train Loss: 2.68954, Valid Loss: 3.00809, lr: 0.000480\n",
      "Epoch: 33/200, Train Loss: 2.62982, Valid Loss: 2.96251, lr: 0.000479\n",
      "Epoch: 34/200, Train Loss: 2.63942, Valid Loss: 2.98408, lr: 0.000477\n",
      "Epoch: 35/200, Train Loss: 2.69335, Valid Loss: 2.96253, lr: 0.000476\n",
      "Epoch: 36/200, Train Loss: 2.66499, Valid Loss: 2.95619, lr: 0.000474\n",
      "Epoch: 37/200, Train Loss: 2.57408, Valid Loss: 2.94520, lr: 0.000473\n",
      "Epoch: 38/200, Train Loss: 2.59675, Valid Loss: 2.91062, lr: 0.000471\n",
      "Epoch: 39/200, Train Loss: 2.60964, Valid Loss: 2.93173, lr: 0.000469\n",
      "Epoch: 40/200, Train Loss: 2.58416, Valid Loss: 2.90946, lr: 0.000468\n",
      "Epoch: 41/200, Train Loss: 2.64921, Valid Loss: 2.89595, lr: 0.000466\n",
      "Epoch: 42/200, Train Loss: 2.61169, Valid Loss: 2.86897, lr: 0.000464\n",
      "Epoch: 43/200, Train Loss: 2.60992, Valid Loss: 2.88651, lr: 0.000462\n",
      "Epoch: 44/200, Train Loss: 2.53419, Valid Loss: 2.85430, lr: 0.000460\n",
      "Epoch: 45/200, Train Loss: 2.55086, Valid Loss: 2.84453, lr: 0.000458\n",
      "Epoch: 46/200, Train Loss: 2.54253, Valid Loss: 2.85755, lr: 0.000456\n",
      "Epoch: 47/200, Train Loss: 2.50414, Valid Loss: 2.81806, lr: 0.000454\n",
      "Epoch: 48/200, Train Loss: 2.58570, Valid Loss: 2.82061, lr: 0.000452\n",
      "Epoch: 49/200, Train Loss: 2.53024, Valid Loss: 2.80194, lr: 0.000450\n",
      "Epoch: 50/200, Train Loss: 2.50282, Valid Loss: 2.80311, lr: 0.000448\n",
      "Epoch: 51/200, Train Loss: 2.50480, Valid Loss: 2.78031, lr: 0.000446\n",
      "Epoch: 52/200, Train Loss: 2.55716, Valid Loss: 2.77523, lr: 0.000444\n",
      "Epoch: 53/200, Train Loss: 2.45378, Valid Loss: 2.74672, lr: 0.000441\n",
      "Epoch: 54/200, Train Loss: 2.51503, Valid Loss: 2.75563, lr: 0.000439\n",
      "Epoch: 55/200, Train Loss: 2.48268, Valid Loss: 2.76303, lr: 0.000437\n",
      "Epoch: 56/200, Train Loss: 2.48981, Valid Loss: 2.71306, lr: 0.000434\n",
      "Epoch: 57/200, Train Loss: 2.43105, Valid Loss: 2.76543, lr: 0.000432\n",
      "Epoch: 58/200, Train Loss: 2.45281, Valid Loss: 2.69281, lr: 0.000430\n",
      "Epoch: 59/200, Train Loss: 2.49954, Valid Loss: 2.71911, lr: 0.000427\n",
      "Epoch: 60/200, Train Loss: 2.45084, Valid Loss: 2.67937, lr: 0.000425\n",
      "Epoch: 61/200, Train Loss: 2.47608, Valid Loss: 2.70195, lr: 0.000422\n",
      "Epoch: 62/200, Train Loss: 2.36248, Valid Loss: 2.66551, lr: 0.000420\n",
      "Epoch: 63/200, Train Loss: 2.40417, Valid Loss: 2.70956, lr: 0.000417\n",
      "Epoch: 64/200, Train Loss: 2.39099, Valid Loss: 2.62468, lr: 0.000414\n",
      "Epoch: 65/200, Train Loss: 2.36630, Valid Loss: 2.69617, lr: 0.000412\n",
      "Epoch: 66/200, Train Loss: 2.39566, Valid Loss: 2.61368, lr: 0.000409\n",
      "Epoch: 67/200, Train Loss: 2.36993, Valid Loss: 2.66294, lr: 0.000406\n",
      "Epoch: 68/200, Train Loss: 2.40923, Valid Loss: 2.61332, lr: 0.000404\n",
      "Epoch: 69/200, Train Loss: 2.36291, Valid Loss: 2.63502, lr: 0.000401\n",
      "Epoch: 70/200, Train Loss: 2.34136, Valid Loss: 2.59493, lr: 0.000398\n",
      "Epoch: 71/200, Train Loss: 2.33528, Valid Loss: 2.58550, lr: 0.000395\n",
      "Epoch: 72/200, Train Loss: 2.39323, Valid Loss: 2.67814, lr: 0.000393\n",
      "Epoch: 73/200, Train Loss: 2.34537, Valid Loss: 2.54423, lr: 0.000390\n",
      "Epoch: 74/200, Train Loss: 2.34713, Valid Loss: 2.69094, lr: 0.000387\n",
      "Epoch: 75/200, Train Loss: 2.35026, Valid Loss: 2.52786, lr: 0.000384\n",
      "Epoch: 76/200, Train Loss: 2.36730, Valid Loss: 2.67719, lr: 0.000381\n",
      "Epoch: 77/200, Train Loss: 2.35161, Valid Loss: 2.51372, lr: 0.000378\n",
      "Epoch: 78/200, Train Loss: 2.39294, Valid Loss: 2.65164, lr: 0.000375\n",
      "Epoch: 79/200, Train Loss: 2.28814, Valid Loss: 2.50165, lr: 0.000372\n",
      "Epoch: 80/200, Train Loss: 2.30738, Valid Loss: 2.59282, lr: 0.000369\n",
      "Epoch: 81/200, Train Loss: 2.32805, Valid Loss: 2.52530, lr: 0.000366\n",
      "Epoch: 82/200, Train Loss: 2.31417, Valid Loss: 2.52495, lr: 0.000363\n",
      "Epoch: 83/200, Train Loss: 2.30341, Valid Loss: 2.51573, lr: 0.000360\n",
      "Epoch: 84/200, Train Loss: 2.26450, Valid Loss: 2.49686, lr: 0.000357\n",
      "Epoch: 85/200, Train Loss: 2.25522, Valid Loss: 2.49282, lr: 0.000354\n",
      "Epoch: 86/200, Train Loss: 2.28160, Valid Loss: 2.52016, lr: 0.000351\n",
      "Epoch: 87/200, Train Loss: 2.30142, Valid Loss: 2.45472, lr: 0.000348\n",
      "Epoch: 88/200, Train Loss: 2.24818, Valid Loss: 2.53169, lr: 0.000345\n",
      "Epoch: 89/200, Train Loss: 2.28279, Valid Loss: 2.45712, lr: 0.000341\n",
      "Epoch: 90/200, Train Loss: 2.27216, Valid Loss: 2.48239, lr: 0.000338\n",
      "Epoch: 91/200, Train Loss: 2.28939, Valid Loss: 2.45895, lr: 0.000335\n",
      "Epoch: 92/200, Train Loss: 2.20196, Valid Loss: 2.45794, lr: 0.000332\n",
      "Epoch: 93/200, Train Loss: 2.25597, Valid Loss: 2.45588, lr: 0.000329\n",
      "Epoch: 94/200, Train Loss: 2.27717, Valid Loss: 2.45278, lr: 0.000326\n",
      "Epoch: 95/200, Train Loss: 2.22007, Valid Loss: 2.45439, lr: 0.000322\n",
      "Epoch: 96/200, Train Loss: 2.24608, Valid Loss: 2.44088, lr: 0.000319\n",
      "Epoch: 97/200, Train Loss: 2.23306, Valid Loss: 2.43595, lr: 0.000316\n",
      "Epoch: 98/200, Train Loss: 2.24223, Valid Loss: 2.44101, lr: 0.000313\n",
      "Epoch: 99/200, Train Loss: 2.25677, Valid Loss: 2.41599, lr: 0.000310\n",
      "Epoch: 100/200, Train Loss: 2.24191, Valid Loss: 2.44211, lr: 0.000306\n",
      "Epoch: 101/200, Train Loss: 2.19911, Valid Loss: 2.44061, lr: 0.000303\n",
      "Epoch: 102/200, Train Loss: 2.26698, Valid Loss: 2.40638, lr: 0.000300\n",
      "Epoch: 103/200, Train Loss: 2.25072, Valid Loss: 2.44382, lr: 0.000297\n",
      "Epoch: 104/200, Train Loss: 2.19557, Valid Loss: 2.41503, lr: 0.000294\n",
      "Epoch: 105/200, Train Loss: 2.17948, Valid Loss: 2.38675, lr: 0.000290\n",
      "Epoch: 106/200, Train Loss: 2.17928, Valid Loss: 2.41513, lr: 0.000287\n",
      "Epoch: 107/200, Train Loss: 2.17837, Valid Loss: 2.40548, lr: 0.000284\n",
      "Epoch: 108/200, Train Loss: 2.19270, Valid Loss: 2.40127, lr: 0.000281\n",
      "Epoch: 109/200, Train Loss: 2.16632, Valid Loss: 2.37238, lr: 0.000278\n",
      "Epoch: 110/200, Train Loss: 2.22682, Valid Loss: 2.38975, lr: 0.000274\n",
      "Epoch: 111/200, Train Loss: 2.24524, Valid Loss: 2.39651, lr: 0.000271\n",
      "Epoch: 112/200, Train Loss: 2.21392, Valid Loss: 2.39384, lr: 0.000268\n",
      "Epoch: 113/200, Train Loss: 2.21428, Valid Loss: 2.40617, lr: 0.000265\n",
      "Epoch: 114/200, Train Loss: 2.21009, Valid Loss: 2.37849, lr: 0.000262\n",
      "Epoch: 115/200, Train Loss: 2.24529, Valid Loss: 2.36080, lr: 0.000259\n",
      "Epoch: 116/200, Train Loss: 2.21948, Valid Loss: 2.41008, lr: 0.000255\n",
      "Epoch: 117/200, Train Loss: 2.19912, Valid Loss: 2.35367, lr: 0.000252\n",
      "Epoch: 118/200, Train Loss: 2.21668, Valid Loss: 2.40541, lr: 0.000249\n",
      "Epoch: 119/200, Train Loss: 2.21720, Valid Loss: 2.36256, lr: 0.000246\n",
      "Epoch: 120/200, Train Loss: 2.18807, Valid Loss: 2.35854, lr: 0.000243\n",
      "Epoch: 121/200, Train Loss: 2.16515, Valid Loss: 2.38237, lr: 0.000240\n",
      "Epoch: 122/200, Train Loss: 2.21147, Valid Loss: 2.36588, lr: 0.000237\n",
      "Epoch: 123/200, Train Loss: 2.21251, Valid Loss: 2.37231, lr: 0.000234\n",
      "Epoch: 124/200, Train Loss: 2.18193, Valid Loss: 2.34339, lr: 0.000231\n",
      "Epoch: 125/200, Train Loss: 2.14893, Valid Loss: 2.36807, lr: 0.000228\n",
      "Epoch: 126/200, Train Loss: 2.13192, Valid Loss: 2.34138, lr: 0.000225\n",
      "Epoch: 127/200, Train Loss: 2.14169, Valid Loss: 2.34760, lr: 0.000222\n",
      "Epoch: 128/200, Train Loss: 2.17080, Valid Loss: 2.34408, lr: 0.000219\n",
      "Epoch: 129/200, Train Loss: 2.15791, Valid Loss: 2.33991, lr: 0.000216\n",
      "Epoch: 130/200, Train Loss: 2.12620, Valid Loss: 2.34872, lr: 0.000213\n",
      "Epoch: 131/200, Train Loss: 2.17924, Valid Loss: 2.35130, lr: 0.000210\n",
      "Epoch: 132/200, Train Loss: 2.14047, Valid Loss: 2.32819, lr: 0.000207\n",
      "Epoch: 133/200, Train Loss: 2.13994, Valid Loss: 2.34723, lr: 0.000205\n",
      "Epoch: 134/200, Train Loss: 2.20344, Valid Loss: 2.33830, lr: 0.000202\n",
      "Epoch: 135/200, Train Loss: 2.17911, Valid Loss: 2.34485, lr: 0.000199\n",
      "Epoch: 136/200, Train Loss: 2.14663, Valid Loss: 2.36013, lr: 0.000196\n",
      "Epoch: 137/200, Train Loss: 2.10790, Valid Loss: 2.31267, lr: 0.000194\n",
      "Epoch: 138/200, Train Loss: 2.18239, Valid Loss: 2.33343, lr: 0.000191\n",
      "Epoch: 139/200, Train Loss: 2.11327, Valid Loss: 2.33265, lr: 0.000188\n",
      "Epoch: 140/200, Train Loss: 2.16024, Valid Loss: 2.31461, lr: 0.000186\n",
      "Epoch: 141/200, Train Loss: 2.11482, Valid Loss: 2.34640, lr: 0.000183\n",
      "Epoch: 142/200, Train Loss: 2.13821, Valid Loss: 2.31762, lr: 0.000180\n",
      "Epoch: 143/200, Train Loss: 2.10161, Valid Loss: 2.32530, lr: 0.000178\n",
      "Epoch: 144/200, Train Loss: 2.15717, Valid Loss: 2.32530, lr: 0.000175\n",
      "Epoch: 145/200, Train Loss: 2.15519, Valid Loss: 2.33175, lr: 0.000173\n",
      "Epoch: 146/200, Train Loss: 2.12494, Valid Loss: 2.31197, lr: 0.000170\n",
      "Epoch: 147/200, Train Loss: 2.11573, Valid Loss: 2.33619, lr: 0.000168\n",
      "Epoch: 148/200, Train Loss: 2.11932, Valid Loss: 2.31914, lr: 0.000166\n",
      "Epoch: 149/200, Train Loss: 2.14457, Valid Loss: 2.32842, lr: 0.000163\n",
      "Epoch: 150/200, Train Loss: 2.09838, Valid Loss: 2.32149, lr: 0.000161\n",
      "Epoch: 151/200, Train Loss: 2.11987, Valid Loss: 2.30617, lr: 0.000159\n",
      "Epoch: 152/200, Train Loss: 2.13835, Valid Loss: 2.32434, lr: 0.000156\n",
      "Epoch: 153/200, Train Loss: 2.13784, Valid Loss: 2.32583, lr: 0.000154\n",
      "Epoch: 154/200, Train Loss: 2.10850, Valid Loss: 2.29401, lr: 0.000152\n",
      "Epoch: 155/200, Train Loss: 2.10765, Valid Loss: 2.31996, lr: 0.000150\n",
      "Epoch: 156/200, Train Loss: 2.14202, Valid Loss: 2.32578, lr: 0.000148\n",
      "Epoch: 157/200, Train Loss: 2.11634, Valid Loss: 2.29269, lr: 0.000146\n",
      "Epoch: 158/200, Train Loss: 2.12535, Valid Loss: 2.33923, lr: 0.000144\n",
      "Epoch: 159/200, Train Loss: 2.12124, Valid Loss: 2.31262, lr: 0.000142\n",
      "Epoch: 160/200, Train Loss: 2.10197, Valid Loss: 2.30292, lr: 0.000140\n",
      "Epoch: 161/200, Train Loss: 2.14488, Valid Loss: 2.31006, lr: 0.000138\n",
      "Epoch: 162/200, Train Loss: 2.13556, Valid Loss: 2.30878, lr: 0.000136\n",
      "Epoch: 163/200, Train Loss: 2.09614, Valid Loss: 2.30249, lr: 0.000134\n",
      "Epoch: 164/200, Train Loss: 2.10489, Valid Loss: 2.31503, lr: 0.000132\n",
      "Epoch: 165/200, Train Loss: 2.08875, Valid Loss: 2.31761, lr: 0.000131\n",
      "Epoch: 166/200, Train Loss: 2.12085, Valid Loss: 2.29298, lr: 0.000129\n",
      "Epoch: 167/200, Train Loss: 2.09231, Valid Loss: 2.32309, lr: 0.000127\n",
      "Epoch: 168/200, Train Loss: 2.10872, Valid Loss: 2.30721, lr: 0.000126\n",
      "Epoch: 169/200, Train Loss: 2.11273, Valid Loss: 2.28978, lr: 0.000124\n",
      "Epoch: 170/200, Train Loss: 2.05352, Valid Loss: 2.30200, lr: 0.000123\n",
      "Epoch: 171/200, Train Loss: 2.08575, Valid Loss: 2.31089, lr: 0.000121\n",
      "Epoch: 172/200, Train Loss: 2.06129, Valid Loss: 2.29758, lr: 0.000120\n",
      "Epoch: 173/200, Train Loss: 2.08340, Valid Loss: 2.30057, lr: 0.000118\n",
      "Epoch: 174/200, Train Loss: 2.09064, Valid Loss: 2.28537, lr: 0.000117\n",
      "Epoch: 175/200, Train Loss: 2.12175, Valid Loss: 2.30274, lr: 0.000116\n",
      "Epoch: 176/200, Train Loss: 2.15409, Valid Loss: 2.29960, lr: 0.000115\n",
      "Epoch: 177/200, Train Loss: 2.09735, Valid Loss: 2.29022, lr: 0.000113\n",
      "Epoch: 178/200, Train Loss: 2.09491, Valid Loss: 2.31968, lr: 0.000112\n",
      "Epoch: 179/200, Train Loss: 2.12557, Valid Loss: 2.29325, lr: 0.000111\n",
      "Epoch: 180/200, Train Loss: 2.10586, Valid Loss: 2.28348, lr: 0.000110\n",
      "Epoch: 181/200, Train Loss: 2.12367, Valid Loss: 2.29970, lr: 0.000109\n",
      "Epoch: 182/200, Train Loss: 2.10239, Valid Loss: 2.28640, lr: 0.000108\n",
      "Epoch: 183/200, Train Loss: 2.09963, Valid Loss: 2.31262, lr: 0.000107\n",
      "Epoch: 184/200, Train Loss: 2.07381, Valid Loss: 2.28799, lr: 0.000107\n",
      "Epoch: 185/200, Train Loss: 2.15671, Valid Loss: 2.28385, lr: 0.000106\n",
      "Epoch: 186/200, Train Loss: 2.07184, Valid Loss: 2.31107, lr: 0.000105\n",
      "Epoch: 187/200, Train Loss: 2.12770, Valid Loss: 2.28193, lr: 0.000104\n",
      "Epoch: 188/200, Train Loss: 2.08628, Valid Loss: 2.27954, lr: 0.000104\n",
      "Epoch: 189/200, Train Loss: 2.10434, Valid Loss: 2.30071, lr: 0.000103\n",
      "Epoch: 190/200, Train Loss: 2.08157, Valid Loss: 2.29680, lr: 0.000103\n",
      "Epoch: 191/200, Train Loss: 2.06092, Valid Loss: 2.28308, lr: 0.000102\n",
      "Epoch: 192/200, Train Loss: 2.13281, Valid Loss: 2.29692, lr: 0.000102\n",
      "Epoch: 193/200, Train Loss: 2.06985, Valid Loss: 2.28028, lr: 0.000101\n",
      "Epoch: 194/200, Train Loss: 2.06146, Valid Loss: 2.28562, lr: 0.000101\n",
      "Epoch: 195/200, Train Loss: 2.08116, Valid Loss: 2.28078, lr: 0.000101\n",
      "Epoch: 196/200, Train Loss: 2.12590, Valid Loss: 2.29150, lr: 0.000100\n",
      "Epoch: 197/200, Train Loss: 2.10866, Valid Loss: 2.30351, lr: 0.000100\n",
      "Epoch: 198/200, Train Loss: 2.11664, Valid Loss: 2.27027, lr: 0.000100\n",
      "Epoch: 199/200, Train Loss: 2.09364, Valid Loss: 2.29317, lr: 0.000100\n",
      "Epoch: 200/200, Train Loss: 2.10266, Valid Loss: 2.29218, lr: 0.000100\n",
      "\n",
      "===== Fold 5 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_10732\\2960053717.py:52: UnderReviewWarning: The feature LinearWarmupCosineAnnealingLR is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  scheduler = LinearWarmupCosineAnnealingLR(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/200, Train Loss: 73.47874, Valid Loss: 71.90727, lr: 0.000167\n",
      "Epoch: 2/200, Train Loss: 21.38145, Valid Loss: 4.94181, lr: 0.000333\n",
      "Epoch: 3/200, Train Loss: 6.53226, Valid Loss: 3.19139, lr: 0.000500\n",
      "Epoch: 4/200, Train Loss: 5.43358, Valid Loss: 2.77083, lr: 0.000500\n",
      "Epoch: 5/200, Train Loss: 3.93595, Valid Loss: 3.34969, lr: 0.000500\n",
      "Epoch: 6/200, Train Loss: 3.39498, Valid Loss: 3.15649, lr: 0.000500\n",
      "Epoch: 7/200, Train Loss: 3.57640, Valid Loss: 2.71604, lr: 0.000500\n",
      "Epoch: 8/200, Train Loss: 3.22061, Valid Loss: 2.78655, lr: 0.000500\n",
      "Epoch: 9/200, Train Loss: 3.18505, Valid Loss: 2.74083, lr: 0.000499\n",
      "Epoch: 10/200, Train Loss: 3.12585, Valid Loss: 2.66945, lr: 0.000499\n",
      "Epoch: 11/200, Train Loss: 3.14811, Valid Loss: 2.65925, lr: 0.000499\n",
      "Epoch: 12/200, Train Loss: 3.06587, Valid Loss: 2.63866, lr: 0.000498\n",
      "Epoch: 13/200, Train Loss: 3.06142, Valid Loss: 2.63040, lr: 0.000498\n",
      "Epoch: 14/200, Train Loss: 3.07574, Valid Loss: 2.61414, lr: 0.000497\n",
      "Epoch: 15/200, Train Loss: 3.09884, Valid Loss: 2.60391, lr: 0.000497\n",
      "Epoch: 16/200, Train Loss: 2.97161, Valid Loss: 2.59029, lr: 0.000496\n",
      "Epoch: 17/200, Train Loss: 3.06180, Valid Loss: 2.57783, lr: 0.000496\n",
      "Epoch: 18/200, Train Loss: 3.01521, Valid Loss: 2.57530, lr: 0.000495\n",
      "Epoch: 19/200, Train Loss: 3.07062, Valid Loss: 2.55512, lr: 0.000494\n",
      "Epoch: 20/200, Train Loss: 3.04941, Valid Loss: 2.54790, lr: 0.000493\n",
      "Epoch: 21/200, Train Loss: 3.00893, Valid Loss: 2.53538, lr: 0.000493\n",
      "Epoch: 22/200, Train Loss: 2.94134, Valid Loss: 2.52616, lr: 0.000492\n",
      "Epoch: 23/200, Train Loss: 2.94832, Valid Loss: 2.51328, lr: 0.000491\n",
      "Epoch: 24/200, Train Loss: 2.97259, Valid Loss: 2.49805, lr: 0.000490\n",
      "Epoch: 25/200, Train Loss: 2.95881, Valid Loss: 2.48709, lr: 0.000489\n",
      "Epoch: 26/200, Train Loss: 2.92089, Valid Loss: 2.47859, lr: 0.000488\n",
      "Epoch: 27/200, Train Loss: 2.93986, Valid Loss: 2.46699, lr: 0.000487\n",
      "Epoch: 28/200, Train Loss: 2.92261, Valid Loss: 2.45611, lr: 0.000485\n",
      "Epoch: 29/200, Train Loss: 2.90142, Valid Loss: 2.44906, lr: 0.000484\n",
      "Epoch: 30/200, Train Loss: 2.85348, Valid Loss: 2.43685, lr: 0.000483\n",
      "Epoch: 31/200, Train Loss: 2.76566, Valid Loss: 2.43534, lr: 0.000482\n",
      "Epoch: 32/200, Train Loss: 2.82314, Valid Loss: 2.41691, lr: 0.000480\n",
      "Epoch: 33/200, Train Loss: 2.82729, Valid Loss: 2.41027, lr: 0.000479\n",
      "Epoch: 34/200, Train Loss: 2.83163, Valid Loss: 2.40292, lr: 0.000477\n",
      "Epoch: 35/200, Train Loss: 2.73472, Valid Loss: 2.39170, lr: 0.000476\n",
      "Epoch: 36/200, Train Loss: 2.74336, Valid Loss: 2.39112, lr: 0.000474\n",
      "Epoch: 37/200, Train Loss: 2.78558, Valid Loss: 2.37519, lr: 0.000473\n",
      "Epoch: 38/200, Train Loss: 2.79615, Valid Loss: 2.36612, lr: 0.000471\n",
      "Epoch: 39/200, Train Loss: 2.80656, Valid Loss: 2.35629, lr: 0.000469\n",
      "Epoch: 40/200, Train Loss: 2.78086, Valid Loss: 2.34807, lr: 0.000468\n",
      "Epoch: 41/200, Train Loss: 2.79011, Valid Loss: 2.33773, lr: 0.000466\n",
      "Epoch: 42/200, Train Loss: 2.74890, Valid Loss: 2.35040, lr: 0.000464\n",
      "Epoch: 43/200, Train Loss: 2.73819, Valid Loss: 2.31909, lr: 0.000462\n",
      "Epoch: 44/200, Train Loss: 2.81724, Valid Loss: 2.32240, lr: 0.000460\n",
      "Epoch: 45/200, Train Loss: 2.69614, Valid Loss: 2.30562, lr: 0.000458\n",
      "Epoch: 46/200, Train Loss: 2.69788, Valid Loss: 2.32204, lr: 0.000456\n",
      "Epoch: 47/200, Train Loss: 2.66948, Valid Loss: 2.28854, lr: 0.000454\n",
      "Epoch: 48/200, Train Loss: 2.67677, Valid Loss: 2.29918, lr: 0.000452\n",
      "Epoch: 49/200, Train Loss: 2.64318, Valid Loss: 2.27166, lr: 0.000450\n",
      "Epoch: 50/200, Train Loss: 2.67583, Valid Loss: 2.30514, lr: 0.000448\n",
      "Epoch: 51/200, Train Loss: 2.67182, Valid Loss: 2.25565, lr: 0.000446\n",
      "Epoch: 52/200, Train Loss: 2.66358, Valid Loss: 2.27430, lr: 0.000444\n",
      "Epoch: 53/200, Train Loss: 2.60762, Valid Loss: 2.24343, lr: 0.000441\n",
      "Epoch: 54/200, Train Loss: 2.65158, Valid Loss: 2.23825, lr: 0.000439\n",
      "Epoch: 55/200, Train Loss: 2.65196, Valid Loss: 2.23458, lr: 0.000437\n",
      "Epoch: 56/200, Train Loss: 2.59266, Valid Loss: 2.22828, lr: 0.000434\n",
      "Epoch: 57/200, Train Loss: 2.57269, Valid Loss: 2.22035, lr: 0.000432\n",
      "Epoch: 58/200, Train Loss: 2.56621, Valid Loss: 2.21876, lr: 0.000430\n",
      "Epoch: 59/200, Train Loss: 2.55215, Valid Loss: 2.21570, lr: 0.000427\n",
      "Epoch: 60/200, Train Loss: 2.51013, Valid Loss: 2.19920, lr: 0.000425\n",
      "Epoch: 61/200, Train Loss: 2.56697, Valid Loss: 2.19444, lr: 0.000422\n",
      "Epoch: 62/200, Train Loss: 2.56371, Valid Loss: 2.18924, lr: 0.000420\n",
      "Epoch: 63/200, Train Loss: 2.54179, Valid Loss: 2.17824, lr: 0.000417\n",
      "Epoch: 64/200, Train Loss: 2.53367, Valid Loss: 2.20488, lr: 0.000414\n",
      "Epoch: 65/200, Train Loss: 2.55569, Valid Loss: 2.17092, lr: 0.000412\n",
      "Epoch: 66/200, Train Loss: 2.45370, Valid Loss: 2.16856, lr: 0.000409\n",
      "Epoch: 67/200, Train Loss: 2.49284, Valid Loss: 2.15810, lr: 0.000406\n",
      "Epoch: 68/200, Train Loss: 2.51815, Valid Loss: 2.17072, lr: 0.000404\n",
      "Epoch: 69/200, Train Loss: 2.50261, Valid Loss: 2.14877, lr: 0.000401\n",
      "Epoch: 70/200, Train Loss: 2.51077, Valid Loss: 2.14428, lr: 0.000398\n",
      "Epoch: 71/200, Train Loss: 2.49197, Valid Loss: 2.14165, lr: 0.000395\n",
      "Epoch: 72/200, Train Loss: 2.44681, Valid Loss: 2.13677, lr: 0.000393\n",
      "Epoch: 73/200, Train Loss: 2.42733, Valid Loss: 2.13263, lr: 0.000390\n",
      "Epoch: 74/200, Train Loss: 2.45515, Valid Loss: 2.12390, lr: 0.000387\n",
      "Epoch: 75/200, Train Loss: 2.49720, Valid Loss: 2.12042, lr: 0.000384\n",
      "Epoch: 76/200, Train Loss: 2.48483, Valid Loss: 2.12169, lr: 0.000381\n",
      "Epoch: 77/200, Train Loss: 2.40899, Valid Loss: 2.11810, lr: 0.000378\n",
      "Epoch: 78/200, Train Loss: 2.46782, Valid Loss: 2.10842, lr: 0.000375\n",
      "Epoch: 79/200, Train Loss: 2.45841, Valid Loss: 2.11180, lr: 0.000372\n",
      "Epoch: 80/200, Train Loss: 2.39712, Valid Loss: 2.09437, lr: 0.000369\n",
      "Epoch: 81/200, Train Loss: 2.41147, Valid Loss: 2.11392, lr: 0.000366\n",
      "Epoch: 82/200, Train Loss: 2.40600, Valid Loss: 2.09513, lr: 0.000363\n",
      "Epoch: 83/200, Train Loss: 2.45135, Valid Loss: 2.08596, lr: 0.000360\n",
      "Epoch: 84/200, Train Loss: 2.49942, Valid Loss: 2.11479, lr: 0.000357\n",
      "Epoch: 85/200, Train Loss: 2.40449, Valid Loss: 2.08216, lr: 0.000354\n",
      "Epoch: 86/200, Train Loss: 2.42953, Valid Loss: 2.09268, lr: 0.000351\n",
      "Epoch: 87/200, Train Loss: 2.45105, Valid Loss: 2.07615, lr: 0.000348\n",
      "Epoch: 88/200, Train Loss: 2.41334, Valid Loss: 2.07313, lr: 0.000345\n",
      "Epoch: 89/200, Train Loss: 2.40577, Valid Loss: 2.07064, lr: 0.000341\n",
      "Epoch: 90/200, Train Loss: 2.37437, Valid Loss: 2.08455, lr: 0.000338\n",
      "Epoch: 91/200, Train Loss: 2.40663, Valid Loss: 2.07464, lr: 0.000335\n",
      "Epoch: 92/200, Train Loss: 2.37090, Valid Loss: 2.06471, lr: 0.000332\n",
      "Epoch: 93/200, Train Loss: 2.36169, Valid Loss: 2.05922, lr: 0.000329\n",
      "Epoch: 94/200, Train Loss: 2.37869, Valid Loss: 2.06856, lr: 0.000326\n",
      "Epoch: 95/200, Train Loss: 2.37569, Valid Loss: 2.05106, lr: 0.000322\n",
      "Epoch: 96/200, Train Loss: 2.37270, Valid Loss: 2.05458, lr: 0.000319\n",
      "Epoch: 97/200, Train Loss: 2.29456, Valid Loss: 2.04479, lr: 0.000316\n",
      "Epoch: 98/200, Train Loss: 2.31311, Valid Loss: 2.04185, lr: 0.000313\n",
      "Epoch: 99/200, Train Loss: 2.35302, Valid Loss: 2.05382, lr: 0.000310\n",
      "Epoch: 100/200, Train Loss: 2.34794, Valid Loss: 2.03851, lr: 0.000306\n",
      "Epoch: 101/200, Train Loss: 2.32780, Valid Loss: 2.07603, lr: 0.000303\n",
      "Epoch: 102/200, Train Loss: 2.28366, Valid Loss: 2.02996, lr: 0.000300\n",
      "Epoch: 103/200, Train Loss: 2.36200, Valid Loss: 2.04485, lr: 0.000297\n",
      "Epoch: 104/200, Train Loss: 2.31987, Valid Loss: 2.04504, lr: 0.000294\n",
      "Epoch: 105/200, Train Loss: 2.25729, Valid Loss: 2.03459, lr: 0.000290\n",
      "Epoch: 106/200, Train Loss: 2.33321, Valid Loss: 2.05218, lr: 0.000287\n",
      "Epoch: 107/200, Train Loss: 2.33513, Valid Loss: 2.02773, lr: 0.000284\n",
      "Epoch: 108/200, Train Loss: 2.32495, Valid Loss: 2.02049, lr: 0.000281\n",
      "Epoch: 109/200, Train Loss: 2.34387, Valid Loss: 2.02968, lr: 0.000278\n",
      "Epoch: 110/200, Train Loss: 2.33643, Valid Loss: 2.02732, lr: 0.000274\n",
      "Epoch: 111/200, Train Loss: 2.31916, Valid Loss: 2.03659, lr: 0.000271\n",
      "Epoch: 112/200, Train Loss: 2.32055, Valid Loss: 2.01037, lr: 0.000268\n",
      "Epoch: 113/200, Train Loss: 2.28497, Valid Loss: 2.03313, lr: 0.000265\n",
      "Epoch: 114/200, Train Loss: 2.29956, Valid Loss: 2.01053, lr: 0.000262\n",
      "Epoch: 115/200, Train Loss: 2.29364, Valid Loss: 2.02978, lr: 0.000259\n",
      "Epoch: 116/200, Train Loss: 2.30129, Valid Loss: 2.02474, lr: 0.000255\n",
      "Epoch: 117/200, Train Loss: 2.29309, Valid Loss: 2.00925, lr: 0.000252\n",
      "Epoch: 118/200, Train Loss: 2.30517, Valid Loss: 2.01544, lr: 0.000249\n",
      "Epoch: 119/200, Train Loss: 2.32733, Valid Loss: 2.00361, lr: 0.000246\n",
      "Epoch: 120/200, Train Loss: 2.28156, Valid Loss: 2.01875, lr: 0.000243\n",
      "Epoch: 121/200, Train Loss: 2.28526, Valid Loss: 2.00501, lr: 0.000240\n",
      "Epoch: 122/200, Train Loss: 2.28479, Valid Loss: 2.00808, lr: 0.000237\n",
      "Epoch: 123/200, Train Loss: 2.28621, Valid Loss: 2.00684, lr: 0.000234\n",
      "Epoch: 124/200, Train Loss: 2.28062, Valid Loss: 2.02456, lr: 0.000231\n",
      "Epoch: 125/200, Train Loss: 2.28608, Valid Loss: 1.99583, lr: 0.000228\n",
      "Epoch: 126/200, Train Loss: 2.29640, Valid Loss: 2.02157, lr: 0.000225\n",
      "Epoch: 127/200, Train Loss: 2.27641, Valid Loss: 1.99379, lr: 0.000222\n",
      "Epoch: 128/200, Train Loss: 2.31887, Valid Loss: 2.02095, lr: 0.000219\n",
      "Epoch: 129/200, Train Loss: 2.26208, Valid Loss: 1.98895, lr: 0.000216\n",
      "Epoch: 130/200, Train Loss: 2.28616, Valid Loss: 2.00435, lr: 0.000213\n",
      "Epoch: 131/200, Train Loss: 2.27574, Valid Loss: 1.98725, lr: 0.000210\n",
      "Epoch: 132/200, Train Loss: 2.28139, Valid Loss: 2.01177, lr: 0.000207\n",
      "Epoch: 133/200, Train Loss: 2.26483, Valid Loss: 1.98298, lr: 0.000205\n",
      "Epoch: 134/200, Train Loss: 2.25908, Valid Loss: 1.99331, lr: 0.000202\n",
      "Epoch: 135/200, Train Loss: 2.27109, Valid Loss: 1.99949, lr: 0.000199\n",
      "Epoch: 136/200, Train Loss: 2.27773, Valid Loss: 1.98205, lr: 0.000196\n",
      "Epoch: 137/200, Train Loss: 2.23140, Valid Loss: 2.00229, lr: 0.000194\n",
      "Epoch: 138/200, Train Loss: 2.31006, Valid Loss: 1.98503, lr: 0.000191\n",
      "Epoch: 139/200, Train Loss: 2.29304, Valid Loss: 2.00305, lr: 0.000188\n",
      "Epoch: 140/200, Train Loss: 2.28437, Valid Loss: 1.97585, lr: 0.000186\n",
      "Epoch: 141/200, Train Loss: 2.25280, Valid Loss: 2.01685, lr: 0.000183\n",
      "Epoch: 142/200, Train Loss: 2.21987, Valid Loss: 1.97314, lr: 0.000180\n",
      "Epoch: 143/200, Train Loss: 2.23005, Valid Loss: 2.01796, lr: 0.000178\n",
      "Epoch: 144/200, Train Loss: 2.25451, Valid Loss: 1.96958, lr: 0.000175\n",
      "Epoch: 145/200, Train Loss: 2.30196, Valid Loss: 2.02234, lr: 0.000173\n",
      "Epoch: 146/200, Train Loss: 2.25038, Valid Loss: 1.97606, lr: 0.000170\n",
      "Epoch: 147/200, Train Loss: 2.24203, Valid Loss: 1.97588, lr: 0.000168\n",
      "Epoch: 148/200, Train Loss: 2.24874, Valid Loss: 2.00058, lr: 0.000166\n",
      "Epoch: 149/200, Train Loss: 2.27589, Valid Loss: 1.97038, lr: 0.000163\n",
      "Epoch: 150/200, Train Loss: 2.23566, Valid Loss: 1.98706, lr: 0.000161\n",
      "Epoch: 151/200, Train Loss: 2.28657, Valid Loss: 1.98132, lr: 0.000159\n",
      "Epoch: 152/200, Train Loss: 2.20468, Valid Loss: 1.97050, lr: 0.000156\n",
      "Epoch: 153/200, Train Loss: 2.24456, Valid Loss: 1.98037, lr: 0.000154\n",
      "Epoch: 154/200, Train Loss: 2.21195, Valid Loss: 1.97791, lr: 0.000152\n",
      "Epoch: 155/200, Train Loss: 2.24650, Valid Loss: 1.97449, lr: 0.000150\n",
      "Epoch: 156/200, Train Loss: 2.23787, Valid Loss: 1.98757, lr: 0.000148\n",
      "Epoch: 157/200, Train Loss: 2.23309, Valid Loss: 1.96551, lr: 0.000146\n",
      "Epoch: 158/200, Train Loss: 2.25488, Valid Loss: 1.97931, lr: 0.000144\n",
      "Epoch: 159/200, Train Loss: 2.21723, Valid Loss: 1.99662, lr: 0.000142\n",
      "Epoch: 160/200, Train Loss: 2.22116, Valid Loss: 1.96360, lr: 0.000140\n",
      "Epoch: 161/200, Train Loss: 2.27116, Valid Loss: 1.99282, lr: 0.000138\n",
      "Epoch: 162/200, Train Loss: 2.23498, Valid Loss: 1.97428, lr: 0.000136\n",
      "Epoch: 163/200, Train Loss: 2.23302, Valid Loss: 1.95797, lr: 0.000134\n",
      "Epoch: 164/200, Train Loss: 2.24026, Valid Loss: 2.01914, lr: 0.000132\n",
      "Epoch: 165/200, Train Loss: 2.22812, Valid Loss: 1.95472, lr: 0.000131\n",
      "Epoch: 166/200, Train Loss: 2.20916, Valid Loss: 1.96570, lr: 0.000129\n",
      "Epoch: 167/200, Train Loss: 2.23666, Valid Loss: 1.99599, lr: 0.000127\n",
      "Epoch: 168/200, Train Loss: 2.22435, Valid Loss: 1.95402, lr: 0.000126\n",
      "Epoch: 169/200, Train Loss: 2.23730, Valid Loss: 2.00333, lr: 0.000124\n",
      "Epoch: 170/200, Train Loss: 2.21465, Valid Loss: 1.95425, lr: 0.000123\n",
      "Epoch: 171/200, Train Loss: 2.25295, Valid Loss: 1.97530, lr: 0.000121\n",
      "Epoch: 172/200, Train Loss: 2.21899, Valid Loss: 1.96423, lr: 0.000120\n",
      "Epoch: 173/200, Train Loss: 2.20210, Valid Loss: 1.97029, lr: 0.000118\n",
      "Epoch: 174/200, Train Loss: 2.21590, Valid Loss: 1.95948, lr: 0.000117\n",
      "Epoch: 175/200, Train Loss: 2.24041, Valid Loss: 1.96100, lr: 0.000116\n",
      "Epoch: 176/200, Train Loss: 2.25486, Valid Loss: 1.99766, lr: 0.000115\n",
      "Epoch: 177/200, Train Loss: 2.19035, Valid Loss: 1.94997, lr: 0.000113\n",
      "Epoch: 178/200, Train Loss: 2.24674, Valid Loss: 1.97751, lr: 0.000112\n",
      "Epoch: 179/200, Train Loss: 2.21354, Valid Loss: 1.98080, lr: 0.000111\n",
      "Epoch: 180/200, Train Loss: 2.21265, Valid Loss: 1.94775, lr: 0.000110\n",
      "Epoch: 181/200, Train Loss: 2.20739, Valid Loss: 1.97772, lr: 0.000109\n",
      "Epoch: 182/200, Train Loss: 2.22764, Valid Loss: 1.97566, lr: 0.000108\n",
      "Epoch: 183/200, Train Loss: 2.27428, Valid Loss: 1.94835, lr: 0.000107\n",
      "Epoch: 184/200, Train Loss: 2.19416, Valid Loss: 1.96420, lr: 0.000107\n",
      "Epoch: 185/200, Train Loss: 2.17785, Valid Loss: 1.96410, lr: 0.000106\n",
      "Epoch: 186/200, Train Loss: 2.22758, Valid Loss: 1.96334, lr: 0.000105\n",
      "Epoch: 187/200, Train Loss: 2.25086, Valid Loss: 1.95772, lr: 0.000104\n",
      "Epoch: 188/200, Train Loss: 2.20452, Valid Loss: 1.95253, lr: 0.000104\n",
      "Epoch: 189/200, Train Loss: 2.22112, Valid Loss: 1.97004, lr: 0.000103\n",
      "Epoch: 190/200, Train Loss: 2.19366, Valid Loss: 1.95718, lr: 0.000103\n",
      "Epoch: 191/200, Train Loss: 2.20753, Valid Loss: 1.95450, lr: 0.000102\n",
      "Epoch: 192/200, Train Loss: 2.22646, Valid Loss: 1.97261, lr: 0.000102\n",
      "Epoch: 193/200, Train Loss: 2.25178, Valid Loss: 1.97445, lr: 0.000101\n",
      "Epoch: 194/200, Train Loss: 2.17254, Valid Loss: 1.94876, lr: 0.000101\n",
      "Epoch: 195/200, Train Loss: 2.15146, Valid Loss: 1.96192, lr: 0.000101\n",
      "Epoch: 196/200, Train Loss: 2.18312, Valid Loss: 1.96782, lr: 0.000100\n",
      "Epoch: 197/200, Train Loss: 2.26270, Valid Loss: 1.94707, lr: 0.000100\n",
      "Epoch: 198/200, Train Loss: 2.22851, Valid Loss: 1.96899, lr: 0.000100\n",
      "Epoch: 199/200, Train Loss: 2.21159, Valid Loss: 1.94954, lr: 0.000100\n",
      "Epoch: 200/200, Train Loss: 2.21465, Valid Loss: 1.96201, lr: 0.000100\n",
      "\n",
      "MLP CV scores: [np.float64(1.5378697095775882), np.float64(1.4532891680561855), np.float64(1.498371641125856), np.float64(1.513995393953667), np.float64(1.4007185304536112)]\n",
      "MLP 평균 RMSE: 1.4808488886333815\n"
     ]
    }
   ],
   "source": [
    "# K-fold CV\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# final_dataset에서 feature / target 분리\n",
    "train_df = final_dataset.drop(['smiles', 'IC50_nM'], axis=1)\n",
    "# Fingerprint 벡터는 2D numpy 배열로 변환\n",
    "fp_array = np.vstack(train_df['Fingerprint'].values).astype(np.float32)  # (n_samples, fp_dim)\n",
    "# Fingerprint 컬럼 제외한 나머지 feature 추출\n",
    "other_features = train_df.drop(columns=['Fingerprint', 'pIC50']).copy()\n",
    "# bool 타입은 float32로 변환\n",
    "for col in other_features.columns:\n",
    "    if other_features[col].dtype == 'bool':\n",
    "        other_features[col] = other_features[col].astype(np.float32)\n",
    "other_array = other_features.values.astype(np.float32)  # (n_samples, other_dim)\n",
    "\n",
    "# 최종 feature 합치기\n",
    "X_np = np.hstack([fp_array, other_array])  # (n_samples, total_features)\n",
    "y_np = train_df['pIC50'].values.astype(np.float32)\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "mlp_cv_scores = []\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(kf.split(X_np)):\n",
    "    print(f\"\\n===== Fold {fold+1} =====\")\n",
    "    \n",
    "    # fold별 train/valid 나누기\n",
    "    X_train, X_valid = X_np[train_idx], X_np[valid_idx]\n",
    "    y_train, y_valid = y_np[train_idx], y_np[valid_idx]\n",
    "\n",
    "    # VarianceThreshold 적용 (train에 fit, valid는 transform)\n",
    "    transform = VarianceThreshold(threshold=0.05)\n",
    "    X_train = transform.fit_transform(X_train)\n",
    "    X_valid = transform.transform(X_valid)\n",
    "    \n",
    "    # Tensor 변환\n",
    "    train_dataset = KfoldCustomDataset(X_train, y_train, is_test=False)\n",
    "    valid_dataset = KfoldCustomDataset(X_valid, y_valid, is_test=False)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=CFG_model['BATCH_SIZE'], shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=CFG_model['BATCH_SIZE'], shuffle=False)\n",
    "\n",
    "    # 모델 초기화 (Net은 MLP 모델 클래스)\n",
    "    model = Net(\n",
    "        input_size=X_train.shape[1], \n",
    "        hidden_size=CFG_model['HIDDEN_SIZE'], \n",
    "        dropout_rate=CFG_model['DROPOUT_RATE'], \n",
    "        out_size=CFG_model['OUTPUT_SIZE']\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=CFG_model['LEARNING_RATE'], weight_decay=1e-4)\n",
    "    scheduler = LinearWarmupCosineAnnealingLR(\n",
    "        optimizer,\n",
    "        warmup_epochs=CFG_model['WARMUP_EPOCHS'],\n",
    "        max_epochs=CFG_model['MAX_EPOCHS'],\n",
    "        eta_min=1e-4\n",
    "    )\n",
    "    \n",
    "    # 학습\n",
    "    model_train, train_losses, valid_losses, learning_rates= train(\n",
    "        train_loader, valid_loader, model, criterion, optimizer, scheduler,\n",
    "        epochs=CFG_model['EPOCHS']\n",
    "    )\n",
    "\n",
    "    # fold별 valid score 저장 (RMSE)\n",
    "    rmse = np.sqrt(valid_losses[-1])  # 마지막 epoch의 valid loss 사용\n",
    "    mlp_cv_scores.append(rmse)\n",
    "\n",
    "print(\"\\nMLP CV scores:\", mlp_cv_scores)\n",
    "print(\"MLP 평균 RMSE:\", np.mean(mlp_cv_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "997dbe62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_10732\\2424086588.py:48: UnderReviewWarning: The feature LinearWarmupCosineAnnealingLR is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  scheduler = LinearWarmupCosineAnnealingLR(\n"
     ]
    }
   ],
   "source": [
    "# k-fold 끝난 뒤, 단독 학습 시작 전에 반드시 환경 초기화 및 새 객체 생성\n",
    "\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "def reset_all(seed=42):\n",
    "    import random\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    import os\n",
    "    \n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "reset_all(CFG['SEED'])\n",
    "\n",
    "# 단독 학습용 데이터셋, DataLoader 다시 만들기\n",
    "X_train = prepare_features(train_df_split)\n",
    "X_valid = prepare_features(valid_df_split)\n",
    "y_train = train_df_split['pIC50'].values.astype(np.float32)\n",
    "y_valid = valid_df_split['pIC50'].values.astype(np.float32)\n",
    "transform = VarianceThreshold(threshold=0.05)\n",
    "X_train_tr = transform.fit_transform(X_train)\n",
    "X_valid_tr = transform.transform(X_valid)\n",
    "train_dataset = CustomDataset(X_train_tr, y_train)\n",
    "valid_dataset = CustomDataset(X_valid_tr, y_valid)\n",
    "train_loader = DataLoader(train_dataset, batch_size=CFG_model['BATCH_SIZE'], shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=CFG_model['BATCH_SIZE'], shuffle=False)\n",
    "\n",
    "# 단독 학습용 모델, optimizer, scheduler 새로 생성\n",
    "model = Net(\n",
    "    input_size=train_dataset.X.shape[1],\n",
    "    hidden_size=CFG_model['HIDDEN_SIZE'],\n",
    "    dropout_rate=CFG_model['DROPOUT_RATE'],\n",
    "    out_size=CFG_model['OUTPUT_SIZE']\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CFG_model['LEARNING_RATE'], weight_decay=1e-4)\n",
    "scheduler = LinearWarmupCosineAnnealingLR(\n",
    "    optimizer,\n",
    "    warmup_epochs=CFG_model['WARMUP_EPOCHS'],\n",
    "    max_epochs=CFG_model['MAX_EPOCHS'],\n",
    "    eta_min=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "1b25967e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Start:\n",
      "Epoch: 1/200, Train Loss: 53.40112, Valid Loss: 50.17640, lr: 0.000167\n",
      "Epoch: 2/200, Train Loss: 20.32659, Valid Loss: 6.59711, lr: 0.000333\n",
      "Epoch: 3/200, Train Loss: 8.05432, Valid Loss: 2.94506, lr: 0.000500\n",
      "Epoch: 4/200, Train Loss: 4.63937, Valid Loss: 3.17540, lr: 0.000500\n",
      "Epoch: 5/200, Train Loss: 3.46808, Valid Loss: 3.32192, lr: 0.000500\n",
      "Epoch: 6/200, Train Loss: 3.26878, Valid Loss: 3.55741, lr: 0.000500\n",
      "Epoch: 7/200, Train Loss: 3.58024, Valid Loss: 2.79612, lr: 0.000500\n",
      "Epoch: 8/200, Train Loss: 3.21830, Valid Loss: 2.94824, lr: 0.000500\n",
      "Epoch: 9/200, Train Loss: 3.02360, Valid Loss: 3.04223, lr: 0.000499\n",
      "Epoch: 10/200, Train Loss: 3.08429, Valid Loss: 2.73910, lr: 0.000499\n",
      "Epoch: 11/200, Train Loss: 2.86137, Valid Loss: 2.71702, lr: 0.000499\n",
      "Epoch: 12/200, Train Loss: 2.83863, Valid Loss: 2.74590, lr: 0.000498\n",
      "Epoch: 13/200, Train Loss: 2.83349, Valid Loss: 2.70676, lr: 0.000498\n",
      "Epoch: 14/200, Train Loss: 2.78575, Valid Loss: 2.67894, lr: 0.000497\n",
      "Epoch: 15/200, Train Loss: 2.76162, Valid Loss: 2.71465, lr: 0.000497\n",
      "Epoch: 16/200, Train Loss: 2.83168, Valid Loss: 2.60892, lr: 0.000496\n",
      "Epoch: 17/200, Train Loss: 2.78651, Valid Loss: 2.60477, lr: 0.000496\n",
      "Epoch: 18/200, Train Loss: 2.76775, Valid Loss: 2.69949, lr: 0.000495\n",
      "Epoch: 19/200, Train Loss: 2.74636, Valid Loss: 2.61150, lr: 0.000494\n",
      "Epoch: 20/200, Train Loss: 2.59700, Valid Loss: 2.75555, lr: 0.000493\n",
      "Epoch: 21/200, Train Loss: 2.79193, Valid Loss: 2.51744, lr: 0.000493\n",
      "Epoch: 22/200, Train Loss: 2.55575, Valid Loss: 2.50886, lr: 0.000492\n",
      "Epoch: 23/200, Train Loss: 2.55887, Valid Loss: 2.56846, lr: 0.000491\n",
      "Epoch: 24/200, Train Loss: 2.59364, Valid Loss: 2.48494, lr: 0.000490\n",
      "Epoch: 25/200, Train Loss: 2.50274, Valid Loss: 2.50039, lr: 0.000489\n",
      "Epoch: 26/200, Train Loss: 2.48874, Valid Loss: 2.47137, lr: 0.000488\n",
      "Epoch: 27/200, Train Loss: 2.60949, Valid Loss: 2.45709, lr: 0.000487\n",
      "Epoch: 28/200, Train Loss: 2.51614, Valid Loss: 2.47178, lr: 0.000485\n",
      "Epoch: 29/200, Train Loss: 2.51699, Valid Loss: 2.52840, lr: 0.000484\n",
      "Epoch: 30/200, Train Loss: 2.48585, Valid Loss: 2.45764, lr: 0.000483\n",
      "Epoch: 31/200, Train Loss: 2.44489, Valid Loss: 2.42492, lr: 0.000482\n",
      "Epoch: 32/200, Train Loss: 2.44652, Valid Loss: 2.51068, lr: 0.000480\n",
      "Epoch: 33/200, Train Loss: 2.47802, Valid Loss: 2.39428, lr: 0.000479\n",
      "Epoch: 34/200, Train Loss: 2.50465, Valid Loss: 2.41084, lr: 0.000477\n",
      "Epoch: 35/200, Train Loss: 2.56866, Valid Loss: 2.37497, lr: 0.000476\n",
      "Epoch: 36/200, Train Loss: 2.42118, Valid Loss: 2.41318, lr: 0.000474\n",
      "Epoch: 37/200, Train Loss: 2.39439, Valid Loss: 2.35505, lr: 0.000473\n",
      "Epoch: 38/200, Train Loss: 2.38509, Valid Loss: 2.32837, lr: 0.000471\n",
      "Epoch: 39/200, Train Loss: 2.32348, Valid Loss: 2.35576, lr: 0.000469\n",
      "Epoch: 40/200, Train Loss: 2.31196, Valid Loss: 2.35745, lr: 0.000468\n",
      "Epoch: 41/200, Train Loss: 2.36661, Valid Loss: 2.41055, lr: 0.000466\n",
      "Epoch: 42/200, Train Loss: 2.37636, Valid Loss: 2.30376, lr: 0.000464\n",
      "Epoch: 43/200, Train Loss: 2.32335, Valid Loss: 2.31028, lr: 0.000462\n",
      "Epoch: 44/200, Train Loss: 2.29282, Valid Loss: 2.31615, lr: 0.000460\n",
      "Epoch: 45/200, Train Loss: 2.21664, Valid Loss: 2.30800, lr: 0.000458\n",
      "Epoch: 46/200, Train Loss: 2.21630, Valid Loss: 2.31309, lr: 0.000456\n",
      "Epoch: 47/200, Train Loss: 2.32673, Valid Loss: 2.30684, lr: 0.000454\n",
      "Epoch: 48/200, Train Loss: 2.24462, Valid Loss: 2.30169, lr: 0.000452\n",
      "Epoch: 49/200, Train Loss: 2.25167, Valid Loss: 2.28984, lr: 0.000450\n",
      "Epoch: 50/200, Train Loss: 2.26805, Valid Loss: 2.32288, lr: 0.000448\n",
      "Epoch: 51/200, Train Loss: 2.26903, Valid Loss: 2.30125, lr: 0.000446\n",
      "Epoch: 52/200, Train Loss: 2.24174, Valid Loss: 2.30190, lr: 0.000444\n",
      "Epoch: 53/200, Train Loss: 2.32769, Valid Loss: 2.30932, lr: 0.000441\n",
      "Epoch: 54/200, Train Loss: 2.23414, Valid Loss: 2.30947, lr: 0.000439\n",
      "Epoch: 55/200, Train Loss: 2.22067, Valid Loss: 2.35627, lr: 0.000437\n",
      "Epoch: 56/200, Train Loss: 2.39237, Valid Loss: 2.30531, lr: 0.000434\n",
      "Epoch: 57/200, Train Loss: 2.19856, Valid Loss: 2.27485, lr: 0.000432\n",
      "Epoch: 58/200, Train Loss: 2.18072, Valid Loss: 2.27388, lr: 0.000430\n",
      "Epoch: 59/200, Train Loss: 2.22529, Valid Loss: 2.28093, lr: 0.000427\n",
      "Epoch: 60/200, Train Loss: 2.21174, Valid Loss: 2.29227, lr: 0.000425\n",
      "Epoch: 61/200, Train Loss: 2.22612, Valid Loss: 2.26769, lr: 0.000422\n",
      "Epoch: 62/200, Train Loss: 2.26804, Valid Loss: 2.28284, lr: 0.000420\n",
      "Epoch: 63/200, Train Loss: 2.28397, Valid Loss: 2.29647, lr: 0.000417\n",
      "Epoch: 64/200, Train Loss: 2.15448, Valid Loss: 2.28713, lr: 0.000414\n",
      "Epoch: 65/200, Train Loss: 2.21467, Valid Loss: 2.27636, lr: 0.000412\n",
      "Epoch: 66/200, Train Loss: 2.17950, Valid Loss: 2.26822, lr: 0.000409\n",
      "Epoch: 67/200, Train Loss: 2.16733, Valid Loss: 2.31200, lr: 0.000406\n",
      "Epoch: 68/200, Train Loss: 2.19544, Valid Loss: 2.32067, lr: 0.000404\n",
      "Epoch: 69/200, Train Loss: 2.21282, Valid Loss: 2.28458, lr: 0.000401\n",
      "Epoch: 70/200, Train Loss: 2.25220, Valid Loss: 2.30143, lr: 0.000398\n",
      "Epoch: 71/200, Train Loss: 2.15368, Valid Loss: 2.29858, lr: 0.000395\n",
      "Epoch: 72/200, Train Loss: 2.12809, Valid Loss: 2.29374, lr: 0.000393\n",
      "Epoch: 73/200, Train Loss: 2.13575, Valid Loss: 2.28074, lr: 0.000390\n",
      "Epoch: 74/200, Train Loss: 2.18214, Valid Loss: 2.26550, lr: 0.000387\n",
      "Epoch: 75/200, Train Loss: 2.11532, Valid Loss: 2.30402, lr: 0.000384\n",
      "Epoch: 76/200, Train Loss: 2.17402, Valid Loss: 2.28518, lr: 0.000381\n",
      "Epoch: 77/200, Train Loss: 2.12128, Valid Loss: 2.27472, lr: 0.000378\n",
      "Epoch: 78/200, Train Loss: 2.13172, Valid Loss: 2.29619, lr: 0.000375\n",
      "Epoch: 79/200, Train Loss: 2.09458, Valid Loss: 2.28543, lr: 0.000372\n",
      "Epoch: 80/200, Train Loss: 2.12067, Valid Loss: 2.28975, lr: 0.000369\n",
      "Epoch: 81/200, Train Loss: 2.15538, Valid Loss: 2.29207, lr: 0.000366\n",
      "Epoch: 82/200, Train Loss: 2.12971, Valid Loss: 2.30188, lr: 0.000363\n",
      "Epoch: 83/200, Train Loss: 2.17182, Valid Loss: 2.28805, lr: 0.000360\n",
      "Epoch: 84/200, Train Loss: 2.11127, Valid Loss: 2.28387, lr: 0.000357\n",
      "Epoch: 85/200, Train Loss: 2.14201, Valid Loss: 2.28952, lr: 0.000354\n",
      "Epoch: 86/200, Train Loss: 2.12162, Valid Loss: 2.27719, lr: 0.000351\n",
      "Epoch: 87/200, Train Loss: 2.08206, Valid Loss: 2.27503, lr: 0.000348\n",
      "Epoch: 88/200, Train Loss: 2.09828, Valid Loss: 2.27069, lr: 0.000345\n",
      "Epoch: 89/200, Train Loss: 2.13441, Valid Loss: 2.26124, lr: 0.000341\n",
      "Epoch: 90/200, Train Loss: 2.12414, Valid Loss: 2.26554, lr: 0.000338\n",
      "Epoch: 91/200, Train Loss: 2.16495, Valid Loss: 2.26395, lr: 0.000335\n",
      "Epoch: 92/200, Train Loss: 2.10025, Valid Loss: 2.29133, lr: 0.000332\n",
      "Epoch: 93/200, Train Loss: 2.15796, Valid Loss: 2.32754, lr: 0.000329\n",
      "Epoch: 94/200, Train Loss: 2.23540, Valid Loss: 2.28264, lr: 0.000326\n",
      "Epoch: 95/200, Train Loss: 2.42127, Valid Loss: 2.24972, lr: 0.000322\n",
      "Epoch: 96/200, Train Loss: 2.18092, Valid Loss: 2.25264, lr: 0.000319\n",
      "Epoch: 97/200, Train Loss: 2.20793, Valid Loss: 2.25196, lr: 0.000316\n",
      "Epoch: 98/200, Train Loss: 2.13467, Valid Loss: 2.31598, lr: 0.000313\n",
      "Epoch: 99/200, Train Loss: 2.10326, Valid Loss: 2.27917, lr: 0.000310\n",
      "Epoch: 100/200, Train Loss: 2.21367, Valid Loss: 2.25832, lr: 0.000306\n",
      "Epoch: 101/200, Train Loss: 2.08651, Valid Loss: 2.25917, lr: 0.000303\n",
      "Epoch: 102/200, Train Loss: 2.05505, Valid Loss: 2.38904, lr: 0.000300\n",
      "Epoch: 103/200, Train Loss: 2.15685, Valid Loss: 2.24101, lr: 0.000297\n",
      "Epoch: 104/200, Train Loss: 2.05568, Valid Loss: 2.25403, lr: 0.000294\n",
      "Epoch: 105/200, Train Loss: 2.07333, Valid Loss: 2.27027, lr: 0.000290\n",
      "Epoch: 106/200, Train Loss: 2.08445, Valid Loss: 2.25998, lr: 0.000287\n",
      "Epoch: 107/200, Train Loss: 2.08747, Valid Loss: 2.36183, lr: 0.000284\n",
      "Epoch: 108/200, Train Loss: 2.21318, Valid Loss: 2.26197, lr: 0.000281\n",
      "Epoch: 109/200, Train Loss: 2.12478, Valid Loss: 2.26324, lr: 0.000278\n",
      "Epoch: 110/200, Train Loss: 2.11751, Valid Loss: 2.27687, lr: 0.000274\n",
      "Epoch: 111/200, Train Loss: 2.12234, Valid Loss: 2.28093, lr: 0.000271\n",
      "Epoch: 112/200, Train Loss: 2.08442, Valid Loss: 2.25994, lr: 0.000268\n",
      "Epoch: 113/200, Train Loss: 2.10807, Valid Loss: 2.30263, lr: 0.000265\n",
      "Epoch: 114/200, Train Loss: 2.14180, Valid Loss: 2.26568, lr: 0.000262\n",
      "Epoch: 115/200, Train Loss: 2.13028, Valid Loss: 2.29034, lr: 0.000259\n",
      "Epoch: 116/200, Train Loss: 2.20809, Valid Loss: 2.30426, lr: 0.000255\n",
      "Epoch: 117/200, Train Loss: 2.15453, Valid Loss: 2.27821, lr: 0.000252\n",
      "Epoch: 118/200, Train Loss: 2.18877, Valid Loss: 2.36741, lr: 0.000249\n",
      "Epoch: 119/200, Train Loss: 2.12618, Valid Loss: 2.30971, lr: 0.000246\n",
      "Epoch: 120/200, Train Loss: 2.11039, Valid Loss: 2.30076, lr: 0.000243\n",
      "Epoch: 121/200, Train Loss: 2.08036, Valid Loss: 2.27397, lr: 0.000240\n",
      "Epoch: 122/200, Train Loss: 2.11801, Valid Loss: 2.28745, lr: 0.000237\n",
      "Epoch: 123/200, Train Loss: 2.08422, Valid Loss: 2.27712, lr: 0.000234\n",
      "Epoch: 124/200, Train Loss: 2.04616, Valid Loss: 2.28627, lr: 0.000231\n",
      "Epoch: 125/200, Train Loss: 2.08685, Valid Loss: 2.28548, lr: 0.000228\n",
      "Epoch: 126/200, Train Loss: 2.09338, Valid Loss: 2.30328, lr: 0.000225\n",
      "Epoch: 127/200, Train Loss: 2.06712, Valid Loss: 2.30691, lr: 0.000222\n",
      "Epoch: 128/200, Train Loss: 2.03898, Valid Loss: 2.28401, lr: 0.000219\n",
      "Epoch: 129/200, Train Loss: 2.06078, Valid Loss: 2.29626, lr: 0.000216\n",
      "Epoch: 130/200, Train Loss: 2.07580, Valid Loss: 2.27162, lr: 0.000213\n",
      "Epoch: 131/200, Train Loss: 2.09814, Valid Loss: 2.26873, lr: 0.000210\n",
      "Epoch: 132/200, Train Loss: 2.12285, Valid Loss: 2.26890, lr: 0.000207\n",
      "Epoch: 133/200, Train Loss: 2.06209, Valid Loss: 2.26955, lr: 0.000205\n",
      "Epoch: 134/200, Train Loss: 2.16617, Valid Loss: 2.28694, lr: 0.000202\n",
      "Epoch: 135/200, Train Loss: 2.09144, Valid Loss: 2.36294, lr: 0.000199\n",
      "Epoch: 136/200, Train Loss: 2.10017, Valid Loss: 2.25882, lr: 0.000196\n",
      "Epoch: 137/200, Train Loss: 2.18257, Valid Loss: 2.26932, lr: 0.000194\n",
      "Epoch: 138/200, Train Loss: 2.14683, Valid Loss: 2.39120, lr: 0.000191\n",
      "Epoch: 139/200, Train Loss: 2.08150, Valid Loss: 2.46039, lr: 0.000188\n",
      "Epoch: 140/200, Train Loss: 2.25821, Valid Loss: 2.29216, lr: 0.000186\n",
      "Epoch: 141/200, Train Loss: 2.19271, Valid Loss: 2.44080, lr: 0.000183\n",
      "Epoch: 142/200, Train Loss: 2.06627, Valid Loss: 2.30302, lr: 0.000180\n",
      "Epoch: 143/200, Train Loss: 2.07342, Valid Loss: 2.25186, lr: 0.000178\n",
      "Epoch: 144/200, Train Loss: 2.05192, Valid Loss: 2.27879, lr: 0.000175\n",
      "Epoch: 145/200, Train Loss: 2.03872, Valid Loss: 2.26505, lr: 0.000173\n",
      "Epoch: 146/200, Train Loss: 2.06316, Valid Loss: 2.24516, lr: 0.000170\n",
      "Epoch: 147/200, Train Loss: 2.06716, Valid Loss: 2.28344, lr: 0.000168\n",
      "Epoch: 148/200, Train Loss: 2.07978, Valid Loss: 2.26913, lr: 0.000166\n",
      "Epoch: 149/200, Train Loss: 2.12768, Valid Loss: 2.24150, lr: 0.000163\n",
      "Epoch: 150/200, Train Loss: 2.08103, Valid Loss: 2.26137, lr: 0.000161\n",
      "Epoch: 151/200, Train Loss: 2.02989, Valid Loss: 2.24992, lr: 0.000159\n",
      "Epoch: 152/200, Train Loss: 1.98695, Valid Loss: 2.24584, lr: 0.000156\n",
      "Epoch: 153/200, Train Loss: 2.10697, Valid Loss: 2.31490, lr: 0.000154\n",
      "Epoch: 154/200, Train Loss: 2.09127, Valid Loss: 2.24773, lr: 0.000152\n",
      "Epoch: 155/200, Train Loss: 2.07884, Valid Loss: 2.26870, lr: 0.000150\n",
      "Epoch: 156/200, Train Loss: 2.08895, Valid Loss: 2.28829, lr: 0.000148\n",
      "Epoch: 157/200, Train Loss: 2.10354, Valid Loss: 2.24524, lr: 0.000146\n",
      "Epoch: 158/200, Train Loss: 2.09021, Valid Loss: 2.32768, lr: 0.000144\n",
      "Epoch: 159/200, Train Loss: 2.08823, Valid Loss: 2.26064, lr: 0.000142\n",
      "Epoch: 160/200, Train Loss: 2.07167, Valid Loss: 2.24133, lr: 0.000140\n",
      "Epoch: 161/200, Train Loss: 2.02109, Valid Loss: 2.33074, lr: 0.000138\n",
      "Epoch: 162/200, Train Loss: 2.16584, Valid Loss: 2.24862, lr: 0.000136\n",
      "Epoch: 163/200, Train Loss: 2.04797, Valid Loss: 2.32756, lr: 0.000134\n",
      "Epoch: 164/200, Train Loss: 2.06030, Valid Loss: 2.26293, lr: 0.000132\n",
      "Epoch: 165/200, Train Loss: 2.11796, Valid Loss: 2.27925, lr: 0.000131\n",
      "Epoch: 166/200, Train Loss: 2.00479, Valid Loss: 2.26604, lr: 0.000129\n",
      "Epoch: 167/200, Train Loss: 2.08104, Valid Loss: 2.24764, lr: 0.000127\n",
      "Epoch: 168/200, Train Loss: 2.04555, Valid Loss: 2.28302, lr: 0.000126\n",
      "Epoch: 169/200, Train Loss: 2.05417, Valid Loss: 2.26020, lr: 0.000124\n",
      "Epoch: 170/200, Train Loss: 2.07676, Valid Loss: 2.25959, lr: 0.000123\n",
      "Epoch: 171/200, Train Loss: 2.00540, Valid Loss: 2.26985, lr: 0.000121\n",
      "Epoch: 172/200, Train Loss: 2.03338, Valid Loss: 2.26733, lr: 0.000120\n",
      "Epoch: 173/200, Train Loss: 2.06728, Valid Loss: 2.29833, lr: 0.000118\n",
      "Epoch: 174/200, Train Loss: 2.12540, Valid Loss: 2.25454, lr: 0.000117\n",
      "Epoch: 175/200, Train Loss: 2.10328, Valid Loss: 2.42187, lr: 0.000116\n",
      "Epoch: 176/200, Train Loss: 2.06615, Valid Loss: 2.25364, lr: 0.000115\n",
      "Epoch: 177/200, Train Loss: 2.02097, Valid Loss: 2.24176, lr: 0.000113\n",
      "Epoch: 178/200, Train Loss: 2.00730, Valid Loss: 2.23963, lr: 0.000112\n",
      "Epoch: 179/200, Train Loss: 2.00653, Valid Loss: 2.26211, lr: 0.000111\n",
      "Epoch: 180/200, Train Loss: 2.09809, Valid Loss: 2.27245, lr: 0.000110\n",
      "Epoch: 181/200, Train Loss: 2.07421, Valid Loss: 2.23874, lr: 0.000109\n",
      "Epoch: 182/200, Train Loss: 2.03318, Valid Loss: 2.29879, lr: 0.000108\n",
      "Epoch: 183/200, Train Loss: 2.05634, Valid Loss: 2.24270, lr: 0.000107\n",
      "Epoch: 184/200, Train Loss: 2.03071, Valid Loss: 2.25525, lr: 0.000107\n",
      "Epoch: 185/200, Train Loss: 2.04332, Valid Loss: 2.27390, lr: 0.000106\n",
      "Epoch: 186/200, Train Loss: 2.04162, Valid Loss: 2.28547, lr: 0.000105\n",
      "Epoch: 187/200, Train Loss: 2.12520, Valid Loss: 2.31692, lr: 0.000104\n",
      "Epoch: 188/200, Train Loss: 2.04371, Valid Loss: 2.24841, lr: 0.000104\n",
      "Epoch: 189/200, Train Loss: 2.03012, Valid Loss: 2.23122, lr: 0.000103\n",
      "Epoch: 190/200, Train Loss: 2.02373, Valid Loss: 2.30028, lr: 0.000103\n",
      "Epoch: 191/200, Train Loss: 2.06220, Valid Loss: 2.23029, lr: 0.000102\n",
      "Epoch: 192/200, Train Loss: 2.07001, Valid Loss: 2.30532, lr: 0.000102\n",
      "Epoch: 193/200, Train Loss: 2.11173, Valid Loss: 2.23496, lr: 0.000101\n",
      "Epoch: 194/200, Train Loss: 2.06453, Valid Loss: 2.35362, lr: 0.000101\n",
      "Epoch: 195/200, Train Loss: 2.02596, Valid Loss: 2.23944, lr: 0.000101\n",
      "Epoch: 196/200, Train Loss: 2.00563, Valid Loss: 2.23661, lr: 0.000100\n",
      "Epoch: 197/200, Train Loss: 2.02364, Valid Loss: 2.24239, lr: 0.000100\n",
      "Epoch: 198/200, Train Loss: 1.97440, Valid Loss: 2.25978, lr: 0.000100\n",
      "Epoch: 199/200, Train Loss: 2.03612, Valid Loss: 2.32647, lr: 0.000100\n",
      "Epoch: 200/200, Train Loss: 2.04943, Valid Loss: 2.23592, lr: 0.000100\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Start:\")\n",
    "model.to(device)\n",
    "model_train, train_losses, valid_losses, learning_rates = train(train_loader, valid_loader, model, criterion, optimizer, scheduler, epochs=CFG_model['EPOCHS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "da59a9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rmse(model, valid_loader, criterion):\n",
    "    model.eval()\n",
    "    valid_running_loss = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in valid_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs).squeeze(dim=1)\n",
    "            loss = criterion(outputs, targets)\n",
    "            batch_size = inputs.size(0)\n",
    "            valid_running_loss += loss.item() * batch_size\n",
    "            total_samples += batch_size\n",
    "    \n",
    "    avg_loss = valid_running_loss / total_samples\n",
    "    rmse = math.sqrt(avg_loss)\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "41f80899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최종 Validation RMSE: 1.5548047930018307\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "# 최종 RMSE 계산\n",
    "final_rmse = evaluate_rmse(model, valid_loader, criterion)\n",
    "print(\"최종 Validation RMSE:\", final_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f051efd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP CV mean score saved: 1.4808488886333815\n"
     ]
    }
   ],
   "source": [
    "# 앙상블을 위한 가중치 파일 저장\n",
    "# mlp_cv_scores\n",
    "# [np.float64(1.4735811248471444),\n",
    "#  np.float64(1.4590740801849447),\n",
    "#  np.float64(1.4521791715262042),\n",
    "#  np.float64(1.5729801947246587),\n",
    "#  np.float64(1.4046790089843888)]\n",
    "\n",
    "import json\n",
    "\n",
    "# 평균 점수 계산\n",
    "mlp_mean_score = np.mean(mlp_cv_scores)\n",
    "\n",
    "# JSON 형식으로 저장 (가중치 계산용)\n",
    "with open('mlp_cv_score.json', 'w') as f:\n",
    "    json.dump({'mean_score': float(mlp_mean_score)}, f)\n",
    "\n",
    "print(f\"MLP CV mean score saved: {mlp_mean_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "fa5ca4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_curves(train_loss, valid_loss, lrs):\n",
    "    epochs = list(range(1, len(train_loss) + 1))\n",
    "    \n",
    "    # Loss\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_loss, label='Train Loss')\n",
    "    plt.plot(epochs, valid_loss, label='Valid Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Train vs Valid Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # LR\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, lrs, label='Learning Rate', color='orange')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.title('Learning Rate Schedule')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "21e4773a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train/Validation loss 및 learning rate 시각화\n",
    "plot_training_curves(train_losses, valid_losses, learning_rates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ad663b",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "45bd3d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"C:/Users/user/Desktop/dacon_drug_development/dataset/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "430d423e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:29:01] DEPRECATION WARNING: please use MorganGenerator\n"
     ]
    }
   ],
   "source": [
    "# test dataset preprocessing\n",
    "test['Fingerprint'] = test['Smiles'].apply(smiles_to_fingerprint)\n",
    "test = test[test['Fingerprint'].notnull()]\n",
    "\n",
    "test_features_df = pd.DataFrame([descriptors(s) for s in test['Smiles']])\n",
    "test_final_dataset = pd.concat([test, test_features_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "4886eea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading for test dataset\n",
    "\n",
    "test_dataset = test_final_dataset.drop(['ID', 'Smiles'], axis=1)\n",
    "# test 데이터 feature 준비\n",
    "X_test = prepare_features(test_dataset)\n",
    "# transform; train에서 fit, test에는 transform만.\n",
    "X_test_tr = transform.transform(X_test)\n",
    "# CustomDataset\n",
    "test_dataset = CustomDataset(X_test_tr, y=None)\n",
    "# DataLoader\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=CFG_model['BATCH_SIZE'], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "id": "368d71c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(test_loader, model):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            output = model(inputs).squeeze(dim=1)  # torch.Size([batch_size])\n",
    "            # 이거 shape 확인해봐야함. \n",
    "            preds.extend(output.cpu().numpy().flatten().tolist())\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "5681ad28",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = inference(test_loader, model_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1db601ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앙상블을 위한 예측값 저장\n",
    "np.save('mlp_test_preds.npy', np.array(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2854abfb",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "id": "ce8bfe5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('C:/Users/user/Desktop/dacon_drug_development/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "8f1e97b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>ASK1_IC50_nM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_000</td>\n",
       "      <td>8.314689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_001</td>\n",
       "      <td>8.310142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_002</td>\n",
       "      <td>8.310142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_003</td>\n",
       "      <td>8.380826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_004</td>\n",
       "      <td>8.450397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>TEST_122</td>\n",
       "      <td>8.152762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>TEST_123</td>\n",
       "      <td>8.263204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>TEST_124</td>\n",
       "      <td>7.819127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>TEST_125</td>\n",
       "      <td>8.055634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>TEST_126</td>\n",
       "      <td>7.941533</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>127 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID  ASK1_IC50_nM\n",
       "0    TEST_000      8.314689\n",
       "1    TEST_001      8.310142\n",
       "2    TEST_002      8.310142\n",
       "3    TEST_003      8.380826\n",
       "4    TEST_004      8.450397\n",
       "..        ...           ...\n",
       "122  TEST_122      8.152762\n",
       "123  TEST_123      8.263204\n",
       "124  TEST_124      7.819127\n",
       "125  TEST_125      8.055634\n",
       "126  TEST_126      7.941533\n",
       "\n",
       "[127 rows x 2 columns]"
      ]
     },
     "execution_count": 515,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission['ASK1_IC50_nM'] = predictions\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "1593e1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('C:/Users/user/Desktop/dacon_drug_development/MLP_5_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
